{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from utils_plotting import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import metpy\n",
    "import numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data_location = 'data'\n",
    "outlooks, pph, reports = read_datasets(data_location, 'labelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_date(outlooks, pph, reports):\n",
    "    earliest_date = max(min(outlooks['DATE']), min(pph['time']), min(reports['DATE'])) #TODO first two days of outlook dataset don't have day 3 forecast. but probably ok since not mdt\n",
    "    latest_date = min(max(outlooks['DATE']), max(pph['time']), max(reports['DATE']))\n",
    "    reports = reports[reports['DATE'] <= latest_date]\n",
    "    reports = reports[reports['DATE'] >= earliest_date]\n",
    "    outlooks = outlooks[outlooks['DATE'] <= latest_date]\n",
    "    outlooks = outlooks[outlooks['DATE'] >= earliest_date]\n",
    "    all_pph_dates = pph['time']\n",
    "    pph_dates = all_pph_dates[all_pph_dates <= latest_date]\n",
    "    pph_dates = pph_dates[pph_dates >= earliest_date]\n",
    "    pph = pph.sel(time = pph_dates)\n",
    "    return(outlooks, pph, reports)\n",
    "\n",
    "(outlooks, pph, reports) = consolidate_date(outlooks, pph, reports)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['MAX_CAT', 'RAMP_CAT', 'SEASON', 'REGION_M', 'RAMP_UP', 'RAMP_DOWN', \n",
    "               'PPH_D_NUM',\n",
    "               'REPORT_NUM', 'TOR_NUM', 'WIND_NUM', 'HAIL_NUM', \n",
    "               'TOR_F', 'WINDSP_NUM', 'HAILSZ_NUM', \n",
    "               'BS_NUM', 'NEIGH_NUM', 'RMSE_NUM',\n",
    "               'POD_NUM', 'FAR_NUM',\n",
    "               'POD_H_NUM', 'FAR_H_NUM',\n",
    "               'POD_W_NUM', 'FAR_W_NUM',\n",
    "               'POD_T_NUM', 'FAR_T_NUM',\n",
    "               'FART_NUM',\n",
    "               'E_SH_NUM', 'E_SH_H_NUM', 'E_SH_W_NUM', 'E_SH_T_NUM',\n",
    "               'N_SH_NUM', 'N_SH_H_NUM', 'N_SH_W_NUM', 'N_SH_T_NUM',\n",
    "               'DIV_NUM', 'DIV_H_NUM', 'DIV_W_NUM', 'DIV_T_NUM'\n",
    "               ]\n",
    "\n",
    "category_dict = {\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "ramp_dict = {\n",
    "    'up': 0,\n",
    "    'down': 1,\n",
    "    'both': 2,\n",
    "    'neither': 3\n",
    "}\n",
    "\n",
    "season_dict = {\n",
    "    'Winter': 0,\n",
    "    'Spring': 1,\n",
    "    'Summer': 2,\n",
    "    'Fall': 3\n",
    "}\n",
    "\n",
    "region_dict = {\n",
    "    'NONE': -1,\n",
    "    'West': 0,\n",
    "    'Great Plains': 1,\n",
    "    'Midwest': 2,\n",
    "    'Northeast': 3,\n",
    "    'South': 4\n",
    "}\n",
    "\n",
    "ramp_up_dict = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4,\n",
    "    '5': 5,\n",
    "    '6': 6\n",
    "}\n",
    "\n",
    "ramp_down_dict = {\n",
    "    '0': 0,\n",
    "    '-1': 1,\n",
    "    '-2': 2,\n",
    "    '-3': 3,\n",
    "    '-4': 4,\n",
    "    '-5': 5,\n",
    "    '-6': 6\n",
    "}\n",
    "\n",
    "pph_dict = {\n",
    "    'ZERO': 0,\n",
    "    'TSTM': 1,\n",
    "    'MRGL': 2,\n",
    "    'SLGT': 3,\n",
    "    'ENH': 4,\n",
    "    'MDT': 5,\n",
    "    'HIGH': 6\n",
    "}\n",
    "\n",
    "tor_dict = {\n",
    "    'NONE': -1,\n",
    "    'EFU': 0,\n",
    "    '(E)F0': 1,\n",
    "    '(E)F1': 2,\n",
    "    '(E)F2': 3,\n",
    "    '(E)F3': 4,\n",
    "    '(E)F4': 5,\n",
    "    '(E)F5': 6\n",
    "}\n",
    "\n",
    "dicts = [category_dict, ramp_dict, season_dict, region_dict, ramp_up_dict, ramp_down_dict, \n",
    "         None, \n",
    "         None, None, None, None, \n",
    "         tor_dict, None, None,\n",
    "         None, None, None,\n",
    "         None, None,\n",
    "         None, None,\n",
    "         None, None,\n",
    "         None, None,\n",
    "         None,\n",
    "         None, None, None, None,\n",
    "         None, None, None, None,\n",
    "         None, None, None, None] \n",
    "\n",
    "steps = [1, 1, 1, 1, 1, 1, \n",
    "         10, \n",
    "         100, 25, 100, 50, \n",
    "         1, 10, 1, \n",
    "         .0025, .001, .01,\n",
    "         .05, .1, \n",
    "         .05, .1,\n",
    "         .05, .1,\n",
    "         .05, .1,\n",
    "         .001,\n",
    "         100000, 100000, 100000, 100000,\n",
    "         100000, 100000, 100000, 100000,\n",
    "         .5, .5, .5, .5\n",
    "         ]\n",
    "\n",
    "written_labels = ['Categorical Risk', 'Ramp', 'Season', 'Region', 'Ramp Up', 'Ramp Down', \n",
    "                  'Max PPH',\n",
    "                  'Total Storm Reports', 'Tornado Reports', 'Wind Reports', 'Hail Reports',\n",
    "                  'Max Tornado Rating', 'Max Wind Speed (kt)', 'Max Hail Size (in)',\n",
    "                  'Brier Score', 'Neighborhood Brier Score', 'RMSE',\n",
    "                  'Probability of Detection', 'False Alarm Ratio',\n",
    "                  'Hail Probability of Detection', 'Hail False Alarm Ratio',\n",
    "                  'Wind Probability of Detection', 'Wind False Alarm Ratio',\n",
    "                  'Tornado Probability of Detection', 'Tornado False Alarm Ratio',\n",
    "                  'False Alarm Rate',\n",
    "                  'East Shift', 'Hail East Shift', 'Wind East Shift', 'Tornado East Shift',\n",
    "                  'North Shift', 'Hail North Shift', 'Wind North Shift', 'Tornado North Shift',\n",
    "                  'Divergence', 'Hail Divergence', 'Wind Divergence', 'Tornado Divergence'\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(pph, label_name, label_dict, written_label, titlestring, save_location, bins = 20, show=False):\n",
    "    if label_dict == None:\n",
    "        plt.hist(pph[label_name].values, bins = bins)\n",
    "    else:\n",
    "        labels = pph[label_name]\n",
    "        label_counts = labels.groupby(labels).count()\n",
    "        def sort_order(key):\n",
    "            return label_dict[str(key[0])]\n",
    "        sorted_labels, sorted_counts = zip(*sorted(zip(list(label_counts[label_name].values), label_counts.values), key = sort_order))\n",
    "        plt.bar(sorted_labels, sorted_counts)  \n",
    "    plt.xlabel(written_label)\n",
    "    plt.title('Number of ' + titlestring + ' Days With Each '  + written_label)\n",
    "    plt.ylabel(\"Number of Days\")\n",
    "    if save_location != None:\n",
    "        plt.savefig(save_location + '/' + written_label + '_distribution.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(pph, 'E_SH_NUM', None, 'East Shift', '', None, show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_2d(pph, label_1, label_2, label_1_string, label_2_string, dict_1, dict_2, titlestring, save_location, show=False, defaultbins = 10, step_1 = 1, step_2 = 1):\n",
    "# plot 2d heatmap for any 2 labels\n",
    "\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "\n",
    "    for i in range(len(pph[label_1])):\n",
    "        if dict_1 != None:\n",
    "            data1.append(dict_1[str(pph[label_1].values[i])])\n",
    "        else: \n",
    "            data1.append(pph[label_1].values[i])\n",
    "        if dict_2 != None:\n",
    "            data2.append(dict_2[str(pph[label_2].values[i])])\n",
    "        else:\n",
    "            data2.append(pph[label_2].values[i])\n",
    "\n",
    "    \n",
    "    if dict_1 != None:\n",
    "        max1 = max(dict_1.values())\n",
    "        min1 = min(dict_1.values())\n",
    "        bins1 = np.linspace(min1-.5, max1+.5, 2+max1-min1)\n",
    "        irange = bins1.size-1\n",
    "\n",
    "    else:\n",
    "        if ((max(data1) - min(data1))/step_1).is_integer():\n",
    "            m = 2\n",
    "        else: \n",
    "            m = 1\n",
    "            \n",
    "        if min(data1) < 0:\n",
    "            bins1 = np.arange(step_1 * round(min(data1)/step_1), max(data1)+m*step_1, step_1)\n",
    "        else:\n",
    "            bins1 = np.insert(np.arange(step_1 * round(min(data1)/step_1), max(data1)+m*step_1, step_1), 0, 0)\n",
    "        \n",
    "            if isinstance(data1[1], numbers.Integral):\n",
    "                bins1[1] = bins1[1] + 1\n",
    "            else:\n",
    "                bins1[1] = bins1[1] + .00000000001\n",
    "        num_bins1 = len(bins1)-1\n",
    "        irange = num_bins1\n",
    "\n",
    "    if dict_2 != None:\n",
    "        max2 = max(dict_2.values())\n",
    "        min2 = min(dict_2.values())\n",
    "        bins2 = np.linspace(min2-.5, max2+.5, 2+max2-min2)\n",
    "        jrange = bins2.size-1\n",
    "    else:\n",
    "        if ((max(data2) - min(data2))/step_2).is_integer():\n",
    "            m = 2\n",
    "        else: \n",
    "            m = 1\n",
    "\n",
    "        if min(data2) < 0:\n",
    "            bins2 = np.arange(step_2 * round(min(data2)/step_2), max(data2)+m*step_2, step_2)\n",
    "        else:\n",
    "            bins2 = np.insert(np.arange(step_2 * round(min(data2)/step_2), max(data2)+m*step_2, step_2), 0, 0)\n",
    "\n",
    "            if isinstance(data2[1], numbers.Integral):\n",
    "                bins2[1] = bins2[1] + 1\n",
    "            else:\n",
    "                bins2[1] = bins2[1] + .00000000001\n",
    "        num_bins2 = len(bins2)-1\n",
    "        jrange = num_bins2\n",
    "\n",
    "    heatmap = np.histogram2d(data1, data2, bins = (bins1, bins2))\n",
    "    im = plt.imshow(heatmap[0],  norm=colors.LogNorm())\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    for i in range(irange): \n",
    "        for j in range(jrange): \n",
    "            plt.annotate(str(int(heatmap[0][i][j])), xy=(j, i), \n",
    "                        ha='center', va='center', color='black') \n",
    "\n",
    "    if dict_1 != None:\n",
    "        plt.yticks(range(len(list(dict_1.values()))), labels=list(dict_1.keys()))\n",
    "    else:\n",
    "        labels = [s + '+' for s in heatmap[1][range(num_bins1)].astype(str)]\n",
    "        if min(data1) >= 0:\n",
    "            labels[0] = '0'\n",
    "            if isinstance(data1[1], numbers.Integral):\n",
    "                labels[1] = '1+'\n",
    "            else:\n",
    "                labels[1] = '0+'\n",
    "        plt.yticks(range(num_bins1), labels = labels) \n",
    "        plt.yticks(fontsize=8)\n",
    "    if dict_2 != None:\n",
    "        plt.xticks(range(len(list(dict_2.values()))), labels=list(dict_2.keys()))\n",
    "    else:\n",
    "        labels = [s + '+' for s in heatmap[2][range(num_bins2)].astype(str)]\n",
    "        if min(data2) >= 0:\n",
    "            labels[0] = '0'\n",
    "            if isinstance(data2[1], numbers.Integral):\n",
    "                labels[1] = '1+'\n",
    "            else:\n",
    "                labels[1] = '0+'\n",
    "        plt.xticks(range(num_bins2), labels = labels)\n",
    "        plt.xticks(fontsize=8)\n",
    "\n",
    "    plt.ylabel(label_1_string)\n",
    "    plt.xlabel(label_2_string)\n",
    "\n",
    "    plt.title(\"Number of \" + titlestring + \" Days with each Combination of \" + label_1_string + \" and \" +  label_2_string)\n",
    "    if save_location != None:\n",
    "        plt.savefig(save_location + '/' + label_1_string + '_' + label_2_string + '_distribution.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_2d(pph, 'N_SH_NUM', 'E_SH_NUM', 'North Shift', 'East Shift', None, None, '', None, show = True, step_1 = 100000, step_2 = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes the next steps work properly for some reason\n",
    "for k in list(pph.keys()):\n",
    "    pph[k].values\n",
    "\n",
    "# moderate and up days only:\n",
    "all_pph_dates = pph['time']\n",
    "mdt_pph_dates = all_pph_dates[pph['MAX_CAT'].isin(['MDT', 'HIGH'])]\n",
    "mdt_pph = pph.sel(time = mdt_pph_dates)\n",
    "\n",
    "# dates since new categorical system \n",
    "new_cutoff = '201410230000'\n",
    "new_pph_dates = all_pph_dates[all_pph_dates >= new_cutoff]\n",
    "new_pph = pph.sel(time = new_pph_dates)\n",
    "\n",
    "# Moderate dates in new system\n",
    "mdt_new_pph_dates = mdt_pph_dates[mdt_pph_dates >= new_cutoff]\n",
    "mdt_new_pph = pph.sel(time = mdt_new_pph_dates)\n",
    "\n",
    "\n",
    "# dates since day 3 added\n",
    "day3_cutoff = '200203300000'\n",
    "day3_pph_dates = all_pph_dates[all_pph_dates >= day3_cutoff]\n",
    "day3_pph = pph.sel(time = day3_pph_dates)\n",
    "\n",
    "# Moderate dates since day 3 added\n",
    "mdt_day3_pph_dates = mdt_pph_dates[mdt_pph_dates >= day3_cutoff]\n",
    "mdt_day3_pph = pph.sel(time = mdt_day3_pph_dates)\n",
    "\n",
    "\n",
    "# dates since early day 2 added\n",
    "day21_cutoff = '199707100000'\n",
    "day21_pph_dates = all_pph_dates[all_pph_dates >= day21_cutoff]\n",
    "day21_pph = pph.sel(time = day21_pph_dates)\n",
    "\n",
    "\n",
    "# Moderate dates since early day 2 added\n",
    "mdt_day21_pph_dates = mdt_pph_dates[mdt_pph_dates >= day21_cutoff]\n",
    "mdt_day21_pph = pph.sel(time = mdt_day21_pph_dates)\n",
    "\n",
    "\n",
    "# dates since late day 2 added\n",
    "day22_cutoff = '199504040000'\n",
    "day22_pph_dates = all_pph_dates[all_pph_dates >= day22_cutoff]\n",
    "day22_pph = pph.sel(time = day22_pph_dates)\n",
    "\n",
    "# Moderate since late day 2 added\n",
    "mdt_day22_pph_dates = mdt_pph_dates[mdt_pph_dates >= day22_cutoff]\n",
    "mdt_day22_pph = pph.sel(time = mdt_day22_pph_dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(pph, label_names, dicts, written_labels, shortstring, titlestring, show=False):\n",
    "    \n",
    "    for i in range(len(written_labels)):\n",
    "        plot_distribution(pph, label_names[i], dicts[i], written_labels[i], titlestring, 'plots/label_distributions/'+ shortstring+'/1d/', show=show)\n",
    "\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(i):\n",
    "            plot_distribution_2d(pph, label_names[i], label_names[j], written_labels[i], written_labels[j], dicts[i], dicts[j], titlestring, 'plots/label_distributions/'+ shortstring+'/2d/', show=show, step_1 = steps[i], step_2 = steps[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_plots(pph, label_names, dicts, written_labels, 'all', 'All')\n",
    "#make_plots(mdt_pph, label_names, dicts, written_labels, 'mdt', 'Moderate')\n",
    "\n",
    "#make_plots(new_pph, label_names, dicts, written_labels, 'new', 'New')\n",
    "#make_plots(mdt_new_pph, label_names, dicts, written_labels, 'mdt_new', 'Moderate New')\n",
    "\n",
    "make_plots(day3_pph, label_names, dicts, written_labels, 'day3', 'Since 2002')\n",
    "make_plots(mdt_day3_pph, label_names, dicts, written_labels, 'mdt_day3', 'Moderate Since 2002')\n",
    "\n",
    "#make_plots(day21_pph, label_names, dicts, written_labels, 'day21', 'Since 1997')\n",
    "#make_plots(mdt_day21_pph, label_names, dicts, written_labels, 'mdt_day21', 'Moderate Since 1997')\n",
    "\n",
    "#make_plots(day22_pph, label_names, dicts, written_labels, 'day22', 'Since 1995')\n",
    "#make_plots(mdt_day22_pph, label_names, dicts, written_labels, 'mdt_day22', 'Moderate Since 1995')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timeseries_2(pph, vars_of_interest, var_names, save = True, save_location = 'plots/timeseries', show = True):\n",
    "    p = pph.sel(time = slice('200203300000', '202212310000'))\n",
    "    times = pd.to_datetime(p['time'].astype(str), format='%Y%m%d%H%M')\n",
    "\n",
    "    rolling = 365\n",
    "    half_rolling_1 = int(np.floor(rolling/2))\n",
    "    half_rolling_2 = rolling - half_rolling_1 - 1\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(times[half_rolling_1:-half_rolling_2], np.convolve(p[vars_of_interest[0]].data, np.ones(rolling)/rolling, 'valid'))\n",
    "    ax2.plot(times[half_rolling_1:-half_rolling_2], np.convolve(p[vars_of_interest[1]].data, np.ones(rolling)/rolling, 'valid'), color = 'r')\n",
    "    ax1.set_ylabel(var_names[0])\n",
    "    ax2.set_ylabel(var_names[1])\n",
    "    ax1.set_xlabel('Centered Time')\n",
    "    plt.title(var_names[0] + ' and ' + var_names[1] + ' 1-Year Running Mean Since 2002')\n",
    "\n",
    "    fig.tight_layout() \n",
    "    fig.legend(var_names)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save_location + '/timeseries_' + var_names[0] + var_names[1] + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_timeseries_n(pph, vars_of_interest, var_names, mod_name, ylabel, save = True, save_location = 'plots/timeseries', show = True):\n",
    "    p = pph.sel(time = slice('200203300000', '202212310000'))\n",
    "    times = pd.to_datetime(p['time'].astype(str), format='%Y%m%d%H%M')\n",
    "\n",
    "    rolling = 365\n",
    "    half_rolling_1 = int(np.floor(rolling/2))\n",
    "    half_rolling_2 = rolling - half_rolling_1 - 1\n",
    "\n",
    "    for var, name in zip(vars_of_interest, var_names):\n",
    "        plt.plot(times[half_rolling_1:-half_rolling_2], np.convolve(p[var].data, np.ones(rolling)/rolling, 'valid'))\n",
    "    \n",
    "    plt.xlabel('Centered Time')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(ylabel + ' 1-Year Running Mean Since 2002')\n",
    "    plt.tight_layout() \n",
    "    plt.legend(var_names)\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(save_location + '/timeseries_' + mod_name + '.png')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lists_2 = [['POD_NUM', 'FAR_NUM'],\n",
    "             ['RAMP_UP', 'RAMP_DOWN'],\n",
    "             ['FAR_NUM', 'FART_NUM'],\n",
    "             ['POD_W_NUM', 'FAR_W_NUM'],\n",
    "             ['POD_H_NUM', 'FAR_H_NUM'],\n",
    "             ['POD_T_NUM', 'FAR_T_NUM']]\n",
    "\n",
    "name_lists_2 = [['Probability of Detection', 'False Alarm Ratio'],\n",
    "             ['Ramp Up', 'Ramp Down'],\n",
    "             ['False Alarm Ratio', 'False Alarm Rate'],\n",
    "             ['Wind POD', 'Wind FAR'],\n",
    "             ['Hail POD', 'Hail FAR'],\n",
    "             ['Tornado POD', 'Tornado FAR']]\n",
    "\n",
    "for var, names in zip(var_lists_2, name_lists_2):\n",
    "    plot_timeseries_2(pph, var, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lists_n = [['REPORT_NUM', 'TOR_NUM', 'WIND_NUM', 'HAIL_NUM'],\n",
    "               ['PPH_D_NUM', 'PPH_NUM'],\n",
    "               ['POD_W_NUM', 'POD_H_NUM', 'POD_T_NUM'],\n",
    "               ['FAR_W_NUM', 'FAR_H_NUM', 'FAR_T_NUM'],\n",
    "               ['DIV_NUM', 'DIV_W_NUM', 'DIV_H_NUM', 'DIV_T_NUM'],\n",
    "               ['E_SH_NUM', 'E_SH_W_NUM', 'E_SH_H_NUM', 'E_SH_T_NUM'],\n",
    "               ['N_SH_NUM', 'N_SH_W_NUM', 'N_SH_H_NUM', 'N_SH_T_NUM']\n",
    "]\n",
    "\n",
    "name_lists_n = [['Total', 'Tornado', 'Wind', 'Hail'],\n",
    "                ['Dependent Hazards', 'Independent Hazards'],\n",
    "                ['Wind POD', 'Hail POD', 'Tornado POD'],\n",
    "                ['Wind FAR', 'Hail FAR', 'Tornado FAR'],\n",
    "                ['Divergence', 'Wind Divergence', 'Hail Divergence', 'Tornado Divergence'],\n",
    "                ['East Shift', 'Wind East Shift', 'Hail East Shift', 'Tornado East Shift'],\n",
    "                ['North Shift', 'Wind North Shift', 'Hail North Shift', 'Tornado North Shift']\n",
    "    \n",
    "]\n",
    "\n",
    "mod_names = ['reports', 'pph', 'pod', 'far', 'div', 'e_shift', 'n_shift']\n",
    "\n",
    "axis_names = ['Daily Storm Reports', 'Daily Max PPH', 'Probability of Detection', 'False Alarm Ratio', 'Divergence', 'East Shift (m)', 'North Shift (m)']\n",
    "\n",
    "for var, names, mod_name, axis_name in zip(var_lists_n, name_lists_n, mod_names, axis_names):\n",
    "    plot_timeseries_n(pph, var, names, mod_name, axis_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_outlook_location = 'data/outlooks/grid_outlooks.nc'\n",
    "grid_outlooks = xr.open_dataset(grid_outlook_location)\n",
    "g = grid_outlooks.sel(time = slice('200203300000', '202212310000'))\n",
    "times = pd.to_datetime(g['time'].astype(str), format='%Y%m%d%H%M')\n",
    "rolling = 365\n",
    "half_rolling_1 = int(np.floor(rolling/2))\n",
    "half_rolling_2 = rolling - half_rolling_1 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(times[half_rolling_1:-half_rolling_2], np.convolve(g['prob'].sel(outlook = 'Day 1').max(dim=['x', 'y']).data, np.ones(rolling)/rolling, 'valid'))\n",
    "plt.plot(times[half_rolling_1:-half_rolling_2], np.convolve(g['prob'].sel(outlook = 'Day 2 17').max(dim=['x', 'y']).data, np.ones(rolling)/rolling, 'valid'))\n",
    "plt.plot(times[half_rolling_1:-half_rolling_2], np.convolve(g['prob'].sel(outlook = 'Day 2 7').max(dim=['x', 'y']).data, np.ones(rolling)/rolling, 'valid'))\n",
    "plt.plot(times[half_rolling_1:-half_rolling_2], np.convolve(g['prob'].sel(outlook = 'Day 3').max(dim=['x', 'y']).data, np.ones(rolling)/rolling, 'valid'))\n",
    "plt.legend(['Day 1', 'Day 2 17z', 'Day 2 7z', 'Day 3'])\n",
    "\n",
    "plt.title('Daily Maximum Any-Hazard Probability Over Time')\n",
    "\n",
    "plt.ylabel('1-year Running Mean Daily Maximum Any-Hazard Probability')\n",
    "plt.xlabel('Centered Time')    \n",
    "plt.savefig('plots/timeseries/prob_over_time.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displacement Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Custom colors and label order (south and midwest on bottom, seasons in order)\n",
    "def stacked_histogram(pph, var_of_interest, var_stack, xlabel, titlestring, legend_title, save_location, bins = 40, show=False, save = True):\n",
    "    \n",
    "    plot_arrays = []\n",
    "\n",
    "    all_var_of_interest = pph[var_of_interest]\n",
    "\n",
    "    all_var_stack = pph[var_stack]\n",
    "    stack_values = list(set(all_var_stack.values))\n",
    "\n",
    "    for stack_value in stack_values:\n",
    "\n",
    "        plot_arrays.append(all_var_of_interest[all_var_stack == stack_value]/1000)\n",
    "\n",
    "    cmap = plt.get_cmap('Purples')\n",
    "    colors = [cmap(.5 + i / 2 / len(plot_arrays)) for i in range(len(plot_arrays))]\n",
    "\n",
    "    plt.hist(plot_arrays, bins, stacked=True, label = stack_values, color = colors, range = (-800, 800))\n",
    "    plt.plot([0, 0], [0, 50], color='k', linestyle='-')\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.title(titlestring)\n",
    "    plt.legend(title = legend_title)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(save_location + '.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "stacked_histogram(mdt_day3_pph, 'E_SH_W_NUM', 'REGION_M', 'Eastward Displacement (km)', 'Wind Eastward Optical Flow Displacement on MDT+ Days 2003-2022, by Region', 'Region', 'plots/results/wind_displacement.png', show = True, save = False)\n",
    "stacked_histogram(mdt_day3_pph, 'E_SH_H_NUM', 'SEASON', 'Eastward Displacement (km)', 'Hail Eastward Optical Flow Displacement on MDT+ Days 2003-2022, by Season', 'Season', 'plots/results/hail_displacement.png', show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD NON-GENERALIZED PLOTTING OF RAMPS / SCRATCH WORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate: old ramp ups by 5? Why lots of 5 and not 4? And none region, make sure no PPH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = outlooks[outlooks['REGION'] != 'NONE']\n",
    "test[test['DATE'] <= '199203160000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outlooks = outlooks[outlooks['MAX_CAT'] == 'SLGT']\n",
    "test_outlooks = test_outlooks[test_outlooks['RAMP_UP'] == 3]\n",
    "set(test_outlooks['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlooks[outlooks['DATE'] == '200208030000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make function that plots all 3 outlooks, PPH, and reports for one day to spotcheck\n",
    "# with utils_plotting functions\n",
    "def plot_day(datestring, outlooks, pph, reports):\n",
    "    outlooks_day = outlooks[outlooks['DATE'] == datestring]\n",
    "    pph_day = pph.sel(time=datestring)\n",
    "    reports_day = reports[reports['DATE'] == datestring]\n",
    "    \n",
    "    print('plotting outlooks')\n",
    "    plot_outlooks_day(outlooks_day, 'plots/daily/'+datestring+'/outlooks', ['CATEGORICAL'], show=True)\n",
    "    print('plotting pph')\n",
    "    plot_pph_day(pph_day, 'plots/daily/'+datestring+'/pph', ['total'], show=True, sig = False)\n",
    "    #print('plotting reports')\n",
    "    #plot_reports(reports_day, 'plots/daily/'+datestring+'/reports', ['Hail', 'Thunderstorm Wind', 'Tornado'], show=True)\n",
    "    return\n",
    "\n",
    "#plot_day('200504220000', outlooks, pph, reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_day('201010100000', outlooks, pph, reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ramps(pph, title_insert=''):\n",
    "    # 2d histogram of ramps\n",
    "    # NOT FOR USE\n",
    "    ramp_up_bins = [-.5, .5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5]\n",
    "    ramp_up_amounts = [0, 1, 2, 3, 4, 5, 6]\n",
    "    ramp_down_amounts = [-6, -5, -4, -3, -2, -1, 0]\n",
    "    ramp_down_bins = [-6.5, -5.5, -4.5, -3.5, -2.5, -1.5, -.5, .5]\n",
    "    heatmap = np.histogram2d(np.array(pph['RAMP_UP'][pph['RAMP_UP']!= 'NONE'], dtype=int), np.array(pph['RAMP_DOWN'][pph['RAMP_UP']!= 'NONE'], dtype=int), bins = (ramp_up_bins, ramp_down_bins))\n",
    "    im = plt.imshow(heatmap[0],  norm=colors.LogNorm())\n",
    "    plt.colorbar(im)\n",
    "    bins = len(ramp_up_bins)\n",
    "    for i in range(bins-1): \n",
    "        for j in range(bins-1): \n",
    "            plt.annotate(str(int(heatmap[0][i][j])), xy=(j, i), \n",
    "                        ha='center', va='center', color='black') \n",
    "\n",
    "    plt.xticks(ramp_up_amounts, labels=ramp_down_amounts)\n",
    "    plt.yticks(ramp_up_amounts, labels=ramp_up_amounts)\n",
    "    plt.xlabel(\"Ramp Down\")\n",
    "    plt.ylabel(\"Ramp Up\")\n",
    "    plt.title(\"Number of \" + title_insert + \"Days with each Ramp Up and Ramp Down Amount\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(mdt_pph['time'].values).replace('\\n ', ' ').replace(' ', ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdt_new_pph['time'].where(mdt_new_pph['MAX_CAT'] == 'HIGH')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
