{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from utils_plotting import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import metpy\n",
    "import numbers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import *\n",
    "from sklearn_som.som import SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = 'data'\n",
    "outlooks, pph, reports = read_datasets(data_location, 'labelled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "\n",
    "season_dict = {\n",
    "    'Winter': 0,\n",
    "    'Spring': 1,\n",
    "    'Summer': 2,\n",
    "    'Fall': 3\n",
    "}\n",
    "\n",
    "region_dict = {\n",
    "    'NONE': -1,\n",
    "    'West': 0,\n",
    "    'Great Plains': 1,\n",
    "    'Midwest': 2,\n",
    "    'Northeast': 3,\n",
    "    'South': 4\n",
    "}\n",
    "\n",
    "tor_dict = {\n",
    "    'NONE': -1,\n",
    "    'EFU': 0,\n",
    "    '(E)F0': 1,\n",
    "    '(E)F1': 2,\n",
    "    '(E)F2': 3,\n",
    "    '(E)F3': 4,\n",
    "    '(E)F4': 5,\n",
    "    '(E)F5': 6\n",
    "}\n",
    "\n",
    "cats = ['MAX_CAT', 'SEASON', 'TOR_F']\n",
    "dicts = [category_dict, season_dict, tor_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unneeded variables and x, y dimensions\n",
    "data = pph.drop(['lat', 'lon', 'p_perfect_wind', 'p_perfect_sig_wind', 'p_perfect_hail', 'p_perfect_sig_hail', 'p_perfect_tor', 'p_perfect_sig_tor', 'RAMP_CAT', 'p_perfect_total', 'p_perfect_max', 'PPH_CAT', 'PPH_NUM', 'PPH_D_CAT', 'WINDSP_CAT', 'HAILSZ_CAT', 'REGION', 'REGION_M'])\n",
    "\n",
    "\n",
    "# make data numerical\n",
    "def convert_strings_to_ints(data, conversion_dict):\n",
    "    return np.vectorize(conversion_dict.get)(data)\n",
    "\n",
    "for cat, dic in zip(cats, dicts):\n",
    "\n",
    "    data[cat] = xr.apply_ufunc(\n",
    "        convert_strings_to_ints,  # function to apply\n",
    "        data[cat],      # input data\n",
    "        kwargs={'conversion_dict': dic},  # additional arguments\n",
    "        vectorize=True            # vectorize the function\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate dates \n",
    "earliest_date = max(min(outlooks['DATE']), min(pph['time']), min(reports['DATE'])) \n",
    "latest_date = min(max(outlooks['DATE']), max(pph['time']), max(reports['DATE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_date = '200203300000'\n",
    "all_pph_dates = data['time']\n",
    "pph_dates = all_pph_dates[all_pph_dates <= latest_date]\n",
    "pph_dates = pph_dates[pph_dates >= earliest_date]\n",
    "new_data = data.sel(time = pph_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick subset\n",
    "new_data = new_data.where(new_data['MAX_CAT'] >= 4, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataset(dataset):\n",
    "    standardized_data = {}\n",
    "    means = []\n",
    "    stds = []\n",
    "\n",
    "    for var in dataset.data_vars:\n",
    "        data_array = dataset[var]\n",
    "        mean = data_array.values.mean()\n",
    "        means.append(mean)\n",
    "        std = data_array.values.std()\n",
    "        stds.append(std)\n",
    "        standardized_data[var] = (data_array - mean) / std\n",
    "    return xr.Dataset(standardized_data), means, stds\n",
    "\n",
    "def unstandardize_dataset(data, means, stds):\n",
    "    var_names = list(new_data.keys())\n",
    "    for i, var in zip(range(len(var_names)), var_names):\n",
    "        data[var] = data[var] * stds[i] + means[i]\n",
    "    return data\n",
    "\n",
    "new_data, means, stds = standardize_dataset(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do pca\n",
    "data_matrix = new_data.to_array(dim = 'time').data.T\n",
    "pca = PCA()\n",
    "\n",
    "pca.fit(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print variance expained and componenets of first PCs\n",
    "plt.plot(pca.explained_variance_/np.sum(pca.explained_variance_))\n",
    "plt.title('Fraction of Variance Explained by Each PC')\n",
    "keys = list(new_data.keys())\n",
    "for i in range(6):\n",
    "    print('PC ' + str(i))\n",
    "    pc = pca.components_[i, :]\n",
    "    for j in range(len(pc)):\n",
    "        print(keys[j] + ': ' + str(pc[j]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(centers, keys, cluster_method = 'METHOD', xkey = 'LON_NUM', xstring = '', ykey = 'LAT_NUM', ystring = '', sizekey = None, sizestring = 'XX', shapekey = None, shapestring = 'XX', colorkey = None, colorstring = 'XX', fill_cmap = 'viridis', edgekey = None, edgestring = 'XX', edge_cmap = 'viridis', save = False, show = True, save_location = 'plots/clustering/'):\n",
    "    fig=plt.figure(figsize=(15,6))\n",
    "    plt.style.use('dark_background')\n",
    "    is_map = xkey == 'LON_NUM' and ykey == 'LAT_NUM'\n",
    "    if is_map:\n",
    "        ax = plt.axes(projection = cp.crs.LambertConformal())\n",
    "        ax.add_feature(cp.feature.LAND,facecolor='grey')\n",
    "        ax.add_feature(cp.feature.OCEAN, alpha = 0.5)\n",
    "        ax.add_feature(cp.feature.COASTLINE,linewidth=0.5)\n",
    "        ax.add_feature(cp.feature.LAKES, alpha = 0.5)\n",
    "        ax.add_feature(cp.feature.STATES,linewidth=0.5)\n",
    "        transform = cp.crs.PlateCarree()\n",
    "    else:\n",
    "        ax = plt.axes()\n",
    "        transform = None\n",
    "\n",
    "    lats = np.empty(centers.shape[0])\n",
    "    lons = np.empty(centers.shape[0])\n",
    "    sizes = np.empty(centers.shape[0]) # pick a scale\n",
    "    shapes = np.empty(centers.shape[0]) # will do pointier for higher numbers\n",
    "    colors = np.empty(centers.shape[0]) # pick a scale\n",
    "    edges = np.empty(centers.shape[0]) # pick a scale\n",
    "\n",
    "    for i in range(centers.shape[0]):\n",
    "        center = centers[i, :]\n",
    "        for j in range(len(center)):\n",
    "            keyj = keys[j]\n",
    "            centerj = center[j]\n",
    "            #print(keyj + ': ' + str(centerj))\n",
    "            if keyj == ykey:\n",
    "                lats[i] = centerj\n",
    "            elif keyj == xkey:\n",
    "                lons[i] = centerj\n",
    "            elif keyj == sizekey:\n",
    "                sizes[i] = centerj\n",
    "            elif keyj == shapekey:\n",
    "                shapes[i] = centerj\n",
    "            elif keyj == colorkey:\n",
    "                colors[i] = centerj\n",
    "            elif keyj == edgekey:\n",
    "                edges[i] = centerj\n",
    "\n",
    "    # PLOTTING    \n",
    "\n",
    "    # Dynamic binning for shapes\n",
    "    num_bins = 5  # Number of shapes\n",
    "    bins = np.linspace(min(shapes), max(shapes), num_bins)  # Corrected binning\n",
    "    shape_dict = {0: 'o', 1: 'H', 2: 's', 3: 'P', 4: '*'}  # Circle, Square, Diamond, Triangle\n",
    "\n",
    "    # Bin the shapes variable\n",
    "    shape_bins = np.digitize(shapes, bins) - 1  # -1 to match dictionary keys\n",
    "\n",
    "    # Normalize colors and edge colors\n",
    "    norm_colors = plt.Normalize(vmin=min(colors), vmax=max(colors))\n",
    "    norm_edge_colors = plt.Normalize(vmin=min(edges), vmax=(max(edges)))\n",
    "\n",
    "    # Convert to RGBA using colormap\n",
    "    colors_rgba = plt.cm.get_cmap(fill_cmap)(norm_colors(colors))\n",
    "    edge_colors_rgba = plt.cm.get_cmap(edge_cmap)(norm_edge_colors(edges))\n",
    "\n",
    "    # Plot each shape separately based on the binned shapes\n",
    "    for bin_idx in np.unique(shape_bins):\n",
    "        ix = shape_bins == bin_idx\n",
    "        if is_map:\n",
    "            ax.scatter(\n",
    "                lons[ix], lats[ix], \n",
    "                s=sizes[ix], \n",
    "                c=colors[ix], \n",
    "                edgecolor=edge_colors_rgba[ix],  # Use RGBA colors for edge colors\n",
    "                linewidth=3,  \n",
    "                marker=shape_dict[bin_idx], \n",
    "                cmap=fill_cmap, \n",
    "                transform=transform,\n",
    "                alpha=1\n",
    "            )\n",
    "        else:\n",
    "            ax.scatter(\n",
    "                lons[ix], lats[ix], \n",
    "                s=sizes[ix], \n",
    "                c=colors[ix], \n",
    "                edgecolor=edge_colors_rgba[ix],  # Use RGBA colors for edge colors\n",
    "                linewidth=3,  \n",
    "                marker=shape_dict[bin_idx], \n",
    "                cmap=fill_cmap,\n",
    "                alpha=1\n",
    "            )\n",
    "\n",
    "    # put numbers on top\n",
    "    for i, (xi, yi) in enumerate(zip(lons, lats)):\n",
    "        if is_map:\n",
    "            ax.scatter(xi, yi, marker=f\"${i}$\", s=60, c = 'white', transform = transform)\n",
    "        else: \n",
    "            ax.scatter(xi, yi, marker=f\"${i}$\", s=60, c = 'white')\n",
    "\n",
    "    if is_map:\n",
    "        ax.set_xlim(-2400000, 2300000)\n",
    "        ax.set_ylim(-1700000, 1800000)\n",
    "    else:\n",
    "        xbuf = (max(lons) - min(lons))/10\n",
    "        ybuf = (max(lats) - min(lats))/10\n",
    "        ax.set_xlim(min(lons) - xbuf, max(lons) + xbuf)\n",
    "        ax.set_ylim(min(lats) - ybuf, max(lats) + ybuf)\n",
    "\n",
    "    # Add the first colorbar for internal colors\n",
    "    cbar1 = plt.colorbar(ax.collections[0], ax=ax)\n",
    "    cbar1.set_label(colorstring + ' (Fill)')\n",
    "\n",
    "    # Create a dummy scatter plot for the edge colors colorbar\n",
    "    scatter_dummy = ax.scatter(\n",
    "        [], [], c=[], edgecolor=edge_colors_rgba, cmap=edge_cmap, alpha=1\n",
    "    )\n",
    "\n",
    "    # Add the second colorbar for edge colors\n",
    "    cbar2 = plt.colorbar(scatter_dummy, ax=ax)\n",
    "    cbar2.set_label(edgestring + ' (Edge)')\n",
    "\n",
    "    # Dynamic size legend with gray points only\n",
    "    unique_sizes = np.percentile(sizes, [10, 30, 50, 70, 90])\n",
    "    for size in unique_sizes:\n",
    "        plt.scatter([], [], s=size, color='gray', label=f'{int(size)} ' + sizestring, edgecolor='gray')\n",
    "\n",
    "    # Dynamic shape legend with gray shapes only\n",
    "    for bin_idx, shape in shape_dict.items():\n",
    "        plt.scatter([], [], marker=shape, color='gray', label=f'{bins[bin_idx]:.3} ' + shapestring, edgecolor='gray')\n",
    "\n",
    "    # Combine size and shape legends into one and position it further to the right of the colorbars\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    legend = ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1.6, 0.5), title=\"Size & Shape Legend\", frameon=False)\n",
    "\n",
    "    plt.title('Cluster Centers with ' + cluster_method)\n",
    "    if not is_map:\n",
    "        plt.xlabel(xstring)\n",
    "        plt.ylabel(ystring)\n",
    "    \n",
    "    if save:\n",
    "        if is_map:\n",
    "            plt.savefig(save_location + cluster_method + '/clusters.png')\n",
    "        else:\n",
    "            plt.savefig(save_location + cluster_method + '/' + xkey + ykey + '_clusters.png')\n",
    "    if not show:\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster with, k-means, knn, optics, birch, dbscan look here for use cases https://scikit-learn.org/stable/modules/clustering.html\n",
    "# Will want to come up with way to visualize: put location on map, color code by other value(s)? Then do similar for many clustering methods\n",
    "\n",
    "save = True\n",
    "\n",
    "# Define constants for plots\n",
    "sizekey='REPORT_NUM'\n",
    "sizestring = 'Total Reports'\n",
    "colorkey= 'NEIGH_NUM'\n",
    "colorstring = 'Neighborhood Brier Score'\n",
    "fill_cmap = 'turbo'\n",
    "shapekey = 'TOR_F'\n",
    "shapestring = 'Max Tornado Rating'\n",
    "edgekey = 'SEASON'\n",
    "edgestring = 'Season'\n",
    "edge_cmap = 'gist_earth'\n",
    "\n",
    "\n",
    "\n",
    "clustering_algorithms = {\n",
    "    'KMeans': KMeans,\n",
    "    'OPTICS': OPTICS,\n",
    "    #'DBSCAN': DBSCAN,\n",
    "    'AgglomerativeClustering': AgglomerativeClustering,\n",
    "    'MeanShift': MeanShift,\n",
    "    'Birch': Birch,\n",
    "    'SpectralClustering': SpectralClustering,\n",
    "    'SOM': SOM\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster by one or two characteristics, show all characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_partial(dataset, cluster_vars, clustering_algorithms):\n",
    "    # clusters data in given variables and returns dictionary (by cluster method) of lists of datasets of points in each cluster\n",
    "    new_data, means, stds = standardize_dataset(dataset)\n",
    "    \n",
    "    ret = {key: [] for key in clustering_algorithms}\n",
    "\n",
    "    X = new_data[cluster_vars].to_array(dim = 'time').data.T\n",
    "    for name, cls in clustering_algorithms.items():\n",
    "        print(name)\n",
    "        datasets = []\n",
    "        if name == 'SOM':\n",
    "            labels = cls(dim = X.shape[1]).fit_predict(X)\n",
    "        else:\n",
    "            labels = cls().fit_predict(X)\n",
    "        for cluster_num in set(labels):\n",
    "            datasets.append(dataset.sel(time=dataset['time'][labels == cluster_num]))\n",
    "        ret[name] = datasets\n",
    "    return(ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstandardized_data = data.sel(time = pph_dates)\n",
    "unstandardized_data = unstandardized_data.where(unstandardized_data['MAX_CAT'] >= 4, drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_plot_dict = {\n",
    "    '-1.0': -1,\n",
    "    '0.0': 0,\n",
    "    '1.0': 1,\n",
    "    '2.0': 2,\n",
    "    '3.0': 3,\n",
    "    '4.0': 4,\n",
    "    '5.0': 5\n",
    "}\n",
    "\n",
    "season_plot_dict = {\n",
    "    '0.0': 0,\n",
    "    '1.0': 1,\n",
    "    '2.0': 2,\n",
    "    '3.0': 3\n",
    "}\n",
    "\n",
    "ramp_up_plot_dict = {\n",
    "    '0.0': 0,\n",
    "    '1.0': 1,\n",
    "    '2.0': 2,\n",
    "    '3.0': 3,\n",
    "    '4.0': 4,\n",
    "    '5.0': 5,\n",
    "    '6.0': 6\n",
    "}\n",
    "\n",
    "ramp_down_plot_dict = {\n",
    "    '0.0': 0,\n",
    "    '-1.0': 1,\n",
    "    '-2.0': 2,\n",
    "    '-3.0': 3,\n",
    "    '-4.0': 4,\n",
    "    '-5.0': 5,\n",
    "    '-6.0': 6\n",
    "}\n",
    "\n",
    "tor_plot_dict = {\n",
    "    '-1.0': -1,\n",
    "    '0.0': 0,\n",
    "    '1.0': 1,\n",
    "    '2.0': 2,\n",
    "    '3.0': 3,\n",
    "    '4.0': 4,\n",
    "    '5.0': 5,\n",
    "    '6.0': 6\n",
    "}\n",
    "\n",
    "\n",
    "written_labels = ['Categorical Risk', 'Ramp Up', 'Ramp Down', 'Season',\n",
    "                  'Total Storm Reports', 'Tornado Reports', 'Wind Reports', 'Hail Reports',\n",
    "                  'Max Tornado Rating', 'Max Wind Speed (kt)', 'Max Hail Size (in)',\n",
    "                  'Neighborhood Brier Score', 'RMSE', 'Brier Score',\n",
    "                  'Max PPH',\n",
    "                  'POD', 'FAR',\n",
    "                  'Hail POD', 'Hail FAR',\n",
    "                  'Wind POD', 'Wind FAR',\n",
    "                  'Tornado POD', 'Tornado FAR',\n",
    "                  'FARate', 'Hail FARate', 'Wind FARate', 'Tornado FARate',\n",
    "                  'East Shift', 'North Shift', 'Divergence',\n",
    "                  'Wind East Shift', 'Wind North Shift', 'Wind Divergence',\n",
    "                  'Hail East Shift', 'Hail North Shift', 'Hail Divergence',\n",
    "                  'Tornado East Shift', 'Tornado North Shift', 'Tornado Divergence']\n",
    "graphing_dicts = [category_plot_dict, ramp_up_plot_dict, ramp_down_plot_dict, season_plot_dict, \n",
    "                  None, None, None, None, \n",
    "                  tor_plot_dict, None, None,\n",
    "                  None, None, None,\n",
    "                  None,\n",
    "                  None, None,\n",
    "                  None, None,\n",
    "                  None, None,\n",
    "                  None, None,\n",
    "                  None, None, None, None,\n",
    "                  None, None, None,\n",
    "                  None, None, None,\n",
    "                  None, None, None,\n",
    "                  None, None, None]\n",
    "steps = [1, 1, 1, 1,\n",
    "         100, 25, 100, 50,\n",
    "         1, 10, 1,\n",
    "         .001, .01, .0025,\n",
    "         10,\n",
    "         .05, .1, \n",
    "         .05, .1,\n",
    "         .05, .1,\n",
    "         .05, .1, \n",
    "         .001, .001, .001, .001,\n",
    "         100000, 100000, .5,\n",
    "         100000, 100000, .5,\n",
    "         100000, 100000, .5,\n",
    "         100000, 100000, .5]\n",
    "\n",
    "def plot_cluster_distribution(pph, label_1, label_2, label_1_string, label_2_string, dict_1, dict_2, titlestring, save_location, show=False, defaultbins = 10, step_1 = 1, step_2 = 1):\n",
    "# plot 2d heatmap for any 2 labels\n",
    "\n",
    "    data1 = []\n",
    "    data2 = []\n",
    "\n",
    "    for i in range(len(pph[label_1])):\n",
    "        if dict_1 != None:\n",
    "            data1.append(dict_1[str(pph[label_1].values[i])])\n",
    "        else: \n",
    "            data1.append(pph[label_1].values[i])\n",
    "        if dict_2 != None:\n",
    "            data2.append(dict_2[str(pph[label_2].values[i])])\n",
    "        else:\n",
    "            data2.append(pph[label_2].values[i])\n",
    "\n",
    "    \n",
    "    if dict_1 != None:\n",
    "        max1 = max(dict_1.values())\n",
    "        min1 = min(dict_1.values())\n",
    "        bins1 = np.linspace(min1-.5, max1+.5, 2+max1-min1)\n",
    "        irange = bins1.size-1\n",
    "    else:\n",
    "        if ((max(data1) - min(data1))/step_1).is_integer():\n",
    "            m = 2\n",
    "        else: \n",
    "            m = 1\n",
    "            \n",
    "        bins1 = np.arange(step_1 * round(min(data1)/step_1), max(data1)+m*step_1, step_1)\n",
    "        \n",
    "        num_bins1 = len(bins1)-1\n",
    "        irange = num_bins1\n",
    "\n",
    "    if dict_2 != None:\n",
    "        max2 = max(dict_2.values())\n",
    "        min2 = min(dict_2.values())\n",
    "        bins2 = np.linspace(min2-.5, max2+.5, 2+max2-min2)\n",
    "        jrange = bins2.size-1\n",
    "    else:\n",
    "        if ((max(data2) - min(data2))/step_2).is_integer():\n",
    "            m = 2\n",
    "        else: \n",
    "            m = 1\n",
    "\n",
    "        if min(data2) < 0:\n",
    "            bins2 = np.arange(step_2 * round(min(data2)/step_2), max(data2)+m*step_2, step_2)\n",
    "        else:\n",
    "            bins2 = np.insert(np.arange(step_2 * round(min(data2)/step_2), max(data2)+m*step_2, step_2), 0, 0)\n",
    "\n",
    "            if isinstance(data2[1], numbers.Integral):\n",
    "                bins2[1] = bins2[1] + 1\n",
    "            else:\n",
    "                bins2[1] = bins2[1] + .00000000001\n",
    "        num_bins2 = len(bins2)-1\n",
    "        jrange = num_bins2\n",
    "\n",
    "    heatmap = np.histogram2d(data1, data2, bins = (bins1, bins2))\n",
    "    im = plt.imshow(heatmap[0],  norm=colors.LogNorm())\n",
    "    plt.colorbar(im)\n",
    "\n",
    "    for i in range(irange): \n",
    "        for j in range(jrange): \n",
    "            plt.annotate(str(int(heatmap[0][i][j])), xy=(j, i), \n",
    "                        ha='center', va='center', color='black') \n",
    "\n",
    "    if dict_1 != None:\n",
    "        plt.yticks(range(len(list(dict_1.values()))), labels=list(dict_1.keys()))\n",
    "    else:\n",
    "        labels = [s for s in heatmap[1][range(num_bins1)].astype(str)]\n",
    "        plt.yticks(range(num_bins1), labels = labels) \n",
    "        plt.yticks(fontsize=8)\n",
    "    if dict_2 != None:\n",
    "        plt.xticks(range(len(list(dict_2.values()))), labels=list(dict_2.keys()))\n",
    "    else:\n",
    "        labels = [s + '+' for s in heatmap[2][range(num_bins2)].astype(str)]\n",
    "        if min(data2) >= 0:\n",
    "            labels[0] = '0'\n",
    "            if isinstance(data2[1], numbers.Integral):\n",
    "                labels[1] = '1+'\n",
    "            else:\n",
    "                labels[1] = '0+'\n",
    "        plt.xticks(range(num_bins2), labels = labels)\n",
    "        plt.xticks(fontsize=8)\n",
    "\n",
    "    plt.ylabel(label_1_string)\n",
    "    plt.xlabel(label_2_string)\n",
    "\n",
    "    plt.title(label_2_string + ' by Cluster with ' + titlestring + ' Clustering')\n",
    "    if save_location != None:\n",
    "        plt.savefig(save_location + '/' + label_1_string + '_' + label_2_string + '_distribution.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_clusters(d, savepath, scatter_nonmap = False,\n",
    "                       showmap = True, savemap = True, \n",
    "                       xkey = 'LON_NUM', xstring = '', ykey = 'LAT_NUM', ystring = '', sizekey='REPORT_NUM', sizestring = 'Total Reports', colorkey= 'NEIGH_NUM', colorstring = 'Neighborhood Brier Score', fill_cmap = 'turbo', shapekey = 'TOR_F', shapestring = 'Max Tornado Rating', edgekey = 'SEASON', edgestring = 'Season', edge_cmap = 'gist_earth',\n",
    "                       showgraph = True, savegraph = True, written_labels = written_labels, graphing_dicts = graphing_dicts, steps = steps,\n",
    "                       showtext = True, savetext = True):\n",
    "    \n",
    "    \n",
    "    for name in d.keys():\n",
    "        print(name)\n",
    "        datasets = d[name]\n",
    "\n",
    "        if savemap or savegraph or savetext:\n",
    "            if not os.path.exists(savepath + name):\n",
    "                os.makedirs(savepath + name)\n",
    "        \n",
    "        keys = list(datasets[0].keys())\n",
    "        centers = np.empty((len(datasets), len(keys)))\n",
    "\n",
    "        for dataset, i in zip(datasets, range(len(datasets))):\n",
    "            center = dataset.to_array(dim = 'time').data.mean(axis = 1)\n",
    "            centers[i, :] = center\n",
    "\n",
    "        if showgraph or savegraph:\n",
    "            # make graph: 2d heatmap, cluster number is one dimension and data variable is other dimension. Separate graph for each variable, so iterate over keys?\n",
    "            # combine datasets into one dataset with cluster number as separate column\n",
    "            indexed_datasets = []\n",
    "            for i, ds in enumerate(datasets):\n",
    "                # Add a new variable 'cluster' to record the index\n",
    "                ds = ds.assign(cluster=i)\n",
    "                indexed_datasets.append(ds)\n",
    "\n",
    "            # Combine all datasets along the common dimension\n",
    "            combined_dataset = xr.concat(indexed_datasets, dim='time')\n",
    "\n",
    "            # call plot_distribution_2d\n",
    "            if savegraph == False:\n",
    "                sp = None\n",
    "            else: \n",
    "                sp = savepath + name \n",
    "\n",
    "            plot_keys = keys.copy()\n",
    "            removekeys = ['LAT_NUM', 'LON_NUM', 'HR_NUM', 'HR_H_NUM', 'HR_W_NUM', 'HR_T_NUM']\n",
    "            for k in removekeys:\n",
    "                plot_keys.remove(k)\n",
    "\n",
    "            for i in range(len(plot_keys)):\n",
    "                plot_cluster_distribution(combined_dataset, 'cluster', plot_keys[i], 'Cluster', written_labels[i], None, graphing_dicts[i], name, sp, show = showgraph, step_2 = steps[i])\n",
    "\n",
    "        if showtext or savetext:\n",
    "            result_string = ''\n",
    "            for i in range(centers.shape[0]):\n",
    "                result_string += '\\nCluster ' + str(i) + ':'\n",
    "                for j in range(centers.shape[1]):\n",
    "                    result_string += '\\n' + str(keys[j]) + ': ' + str(centers[i, j])\n",
    "            \n",
    "            if savetext:\n",
    "                path = savepath+name\n",
    "                if not os.path.exists(path):\n",
    "                    os.makedirs(path)\n",
    "                with open(savepath + name + '/centers.txt', \"w\") as file:\n",
    "                    file.write(result_string)\n",
    "\n",
    "            if showtext:\n",
    "                print(result_string)\n",
    "\n",
    "        if showmap or savemap:\n",
    "            plot_clusters(centers, keys, save_location = savepath, save = savemap, show = showmap, cluster_method = name, sizekey = sizekey, sizestring = sizestring, colorkey = colorkey, colorstring = colorstring, fill_cmap = fill_cmap, shapekey = shapekey, shapestring = shapestring, edgekey = edgekey, edgestring = edgestring, edge_cmap = edge_cmap)\n",
    "            if scatter_nonmap:\n",
    "                plot_clusters(centers, keys, save_location = savepath,  save = savemap, show = showmap, cluster_method = name, xkey = xkey, xstring = xstring, ykey = ykey, ystring = ystring, sizekey = sizekey, sizestring = sizestring, colorkey = colorkey, colorstring = colorstring, fill_cmap = fill_cmap, shapekey = shapekey, shapestring = shapestring, edgekey = edgekey, edgestring = edgestring, edge_cmap = edge_cmap)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = cluster_partial(unstandardized_data, ['LAT_NUM', 'LON_NUM'], clustering_algorithms)\n",
    "#summarize_clusters(d, 'plots/clustering/latlon/', showmap = False, showtext = False, showgraph=False)\n",
    "\n",
    "#d = cluster_partial(unstandardized_data, list(unstandardized_data.keys()), clustering_algorithms)\n",
    "#summarize_clusters(d, 'plots/clustering/full/', showmap = False, showtext = False, showgraph = False)\n",
    "\n",
    "#d = cluster_partial(unstandardized_data, ['NEIGH_NUM'], clustering_algorithms)\n",
    "#summarize_clusters(d, 'plots/clustering/nbs/', showmap = False, showtext = False, showgraph = False)\n",
    "\n",
    "#d = cluster_partial(unstandardized_data, ['PPH_D_NUM'], clustering_algorithms)\n",
    "#summarize_clusters(d, 'plots/clustering/pph/', showmap = False, showtext = False, showgraph = False)\n",
    "\n",
    "#d = cluster_partial(unstandardized_data, ['POD_NUM', 'FAR_NUM'], clustering_algorithms)\n",
    "#summarize_clusters(d, 'plots/clustering/podfar/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'POD_NUM', ykey = 'FAR_NUM', xstring = 'POD', ystring = 'FAR')\n",
    "\n",
    "d = cluster_partial(unstandardized_data, ['E_SH_NUM', 'N_SH_NUM'], clustering_algorithms)\n",
    "summarize_clusters(d, 'plots/clustering/all_hazard_shifts/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'E_SH_NUM', ykey = 'N_SH_NUM', xstring = 'East Shift', ystring = 'North Shift')\n",
    "\n",
    "d = cluster_partial(unstandardized_data, ['E_SH_NUM', 'N_SH_NUM', 'DIV_NUM'], clustering_algorithms)\n",
    "summarize_clusters(d, 'plots/clustering/all_hazard_shifts_div/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'E_SH_NUM', ykey = 'N_SH_NUM', xstring = 'East Shift', ystring = 'North Shift', colorkey='DIV_NUM', colorstring = 'Divergence')\n",
    "\n",
    "d = cluster_partial(unstandardized_data, ['E_SH_W_NUM', 'N_SH_W_NUM', 'DIV_W_NUM'], clustering_algorithms)\n",
    "summarize_clusters(d, 'plots/clustering/wind_all_hazard_shifts_div/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'E_SH_W_NUM', ykey = 'N_SH_W_NUM', xstring = 'East Shift', ystring = 'North Shift', colorkey='DIV_W_NUM', colorstring = 'Divergence')\n",
    "\n",
    "d = cluster_partial(unstandardized_data, ['E_SH_H_NUM', 'N_SH_H_NUM', 'DIV_H_NUM'], clustering_algorithms)\n",
    "summarize_clusters(d, 'plots/clustering/hail_all_hazard_shifts_div/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'E_SH_H_NUM', ykey = 'N_SH_H_NUM', xstring = 'East Shift', ystring = 'North Shift', colorkey='DIV_H_NUM', colorstring = 'Divergence')\n",
    "\n",
    "d = cluster_partial(unstandardized_data, ['E_SH_T_NUM', 'N_SH_T_NUM', 'DIV_T_NUM'], clustering_algorithms)\n",
    "summarize_clusters(d, 'plots/clustering/tor_all_hazard_shifts_div/', showmap = False, showtext = False, showgraph = False, scatter_nonmap= True, xkey = 'E_SH_T_NUM', ykey = 'N_SH_T_NUM', xstring = 'East Shift', ystring = 'North Shift', colorkey='DIV_T_NUM', colorstring = 'Divergence')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
