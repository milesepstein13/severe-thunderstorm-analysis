{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import cartopy as cp\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from scipy.ndimage import map_coordinates\n",
    "import metpy.calc as mpcalc\n",
    "from geographiclib.geodesic import Geodesic\n",
    "from utils_datetime import *\n",
    "from utils_filter import *\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_outlooks = xr.open_dataset('data/outlooks/grid_outlooks.nc')\n",
    "grid_outlooks = grid_outlooks.sel(time = grid_outlooks['time'] >= '200203300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates new displacements dataset by keeping data (get rid of shifts and div) \n",
    "# but changing x and y coords so that (0, 0) is at max or weighted center of outlook (or pph?). Since highest risk is seen as \"center\" of what will happen, even if weaker storms are focused to one direction\n",
    "# then calculate things like shifts of quadrents\n",
    "\n",
    "# note: grids will not quite be aligned when each is centered over a different point, but probably close enough\n",
    "\n",
    "\n",
    "# finding x and y to make center for each date\n",
    "grouped = grid_outlooks['prob'].groupby('time')\n",
    "\n",
    "# Step 1: Find all points with the maximum prob for each day and compute mean coordinates\n",
    "def find_mean_coords(group):\n",
    "    max_prob = group.max()  # Maximum value in the group\n",
    "    if max_prob == 0:\n",
    "        mean_x = group['x'].mean().item()\n",
    "        mean_y = group['y'].mean().item()\n",
    "    else:\n",
    "        # Select all points with prob == max_prob\n",
    "        max_points = group.where(group == max_prob, drop=True)\n",
    "        # Compute the mean of x and y\n",
    "        mean_x = max_points['x'].mean().item()\n",
    "        mean_y = max_points['y'].mean().item()\n",
    "    # Round to nearest integers\n",
    "    nearest_x = round(mean_x)\n",
    "    nearest_y = round(mean_y)\n",
    "    # Return as a Dataset\n",
    "    return xr.Dataset({'nearest_x': nearest_x, 'nearest_y': nearest_y})\n",
    "\n",
    "# Apply the function to each group\n",
    "center_coords = grouped.map(find_mean_coords)\n",
    "center_coords\n",
    "\n",
    "del grid_outlooks, grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract nearest_x and nearest_y\n",
    "nearest_x = center_coords['nearest_x']\n",
    "nearest_y = center_coords['nearest_y']\n",
    "\n",
    "\n",
    "# Re-center displacements\n",
    "def recenter_displacements(displacements, nearest_x, nearest_y):\n",
    "    # Shift x and y coordinates based on nearest_x, nearest_y for each time\n",
    "    new_x = displacements['x'] - nearest_x\n",
    "    new_y = displacements['y'] - nearest_y\n",
    "    \n",
    "    # Assign shifted coordinates\n",
    "    displacements = displacements.assign_coords({\n",
    "        'x': new_x,\n",
    "        'y': new_y\n",
    "    })\n",
    "    \n",
    "    # Update lat and lon to depend on time, x, and y\n",
    "    \n",
    "    return displacements\n",
    "# Initialize an empty list to store the recentered displacements\n",
    "\n",
    "\n",
    "\n",
    "for dataset_location in ['data/pph/labelled_pph', 'data/displacement/displacements', 'data/outlooks/grid_outlooks']:\n",
    "    print(dataset_location)\n",
    "    recentered_list = []\n",
    "    ds = xr.open_dataset(dataset_location + '.nc')\n",
    "    ds = ds.sel(time = ds['time'] >= '200203300000')\n",
    "\n",
    "    # Loop over each time step and apply recenter_displacements\n",
    "    for t in range(len(ds.time)):\n",
    "        # Get the current time slice of displacements\n",
    "        displacement_slice = ds.isel(time=t)\n",
    "\n",
    "        # Get the corresponding nearest_x and nearest_y for this time step\n",
    "        nearest_x_t = nearest_x.isel(time=t)\n",
    "        nearest_y_t = nearest_y.isel(time=t)\n",
    "\n",
    "        # Apply the recentering function to the current time slice\n",
    "        recentered_t = recenter_displacements(displacement_slice, nearest_x_t, nearest_y_t)\n",
    "\n",
    "        # expanding needs to be done differently\n",
    "\n",
    "        # Append the recentered displacements for this time slice\n",
    "        recentered_list.append(recentered_t)\n",
    "    print('combining')\n",
    "    # Combine the recentered displacements back into a single xarray object\n",
    "    recentered = xr.concat(recentered_list, dim='time')\n",
    "    # Ensure proper ordering of dimensions\n",
    "    #recentered = recentered.transpose('time', 'y', 'x', 'hazard')\n",
    "    recentered.to_netcdf('~/recentered_data/' + dataset_location.split('/')[-1] + '_recentered.nc') \n",
    "    del recentered, recentered_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open pph, outlook, and displacements\n",
    "#test_time = ['201104270000', '201905310000']\n",
    "# do something like how shifts and divergence are calculated but in each quadrent\n",
    "\n",
    "pph_recentered = xr.open_dataset('~/recentered_data/labelled_pph_recentered.nc')\n",
    "outlooks_recentered = xr.open_dataset('~/recentered_data/grid_outlooks_recentered.nc')\n",
    "displacements_recentered = xr.open_dataset('~/recentered_data/displacements_recentered.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_types= ['Wind', 'Hail', 'Tornado', 'All Hazard']\n",
    "\n",
    "pph_key_dict = {\n",
    "    'Wind': 'p_perfect_wind',\n",
    "    'Hail': 'p_perfect_hail',\n",
    "    'Tornado': 'p_perfect_tor',\n",
    "    'All Hazard': 'p_perfect_totalsvr'\n",
    "}\n",
    "\n",
    "outlook_key_dict = {\n",
    "    'Wind': 'Day 1 Wind',\n",
    "    'Hail': 'Day 1 Hail',\n",
    "    'Tornado': 'Day 1 Tornado',\n",
    "    'All Hazard': 'Day 1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "displacements_recentered = displacements_recentered.assign(e_shift_n = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(n_shift_n = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(total_div_n = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "\n",
    "displacements_recentered = displacements_recentered.assign(e_shift_e = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(n_shift_e = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(total_div_e = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "\n",
    "displacements_recentered = displacements_recentered.assign(e_shift_s = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(n_shift_s = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(total_div_s = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "\n",
    "displacements_recentered = displacements_recentered.assign(e_shift_w = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(n_shift_w = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "displacements_recentered = displacements_recentered.assign(total_div_w = (('time', 'hazard'), np.full((len(displacements_recentered['time']), len(hazard_types)), 0.0)))\n",
    "\n",
    "for side in ['n', 'e', 's', 'w']:\n",
    "    print(side)\n",
    "    for hazard in hazard_types:\n",
    "\n",
    "        print(hazard)\n",
    "\n",
    "        e_shifts = []\n",
    "        n_shifts = []\n",
    "        total_divs = []\n",
    "\n",
    "        hazard_dataset = displacements_recentered.sel(hazard = hazard)\n",
    "        for date in displacements_recentered['time']:\n",
    "\n",
    "            weights = outlooks_recentered.sel(time = date, outlook = outlook_key_dict[hazard])['prob']\n",
    "            if side == 'n':\n",
    "                mask = weights.y >= 0\n",
    "            elif side == 'e':\n",
    "                mask = weights.x >= 0\n",
    "            elif side == 's':\n",
    "                mask = weights.y <= 0\n",
    "            elif side == 'w':\n",
    "                mask = weights.x <= 0\n",
    "            weights = weights.where(mask, 0).fillna(0).data\n",
    "            \n",
    "            if weights.max() == 0: # no outlook, so weight at pph\n",
    "                weights = pph_recentered.sel(time = date)[pph_key_dict[hazard]].where(mask, 0).fillna(0).data\n",
    "            if weights.max() == 0:\n",
    "                weights = None\n",
    "            hazard_time_dataset = hazard_dataset.sel(time = date)\n",
    "            e_shift = np.average(hazard_time_dataset['e_flow'].fillna(0), weights = weights)\n",
    "            n_shift = np.average(hazard_time_dataset['n_flow'].fillna(0), weights = weights)\n",
    "            div = np.gradient(hazard_time_dataset['x_flow'].fillna(0))[1] + np.gradient(hazard_time_dataset['y_flow'].fillna(0))[0]\n",
    "            total_div = np.average(div, weights = weights)\n",
    "\n",
    "            displacements_recentered['e_shift_' + side].loc[dict(time = date, hazard = hazard)] = e_shift\n",
    "            displacements_recentered['n_shift_' + side].loc[dict(time = date, hazard = hazard)] = n_shift\n",
    "            displacements_recentered['total_div_' + side].loc[dict(time = date, hazard = hazard)] = total_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacements_recentered.to_netcdf('~/recentered_data/displacements_recentered.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('~/recentered_data/displacements_recentered.nc')\n",
    "pph = xr.open_dataset('~/recentered_data/labelled_pph_recentered.nc')\n",
    "grid_outlooks = xr.open_dataset('~/recentered_data/grid_outlooks_recentered.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_periods = ['all', '2002_2006', '2007_2011', '2012_2016', '2017_2023']\n",
    "dpi = 1000\n",
    "mdt = False\n",
    "\n",
    "hazard_types= ['All Hazard', 'Wind', 'Hail', 'Tornado']\n",
    "\n",
    "pph_key_dict = {\n",
    "    'Wind': 'p_perfect_wind',\n",
    "    'Hail': 'p_perfect_hail',\n",
    "    'Tornado': 'p_perfect_tor',\n",
    "    'All Hazard': 'p_perfect_totalsvr'\n",
    "}\n",
    "\n",
    "outlook_key_dict = {\n",
    "    'Wind': 'Day 1 Wind',\n",
    "    'Hail': 'Day 1 Hail',\n",
    "    'Tornado': 'Day 1 Tornado',\n",
    "    'All Hazard': 'Day 1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time_period in time_periods:\n",
    "    print(time_period)\n",
    "    if time_period != 'all':\n",
    "        year1 = time_period.split('_')[0]\n",
    "        year2 = time_period.split('_')[1]\n",
    "        this_ds = ds.sel(time = (ds['time'] >= year1 + '01010000') & (ds['time'] <= year2 + '12312359'))\n",
    "        this_pph = pph.sel(time = (pph['time'] >= year1 + '01010000') & (pph['time'] <= year2 + '12312359'))\n",
    "        this_grid_outlooks = grid_outlooks.sel(time = (grid_outlooks['time'] >= year1 + '01010000') & (grid_outlooks['time'] <= year2 + '12312359'))\n",
    "    else:\n",
    "        year1 = '2002'\n",
    "        year2 = '2023'\n",
    "        this_ds = ds\n",
    "        this_pph = pph\n",
    "        this_grid_outlooks = grid_outlooks\n",
    "\n",
    "    mdt_pph = this_pph.sel(time = (this_pph['MAX_CAT'].isin(['MDT', 'HIGH'])))\n",
    "    mdt_ds = this_ds.sel(time = (this_pph['MAX_CAT'].isin(['MDT', 'HIGH'])))\n",
    "    mdt_outlooks = this_grid_outlooks.sel(time = (this_pph['MAX_CAT'].isin(['MDT', 'HIGH'])))\n",
    "\n",
    "    mean_displacements = this_ds.mean(dim = 'time')\n",
    "    mdt_mean_displacements = mdt_ds.mean(dim = 'time')\n",
    "\n",
    "    mean_outlooks = this_grid_outlooks['prob'].mean(dim = 'time')\n",
    "    mdt_mean_outlooks = mdt_outlooks['prob'].mean(dim = 'time')\n",
    "\n",
    "    mean_pph = this_pph[['p_perfect_hail', 'p_perfect_wind', 'p_perfect_tor', 'p_perfect_totalsvr']].mean(dim = 'time')\n",
    "    mdt_mean_pph = mdt_pph[['p_perfect_hail', 'p_perfect_wind', 'p_perfect_tor', 'p_perfect_totalsvr']].mean(dim = 'time')\n",
    "\n",
    "    # plotting composite displacement\n",
    "    print('composite displacement')\n",
    "    s = slice(-20, 20)\n",
    "\n",
    "    if mdt:\n",
    "        mean_displacements_slice = mdt_mean_displacements.sel(x = s, y = s)\n",
    "        mean_outlooks_slice = mdt_mean_outlooks.sel(x = s, y = s)\n",
    "    else:\n",
    "        mean_displacements_slice = mean_displacements.sel(x = s, y = s)\n",
    "        mean_outlooks_slice = mean_outlooks.sel(x = s, y = s)\n",
    "\n",
    "    x, y = np.meshgrid(mean_displacements_slice['x'],  \n",
    "                    mean_displacements_slice['y']) \n",
    "    if mdt:\n",
    "        output_pdf = 'plots/results/colocated/composite_displacements_' + time_period + '.pdf'\n",
    "    else:\n",
    "        output_pdf = 'plots/results/colocated/allcat_composite_displacements_' + time_period + '.pdf'\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        labels = ['a', 'b', 'c', 'd']\n",
    "        fig.suptitle(('Displacement Composites for MDT+ Days ' if mdt else ' Displacement Composite ') + year1 + '-' + year2)\n",
    "\n",
    "        for i, hazard in enumerate(hazard_types):\n",
    "            print(hazard)\n",
    "            \n",
    "            x, y = np.meshgrid(mean_displacements_slice['x'], mean_displacements_slice['y'])\n",
    "            x_flow = mean_displacements_slice.sel(hazard=hazard)['x_flow']\n",
    "            y_flow = mean_displacements_slice.sel(hazard=hazard)['y_flow']\n",
    "            \n",
    "            c = axes[i].contourf(x, y, mean_outlooks_slice.sel(outlook=outlook_key_dict[hazard]) + 0.01,\n",
    "                                levels=[0.02, 0.05, 0.10, 0.15, 0.30, 0.45, 0.60, 1.00], \n",
    "                                alpha=0.5,\n",
    "                                colors=['#008b00', '#8b4726', '#ffc800', '#ff0000', '#ff00ff', '#912cee', '#104e8b'])\n",
    "            axes[i].quiver(x, y, x_flow, y_flow, scale=1, scale_units='xy')\n",
    "            axes[i].set_title(hazard)\n",
    "            axes[i].text(0.02, 1.05, labels[i], transform=axes[i].transAxes, fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "            fig.colorbar(c, ax=axes[i], orientation='vertical', pad=0.01, aspect=50, fraction=0.1)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"PDF saved at {output_pdf}\")\n",
    "\n",
    "    # plotting composite of pph-outlook\n",
    "    print('composite difference')\n",
    "    s = slice(-20, 20)\n",
    "\n",
    "    if mdt:\n",
    "        mean_pph_slice = mdt_mean_pph.sel(x = s, y = s)\n",
    "        mean_outlooks_slice = mdt_mean_outlooks.sel(x = s, y = s)\n",
    "    else:\n",
    "        mean_pph_slice = mean_pph.sel(x = s, y = s)\n",
    "        mean_outlooks_slice = mean_outlooks.sel(x = s, y = s)\n",
    "\n",
    "    x, y = np.meshgrid(mean_pph_slice['x'],  \n",
    "                    mean_pph_slice['y']) \n",
    "    # Set up PDF file\n",
    "    if mdt:\n",
    "        output_pdf = 'plots/results/colocated/composite_pph_outlooks_' + time_period + '.pdf'\n",
    "    else:\n",
    "        output_pdf = 'plots/results/colocated/allcat_composite_pph_outlooks_' + time_period + '.pdf'\n",
    "    with PdfPages(output_pdf) as pdf:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        labels = ['a', 'b', 'c', 'd']\n",
    "\n",
    "        fig.suptitle(('PPH - Outlook Composites on MDT+ Days ' if mdt else 'PPH - Outlook Composites ') + year1 + '-' + year2)\n",
    "\n",
    "        # Iterate through hazards and plot on subplots\n",
    "        for i, hazard in enumerate(hazard_types):\n",
    "            print(hazard)\n",
    "\n",
    "            # Calculate differences and plot filled contours\n",
    "            c = axes[i].contourf(\n",
    "                x, y, mean_pph_slice[pph_key_dict[hazard]] / 100 - mean_outlooks_slice.sel(outlook=outlook_key_dict[hazard]),\n",
    "                levels=np.linspace(-0.12, 0.12, 13), cmap='bwr'\n",
    "            )\n",
    "\n",
    "            # Add outlook contours\n",
    "            axes[i].contour(\n",
    "                x, y, mean_outlooks_slice.sel(outlook=outlook_key_dict[hazard]) + 0.01,\n",
    "                levels=[0.02, 0.05, 0.10, 0.15, 0.30, 0.45, 0.60, 1.00],\n",
    "                colors='black', linestyles='dashed'\n",
    "            )\n",
    "\n",
    "            # Title for each subplot\n",
    "            title = hazard\n",
    "            axes[i].set_title(title)\n",
    "\n",
    "            # Label each subplot (a, b, c, d)\n",
    "            axes[i].text(0.02, 1.05, labels[i], transform=axes[i].transAxes, fontsize=12, fontweight='bold', va='top', ha='left')\n",
    "\n",
    "            # Add colorbar to each subplot\n",
    "            fig.colorbar(c, ax=axes[i], orientation='vertical', pad=0.01, aspect=50, fraction=0.1, extend='both')\n",
    "\n",
    "        # Adjust layout and save to PDF\n",
    "        plt.tight_layout()\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"PDF saved at {output_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(time = '201104270000').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_types= ['Wind', 'Hail', 'Tornado', 'All Hazard']\n",
    "vs = ['e_shift',\n",
    " 'n_shift',\n",
    " 'total_div',\n",
    " 'e_shift_n',\n",
    " 'n_shift_n',\n",
    " 'total_div_n',\n",
    " 'e_shift_e',\n",
    " 'n_shift_e',\n",
    " 'total_div_e',\n",
    " 'e_shift_s',\n",
    " 'n_shift_s',\n",
    " 'total_div_s',\n",
    " 'e_shift_w',\n",
    " 'n_shift_w',\n",
    " 'total_div_w']\n",
    "for hazard in hazard_types:\n",
    "    for v in vs:\n",
    "        ds_hazard = ds.sel(hazard = hazard)[v]\n",
    "        if 'div' in v: \n",
    "            y, _, _ = plt.hist(ds_hazard, bins = 40, range = (-8, 8))\n",
    "        else:\n",
    "            y, _, _ = plt.hist(ds_hazard/1000, bins = 40, range = (-800, 800))\n",
    "        plt.plot([0, 0], [0, y.max()], color='k', linestyle='-')\n",
    "        plt.title(hazard + ' ' + v + ' distribution on MDT+ days since 2002')\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds_hazard, bins = 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
