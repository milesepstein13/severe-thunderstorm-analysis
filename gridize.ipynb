{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open and Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading outlooks 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miles\\OneDrive\\Documents\\UW\\Research\\utils_filter.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  outlooks = outlooks.append(gp.read_file(data_location + '/outlooks/' + mod_string + '_outlooks_2.shp'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pph\n",
      "reading storm reports\n"
     ]
    }
   ],
   "source": [
    "data_location = 'data'\n",
    "outlooks, pph, reports = read_datasets(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_conversions = {'PST': timedelta(hours=8),\n",
    "                  'MST': timedelta(hours=7),\n",
    "                  'CST': timedelta(hours=6),\n",
    "                  'CSt': timedelta(hours=6),\n",
    "                  'CSC': timedelta(hours=6),\n",
    "                  'SCT': timedelta(hours=6),\n",
    "                  'EST': timedelta(hours=5),\n",
    "                  'ESt': timedelta(hours=5),\n",
    "                  'PDT': timedelta(hours=7),\n",
    "                  'MDT': timedelta(hours=6),\n",
    "                  'CDT': timedelta(hours=5),\n",
    "                  'EDT': timedelta(hours=4),\n",
    "                  'HST': timedelta(hours=10),\n",
    "                  'SST': timedelta(hours=11),\n",
    "                  'GST': timedelta(hours=10),\n",
    "                  'AKS': timedelta(hours=9),\n",
    "                  'AST': timedelta(hours=4),\n",
    "                  'UNK': timedelta(hours=5),\n",
    "                  'GMT': timedelta(0)}\n",
    "\n",
    "def get_reports_date_strings(date_times, timezones):\n",
    "    # returns list of strings of date of given datetime and timezone (where day cutoffs are 12-12 UTC) formatted as 'YYYYMMDD0000'\n",
    "    for datetime, timezone, i in zip(date_times, timezones, range(len(timezones))):\n",
    "        #print(datetime + ' ' + timezone[:3])\n",
    "        datetime = parser.parse(datetime)\n",
    "        datetime = datetime + tz_conversions[timezone[:3]]\n",
    "        #print(datetime)\n",
    "        if (datetime.hour < 12):\n",
    "            datetime = datetime - timedelta(days = 1)\n",
    "        if datetime.year > 2049:\n",
    "            datetime = datetime - relativedelta(years = 100)\n",
    "        datetime = datetime.strftime(\"%Y%m%d\") + '0000'\n",
    "        if i == 0:\n",
    "            ret = [datetime]\n",
    "        else:\n",
    "            ret.append(datetime)\n",
    "    return ret\n",
    "\n",
    "def get_pph_date_strings(times):\n",
    "    # returns a list of strings of given dates formatted as 'YYYYMMDD0000'\n",
    "    for datetime, i in zip(times, range(len(times))):\n",
    "        string = datetime.dt.strftime(\"%Y%m%d\").values + '0000'\n",
    "        if i == 0:\n",
    "            ret = [string]\n",
    "        else:\n",
    "            ret.append(string)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports['DATE'] = get_reports_date_strings(reports['BEGIN_DATE_TIME'], reports['CZ_TIMEZONE']) \n",
    "pph['time'] = get_pph_date_strings(pph.time) \n",
    "# subset outlooks into only one day 1, two day 2, and one day 3 categorical outlooks \n",
    "# day 3: cycle not -1. day 2: cycle not -1. Day 1: cycle 6. Category: categorical. \n",
    "#outlooks = outlooks[(((outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6)) | ((outlooks['DAY'] == 2) & (outlooks['CYCLE'] != -1)) | ((outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1)))\n",
    "#        & (outlooks['CATEGORY'] == 'CATEGORICAL')]\n",
    "\n",
    "# reset incicies\n",
    "outlooks = outlooks.reset_index(drop=True)\n",
    "reports = reports.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridize outlooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outlooks_subset(outlooks, outlook_type):\n",
    "    outlooks = outlooks[(outlooks['THRESHOLD'] != 'SIGN') & (outlooks['THRESHOLD'] != 'TSTM')]\n",
    "    if outlook_type == 'Day 3':\n",
    "        return outlooks[(outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1) & (outlooks['CATEGORY'] == 'ANY SEVERE')]\n",
    "    elif outlook_type == 'Day 2 7':\n",
    "        return outlooks[(outlooks['DAY'] == 2) & (outlooks['CYCLE'] == 7) & (outlooks['CATEGORY'] == 'ANY SEVERE')]\n",
    "    elif outlook_type == 'Day 2 17':\n",
    "        return outlooks[(outlooks['DAY'] == 2) & (outlooks['CYCLE'] == 17) & (outlooks['CATEGORY'] == 'ANY SEVERE')]\n",
    "    elif outlook_type == 'Day 1 Wind':\n",
    "        return outlooks[(outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6) & (outlooks['CATEGORY'] == 'WIND')]\n",
    "    elif outlook_type == 'Day 1 Hail':\n",
    "        return outlooks[(outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6) & (outlooks['CATEGORY'] == 'HAIL')]\n",
    "    elif outlook_type == 'Day 1 Tornado':\n",
    "        return outlooks[(outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6) & (outlooks['CATEGORY'] == 'TORNADO')]\n",
    "    raise Exception(\"Invalid outlook_type given to get_outlooks_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as in mca: create_gridded_outlook_dataset. But will need to work overall, not just by hazard. day 3 is overall probability, days 1 and 2 by hazard\n",
    "# day 3 use ANY SEVERE, day 2 use ANY SEVERE, day 1 construct from each hazard (assuming independence? or use highest prob, which basically assumes complete dependence? Is this what is done for day 2?)\n",
    "# Things are entirely categorical before 2002, so even more reason to make that the cutoff.\n",
    "save_location = 'data/outlooks/grid_outlooks.nc'\n",
    "\n",
    "outlook_types = ['Day 3', 'Day 2 7', 'Day 2 17', 'Day 1 Wind', 'Day 1 Hail', 'Day 1 Tornado', 'Day 1']\n",
    "\n",
    "outlook_dataset = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        lat=(['y', 'x'], pph['lat'].data),\n",
    "        lon=(['y', 'x'], pph['lon'].data)\n",
    "    ),\n",
    "    coords=dict(\n",
    "        time=(['time'], pph['time'].data),\n",
    "        x=(['x'], pph['x'].data),\n",
    "        y=(['y'], pph['y'].data),\n",
    "        outlook=(['outlook'], outlook_types)\n",
    "    ),\n",
    "    attrs=dict(description=\"outlook as a percentage as a function of date, lat/lon, and which hazard type\",\n",
    "            grid = pph.grid),\n",
    ")\n",
    "\n",
    "outlook_dataset = outlook_dataset.assign(prob = (('time', 'y', 'x', 'outlook'), np.full((len(outlook_dataset['time']), len(outlook_dataset['y']), len(outlook_dataset['x']), len(outlook_types)), 0.0)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = 'data/outlooks/grid_outlooks.nc'\n",
    "\n",
    "outlook_types = ['Day 3', 'Day 2 7', 'Day 2 17', 'Day 1 Wind', 'Day 1 Hail', 'Day 1 Tornado', 'Day 1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if picking up partway through\n",
    "outlook_dataset = xr.open_dataset('data/outlooks/grid_outlooks.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 1 Tornado\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "# for each outlook type\n",
    "for outlook_type in outlook_types[1:-1]:\n",
    "    print(outlook_type)\n",
    "    outlooks_subset = get_outlooks_subset(outlooks, outlook_type)\n",
    "\n",
    "    oldyear = 0\n",
    "    array = np.zeros((len(outlook_dataset.time.values), len(outlook_dataset.y.values), len(outlook_dataset.x.values)))\n",
    "\n",
    "    for date, i in zip(outlook_dataset.time.values, range(len(outlook_dataset.time.values))):\n",
    "        lats = outlook_dataset.lat\n",
    "        lons = outlook_dataset.lon\n",
    "        datestring = str(date)\n",
    "        datestring = datestring[0:4] + datestring[5:7] + datestring[8:10] + '0000'\n",
    "        \n",
    "        # printing to track progress\n",
    "        year = datestring[0:4]\n",
    "        if year != oldyear:\n",
    "            print(year)\n",
    "            oldyear = year\n",
    "\n",
    "        outlooks_date = outlooks_subset[outlooks_subset['DATE'] == datestring]\n",
    "        if (len(outlooks_date) > 0):\n",
    "            for x, j in zip(outlook_dataset.x.values, range(len(outlook_dataset.x.values))):\n",
    "                for y, k in zip(outlook_dataset.y.values, range(len(outlook_dataset.y.values))):\n",
    "                    lat = lats.sel(x = x, y = y).values\n",
    "                    lon = lons.sel(x = x, y = y).values\n",
    "                    array[i, k, j] = find_threshold(outlooks_date, lat, lon)\n",
    "    me = xr.DataArray(array, coords={'time': outlook_dataset.time.values, 'y': outlook_dataset.y.values, 'x': outlook_dataset.x.values},\n",
    "                      dims=['time', 'y', 'x'])\n",
    "    outlook_dataset['prob'].loc[dict(outlook = outlook_type)] = me\n",
    "    outlook_dataset.to_netcdf(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = 'data/outlooks/grid_outlooks.nc'\n",
    "outlook_dataset.to_netcdf(save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build day 1 probabilities: max (assuming total dependence) or combined (assumes total independence). (Do both for later use? Or just 1 sine easy to rerun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
