{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsets outlooks, pph, and report data in various ways (add label to each datapoint for each method of subsetting so can be pulled out of full dataset later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading outlooks\n",
      "reading pph\n",
      "reading storm reports\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data_location = 'data'\n",
    "labelled = True # if starting with already labelled data\n",
    "if labelled == True:\n",
    "    outlooks, pph, reports = read_datasets(data_location, 'labelled')\n",
    "else:\n",
    "    outlooks, pph, reports = read_datasets(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_conversions = {'PST': timedelta(hours=8),\n",
    "                  'MST': timedelta(hours=7),\n",
    "                  'CST': timedelta(hours=6),\n",
    "                  'CSt': timedelta(hours=6),\n",
    "                  'CSC': timedelta(hours=6),\n",
    "                  'SCT': timedelta(hours=6),\n",
    "                  'EST': timedelta(hours=5),\n",
    "                  'ESt': timedelta(hours=5),\n",
    "                  'PDT': timedelta(hours=7),\n",
    "                  'MDT': timedelta(hours=6),\n",
    "                  'CDT': timedelta(hours=5),\n",
    "                  'EDT': timedelta(hours=4),\n",
    "                  'HST': timedelta(hours=10),\n",
    "                  'SST': timedelta(hours=11),\n",
    "                  'GST': timedelta(hours=10),\n",
    "                  'AKS': timedelta(hours=9),\n",
    "                  'AST': timedelta(hours=4),\n",
    "                  'UNK': timedelta(hours=5),\n",
    "                  'GMT': timedelta(0)}\n",
    "\n",
    "def get_reports_date_strings(date_times, timezones):\n",
    "    # returns list of strings of date of given datetime and timezone (where day cutoffs are 12-12 UTC) formatted as 'YYYYMMDD0000'\n",
    "    for datetime, timezone, i in zip(date_times, timezones, range(len(timezones))):\n",
    "        #print(datetime + ' ' + timezone[:3])\n",
    "        datetime = parser.parse(datetime)\n",
    "        datetime = datetime + tz_conversions[timezone[:3]]\n",
    "        #print(datetime)\n",
    "        if (datetime.hour < 12):\n",
    "            datetime = datetime - timedelta(days = 1)\n",
    "        if datetime.year > 2049:\n",
    "            datetime = datetime - relativedelta(years = 100)\n",
    "        datetime = datetime.strftime(\"%Y%m%d\") + '0000'\n",
    "        if i == 0:\n",
    "            ret = [datetime]\n",
    "        else:\n",
    "            ret.append(datetime)\n",
    "    return ret\n",
    "\n",
    "def get_pph_date_strings(times):\n",
    "    # returns a list of strings of given dates formatted as 'YYYYMMDD0000'\n",
    "    for datetime, i in zip(times, range(len(times))):\n",
    "        string = datetime.dt.strftime(\"%Y%m%d\").values + '0000'\n",
    "        if i == 0:\n",
    "            ret = [string]\n",
    "        else:\n",
    "            ret.append(string)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dates to reports and pph in same format as in outlooks\n",
    "if labelled == False:\n",
    "    reports['DATE'] = get_reports_date_strings(reports['BEGIN_DATE_TIME'], reports['CZ_TIMEZONE']) \n",
    "    pph['time'] = get_pph_date_strings(pph.time) \n",
    "    # subset outlooks into only one day 1, two day 2, and one day 3 categorical outlooks \n",
    "    # day 3: cycle not -1. day 2: cycle not -1. Day 1: cycle 6. Category: categorical.\n",
    "    outlooks = outlooks[(((outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6)) | ((outlooks['DAY'] == 2) & (outlooks['CYCLE'] != -1)) | ((outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1)))\n",
    "            & (outlooks['CATEGORY'] == 'CATEGORICAL')]\n",
    "\n",
    "    # reset incicies\n",
    "    outlooks = outlooks.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outlooks_label(outlooks, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding a new column in outlooks\")\n",
    "    outlooks[label_name] = none_label\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label)\n",
    "        outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
    "    return outlooks\n",
    "\n",
    "def add_pph_label(pph, label_dates, labels, label_name, none_label):\n",
    "    # adds new variable with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding new variable in pph\")\n",
    "    pph[label_name] = (('time'), np.full(len(pph['time']), none_label))\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label) # TODO: may only be adding first 4 letters of label\n",
    "        pph[label_name].loc[pph['time'].isin(dates)] = label  \n",
    "    return pph\n",
    "\n",
    "def add_reports_label(reports, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    reports[label_name] = none_label\n",
    "    print(\"adding a new column in reports\")\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "       #print(label)\n",
    "       reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
    "    return reports\n",
    "\n",
    "def add_labels(outlooks, pph, reports, label_dates, labels, label_name, none_label):\n",
    "    # adds labels, overwriting with later ones if a date has multiple labels\n",
    "    return(add_outlooks_label(outlooks, label_dates, labels, label_name, none_label), \n",
    "           add_pph_label(pph, label_dates, labels, label_name, none_label),\n",
    "           add_reports_label(reports, label_dates, labels, label_name, none_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, values, category_thresholds, name, add_cat = True, add_num = True):\n",
    "    # from a list of dates and the corresponding values on those dates, adds numerical and categorical labels\n",
    "    # category thresholds: lower thresholds of each category for values (e.g. 60 for HIGH PPH)\n",
    "    num_dict = {\n",
    "        0: []\n",
    "    }\n",
    "    cat_dict  = dict((k, []) for k, v in category_thresholds.items())    \n",
    "        \n",
    "    for date, value in zip(dates, values):\n",
    "        # add to categorical dict\n",
    "        for cat in cat_dict:\n",
    "            if value >= category_thresholds[cat]:\n",
    "                # ... copy from non-generalized PPH code below\n",
    "                cat_dict[cat].append(date)\n",
    "                break\n",
    "        \n",
    "        # add to numerical dict\n",
    "        if value in num_dict:\n",
    "            num_dict[value].append(date)\n",
    "        else:\n",
    "            num_dict[value] = [date]\n",
    "\n",
    "    if add_cat:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(cat_dict.values()), cat_dict.keys(), name + '_CAT', 'NONE')\n",
    "    if add_num:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(num_dict.values()), num_dict.keys(), name + '_NUM', 0)\n",
    "\n",
    "    return(new_outlooks, new_pph, new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = outlooks\n",
    "new_pph = pph\n",
    "new_reports = reports.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALWAYS RUN THROUGH HERE. THEN TO ADD MORE LABELS, RUN JUST THE LABELLING YOU WISH TO BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max threshold forecasted for valid day to each datapoint\n",
    "\n",
    "categories = ['TSTM', 'MRGL', 'SLGT', 'ENH', 'MDT', 'HIGH']\n",
    "category_dates = []\n",
    "for category in categories:\n",
    "    category_dates.append(identify_dates_above_threshold(new_outlooks, category))\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, category_dates, categories, 'MAX_CAT', 'NONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by ramp up/down amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put new_outlooks in correct order\n",
    "\n",
    "new_outlooks['DATE_ORDER'] = 0\n",
    "for index, row in new_outlooks.iterrows():\n",
    "    if row['DAY'] == 3:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '1'\n",
    "    elif row['DAY'] == 1:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '4'\n",
    "    elif row['CYCLE'] == 7:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '2'\n",
    "    else:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '3'\n",
    "new_outlooks = new_outlooks.sort_values('DATE_ORDER')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_3_cutoff(outlooks):\n",
    "    # returns list of dates in outlooks \n",
    "    return outlooks[outlooks['DAY'] == 3]['DATE'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ramp_lists(outlooks, category_dict):\n",
    "\n",
    "    first_day_3 = get_day_3_cutoff(outlooks)\n",
    "    first_day_2_1 = '199707100000'\n",
    "    first_day_2_2 = '199504040000'\n",
    "\n",
    "    ramp_ups = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: [],\n",
    "        6: []\n",
    "    }\n",
    "\n",
    "    ramp_downs = {\n",
    "        0: [],\n",
    "        -1: [],\n",
    "        -2: [],\n",
    "        -3: [],\n",
    "        -4: [],\n",
    "        -5: [],\n",
    "        -6: []\n",
    "    }\n",
    "\n",
    "    ramp_categories = {\n",
    "        'up': [],\n",
    "        'down': [],\n",
    "        'both': [],\n",
    "        'neither': []\n",
    "    }\n",
    "\n",
    "    old_date = '0'\n",
    "    old_do = '0'\n",
    "    first = True\n",
    "\n",
    "    for index, row in outlooks.iterrows(): #iterrating through each polygon in the outlook dataset\n",
    "        cat = category_dict[row['THRESHOLD']]\n",
    "        do = row['DATE_ORDER']\n",
    "        date = row['DATE']\n",
    "\n",
    "        if date != old_date: # New date, save ramp up and ramp down and save alongside old date, then reset ramps, max and min categories seen, and do threshold\n",
    "            \n",
    "            \n",
    "            if first == True:\n",
    "                first = False\n",
    "            else:\n",
    "\n",
    "                if max_cat_do - min_cat_date > ramp_up:\n",
    "                    ramp_up = max_cat_do - min_cat_date\n",
    "                if max_cat_do - max_cat_date < ramp_down:\n",
    "                    ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "                if max_cat_do > max_cat_date:\n",
    "                    max_cat_date = max_cat_do\n",
    "                if max_cat_do < min_cat_date:\n",
    "                    min_cat_date = max_cat_do\n",
    "\n",
    "                ramp_ups[ramp_up].append(old_date)\n",
    "                ramp_downs[ramp_down].append(old_date)\n",
    "                if ramp_up > 0 and ramp_down < 0:\n",
    "                    ramp_categories['both'].append(old_date)\n",
    "                elif ramp_up > 0:\n",
    "                    ramp_categories['up'].append(old_date)\n",
    "                elif ramp_down < 0:\n",
    "                    ramp_categories['down'].append(old_date)\n",
    "                else:\n",
    "                    ramp_categories['neither'].append(old_date)\n",
    "\n",
    "            old_date = date\n",
    "            old_do = do\n",
    "            \n",
    "            ramp_down = 0\n",
    "            ramp_up = 0\n",
    "            max_cat_date = -1\n",
    "            \n",
    "    \n",
    "            if date > first_day_3 and do[-1] == '1': # Since 2002\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_1 and (do[-1] == '2' or do[-1] == '1'): # Since 1997. Check for 1 is in case there is an earlier forecast issued.\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_2 and (do[-1] == '3' or do[-1] == '2' or do[-1] == '1'): # Since 1995\n",
    "                min_cat_date = 5 \n",
    "            elif date <= first_day_2_2:\n",
    "                min_cat_date = 5\n",
    "            else:\n",
    "                min_cat_date = -1 # The first outlook on this date was not the earliest forecast it could have been, so it ramped up from no forecast\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "\n",
    "        elif do != old_do: # new outlook, update min and max categories seen, ramp value\n",
    "            if max_cat_do - min_cat_date > ramp_up:\n",
    "                ramp_up = max_cat_do - min_cat_date\n",
    "            if max_cat_do - max_cat_date < ramp_down:\n",
    "                ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "            if max_cat_do > max_cat_date:\n",
    "                max_cat_date = max_cat_do\n",
    "            if max_cat_do < min_cat_date:\n",
    "                min_cat_date = max_cat_do\n",
    "\n",
    "            old_do = do\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "        else: # Just another threshold within the same polygon\n",
    "            if cat > max_cat_do:\n",
    "                max_cat_do = cat\n",
    "            \n",
    "        \n",
    "    # for last iteration\n",
    "    ramp_ups[ramp_up].append(old_date)\n",
    "    ramp_downs[ramp_down].append(old_date)\n",
    "    if ramp_up > 0 and ramp_down < 0:\n",
    "        ramp_categories['both'].append(old_date)\n",
    "    elif ramp_up > 0:\n",
    "        ramp_categories['up'].append(old_date)\n",
    "    elif ramp_down < 0:\n",
    "        ramp_categories['down'].append(old_date)\n",
    "    else:\n",
    "        ramp_categories['neither'].append(old_date)\n",
    "\n",
    "    return(ramp_ups, ramp_downs, ramp_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and add ramp category for each datapoint. Potentially add 2 binary ramp up and ramp down (4 options are [up, down, up and down, niether]). How many forecasts to consider for each day? The day 3, both day 2, and the first day 1 (so 4 forecasts ramp)\n",
    "# dictionary of category to number\n",
    "category_dict = {\n",
    "    None : -1,\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "(ramp_ups, ramp_downs, ramp_categories) = create_ramp_lists(new_outlooks, category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ramp up\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_ups.values()), ramp_ups.keys(), 'RAMP_UP', 0)\n",
    "\n",
    "# ramp down\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_downs.values()), ramp_downs.keys(), 'RAMP_DOWN', 0)\n",
    "\n",
    "# ramp categories\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_categories.values()), ramp_categories.keys(), 'RAMP_CATEGORIES', 'neither')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by accurate/inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider forecast issue time for day 1--subset reports (and revise pph?)--look at other studies to see how they handled multiple day 1 outlooks (see the one in slack). Starting point should be use first forecasts, later ones would be a different analysis for another time. Decide if we should denote these somehow.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(pph):\n",
    "    dates = list(set(pph['time'].values))\n",
    "    season_dates = [[], [], [], []]\n",
    "    for date in dates:\n",
    "        month = int(date[4:6])\n",
    "        if month == 12 or month < 3:\n",
    "            season_dates[0].append(date)\n",
    "        elif month < 6:\n",
    "            season_dates[1].append(date)\n",
    "        elif month < 9:\n",
    "            season_dates[2].append(date)\n",
    "        else:\n",
    "            season_dates[3].append(date)\n",
    "    return season_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column denoting season (4 met seasons as starting point)\n",
    "\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "season_dates = get_season_dates(new_pph)\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, season_dates, seasons, 'SEASON', 'xxxxxx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide how to do... lower priotity, could do by state of center of highest category (or center of PPH?) Could start with super basic version\n",
    "# same regions as Anderson-Frey 2016\n",
    "# center is grid square with highest p_perfect prob of at least one hazard (assuming each hazard in independent)\n",
    "\n",
    "# practically perfect probability of at least one hazard occuring near datapoint\n",
    "new_pph['p_perfect_total'] = (1 - (1-new_pph['p_perfect_wind']/100)*(1-new_pph['p_perfect_hail']/100)*(1-new_pph['p_perfect_tor']/100))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect regions in chunks (since doing it all at once can time out)\n",
    "regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(lat, lon, geolocator):\n",
    "    location = geolocator.reverse(str(lat)+\",\"+str(lon))\n",
    "    if location == None:\n",
    "        return None\n",
    "    address = location.raw['address']\n",
    "    state = address.get('state', '')\n",
    "    return state\n",
    "\n",
    "def get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator):\n",
    "    state = get_state(lat, lon, geolocator)\n",
    "    if state == 'Colorado' or state == 'New Mexico':\n",
    "        if lon < west_threshold_co_nm:\n",
    "            return('West')\n",
    "        else:\n",
    "            return('Great Plains')\n",
    "    for region in regions_dict:\n",
    "        if state in regions_dict[region]:\n",
    "            return region\n",
    "    # Cases where highest PPH is out of contiguous states, usually just outside bc nearest gridpoint is on other side of border\n",
    "    if lat > 38:\n",
    "        if lon > -80.5:\n",
    "            return('Northeast')\n",
    "        elif lon > -104:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    else:\n",
    "        if lon > -93.8:\n",
    "            return('South')\n",
    "        elif lon > -106.5:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    return('NONE')\n",
    "\n",
    "\n",
    "def create_regions(pph):\n",
    "    regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"severe_thunderstorm_miles\")\n",
    "    west_threshold_co_nm = -105\n",
    "    regions_dict = { # list of states fully within each region (doesn't include AK, HI, CO, NM)\n",
    "        'West': ['Washington', 'Oregon', 'California', 'Idaho', 'Montana', 'Wyoming', 'Utah', 'Arizona'],\n",
    "        'Midwest': ['North Dakota', 'South Dakota', 'Minnesota', 'Iowa', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana', 'Ohio', 'Kentucky'],\n",
    "        'Great Plains': ['Nebraska', 'Kansas', 'Oklahoma', 'Texas', 'Missouri'],\n",
    "        'Northeast': ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Rhode Island', 'Connecticut', 'New York', 'Pennsylvania', 'New Jersey', 'Delaware', 'Maryland', 'District of Columbia', 'West Virginia'],\n",
    "        'South': ['Virginia', 'Arkansas', 'Louisiana', 'Tennessee', 'Mississippi', 'Alabama', 'Georgia', 'North Carolina', 'South Carolina', 'Florida']\n",
    "    }\n",
    "\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        if date_pph['p_perfect_total'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding regions for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_total'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            region = get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator)\n",
    "            regions[region].append(date)\n",
    "            \n",
    "    return(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 10\n",
    "time_array = new_pph['time']\n",
    "chunk_size = math.ceil(len(time_array)/chunks)\n",
    "time_arrays = [time_array[i:i + chunk_size] for i in range(0, len(time_array), chunk_size)]\n",
    "for i in range(2, 3):\n",
    "    chunk_regions = create_regions(new_pph.sel(time = time_arrays[i]))\n",
    "    for region in regions:\n",
    "        regions[region] += chunk_regions[region]\n",
    "    print('Added chunk ' + str(i) + ' to regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(regions.values()), regions.keys(), 'REGION', 'NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by environmental data (to do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by max total pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing add numerical_categorical_columns\n",
    "dates = []\n",
    "pphs = []\n",
    "for date, date_pph in pph.groupby('time'):\n",
    "    dates.append(date)\n",
    "    pphs.append(float(date_pph['p_perfect_total'].max().values))\n",
    "\n",
    "pph_thresholds = { \n",
    "    'HIGH': 60,\n",
    "    'MDT': 45,\n",
    "    'ENH': 30,\n",
    "    'SLGT': 15,\n",
    "    'MRGL': 5,\n",
    "    'ZERO': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n",
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs, pph_thresholds, 'MAX_PPH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of storm reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_severe = new_reports\n",
    "new_reports.loc[new_reports['MAGNITUDE'] == '', 'MAGNITUDE'] = 0\n",
    "reports_severe = reports_severe[(reports_severe['EVENT_TYPE'] == 'Tornado') | \n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind') & (reports_severe['MAGNITUDE'].astype(float) >= 50)) |\n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Hail') & (reports_severe['MAGNITUDE'].astype(float) >= 1))]\n",
    "full_dates = reports_severe['DATE']\n",
    "c = Counter(full_dates)\n",
    "\n",
    "num_reports_thresholds = {\n",
    "    '1000': 1000,\n",
    "    '500': 500,\n",
    "    '100': 100,\n",
    "    '50': 50,\n",
    "    '10': 10,\n",
    "    '1': 1,\n",
    "    '0': 0\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, c.keys(), c.values(), num_reports_thresholds, 'NUM_REPORTS', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of each type of report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "tornado_reports = new_reports[new_reports['EVENT_TYPE'] == 'Tornado']\n",
    "\n",
    "wind_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind']\n",
    "\n",
    "hail_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Hail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "tornado_dates = tornado_reports['DATE']\n",
    "tornado_c = Counter(tornado_dates)\n",
    "\n",
    "wind_dates = wind_reports['DATE']\n",
    "wind_c = Counter(wind_dates)\n",
    "\n",
    "hail_dates = hail_reports['DATE']\n",
    "hail_c = Counter(hail_dates)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, tornado_c.keys(), tornado_c.values(), num_reports_thresholds, 'TOR_REPORTS', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, wind_c.keys(), wind_c.values(), num_reports_thresholds, 'WIND_REPORTS', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hail_c.keys(), hail_c.values(), num_reports_thresholds, 'HAIL_REPORTS', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by size of largest report of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "# tor: Categorical. Decide if we want to combine f and ef max \n",
    "strongest_tornadoes = tornado_reports.groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "strongest_tornadoes_without_u = tornado_reports[tornado_reports['TOR_F_SCALE'] != 'EFU'].groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "\n",
    "tornado_strengths = {\n",
    "    'EFU': [],\n",
    "    '(E)F0': [],\n",
    "    '(E)F1': [],\n",
    "    '(E)F2': [],\n",
    "    '(E)F3': [],\n",
    "    '(E)F4': [],\n",
    "    '(E)F5': []\n",
    "}\n",
    "for date, s in zip(strongest_tornadoes.index, strongest_tornadoes):\n",
    "    if s == 'EFU':\n",
    "        if date in strongest_tornadoes_without_u:\n",
    "            tornado_strengths['(E)F' + strongest_tornadoes_without_u[date][-1]].append(date)\n",
    "        else:\n",
    "            tornado_strengths['EFU'].append(date)\n",
    "\n",
    "    elif s != '':\n",
    "        tornado_strengths['(E)F' + s[-1]].append(date)\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(tornado_strengths.values()), tornado_strengths.keys(), 'MAX_TORNADO_RATING', 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both wind and hail have a few weird outliers: Wind all pre-2000. Hail has 2 0.00 and 1 0.01 in 2005. Unmeasured wind reports are 50 (knots)\n",
    "winds = wind_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})\n",
    "hails = hail_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_thresholds = {\n",
    "    'sig_severe': 65,\n",
    "    'severe': 50,\n",
    "    'NONE': 0\n",
    "}\n",
    "\n",
    "hail_thresholds = {\n",
    "    'sig_severe': 2,\n",
    "    'severe': 1,\n",
    "    'NONE': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n",
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(              ISSUE        EXPIRE       PRODISS TYPE  DAY THRESHOLD  \\\n",
       " 0      198701161200  198701171200  198701160615    C    1      TSTM   \n",
       " 1      198701271200  198701281200  198701270643    C    1      TSTM   \n",
       " 2      198701281200  198701291200  198701280631    C    1      TSTM   \n",
       " 3      198701291200  198701301200  198701290631    C    1      TSTM   \n",
       " 4      198701301200  198701311200  198701300636    C    1      TSTM   \n",
       " ...             ...           ...           ...  ...  ...       ...   \n",
       " 62396  202312301200  202312311200  202312280746    C    3      TSTM   \n",
       " 62397  202312301200  202312311200  202312290600    C    2      TSTM   \n",
       " 62398  202312301200  202312311200  202312291644    C    2      TSTM   \n",
       " 62399  202312311200  202401011200  202312290818    C    3      TSTM   \n",
       " 62400  202401021200  202401031200  202312310815    C    3      TSTM   \n",
       " \n",
       "           CATEGORY  CYCLE          DATE MAX_CAT  ... MAX_PPH_NUM  \\\n",
       " 0      CATEGORICAL      6  198701160000    TSTM  ...         0.0   \n",
       " 1      CATEGORICAL      6  198701270000    TSTM  ...         0.0   \n",
       " 2      CATEGORICAL      6  198701280000    TSTM  ...         0.0   \n",
       " 3      CATEGORICAL      6  198701290000    TSTM  ...         0.0   \n",
       " 4      CATEGORICAL      6  198701300000    TSTM  ...         0.0   \n",
       " ...            ...    ...           ...     ...  ...         ...   \n",
       " 62396  CATEGORICAL      8  202312300000    TSTM  ...         0.0   \n",
       " 62397  CATEGORICAL      7  202312300000    TSTM  ...         0.0   \n",
       " 62398  CATEGORICAL     17  202312300000    TSTM  ...         0.0   \n",
       " 62399  CATEGORICAL      8  202312310000    TSTM  ...         0.0   \n",
       " 62400  CATEGORICAL      8  202401020000    TSTM  ...         0.0   \n",
       " \n",
       "        NUM_REPORTS_NUM  TOR_REPORTS_NUM WIND_REPORTS_NUM HAIL_REPORTS_NUM  \\\n",
       " 0                    0                0                0                0   \n",
       " 1                    0                0                0                0   \n",
       " 2                    0                0                0                0   \n",
       " 3                    0                0                0                0   \n",
       " 4                    0                0                0                0   \n",
       " ...                ...              ...              ...              ...   \n",
       " 62396                0                0                0                0   \n",
       " 62397                0                0                0                0   \n",
       " 62398                0                0                0                0   \n",
       " 62399                0                0                0                0   \n",
       " 62400                0                0                0                0   \n",
       " \n",
       "       MAX_TORNADO_RATING MAX_WIND_SPEED_CAT MAX_WIND_SPEED_NUM  \\\n",
       " 0                   NONE               NONE                0.0   \n",
       " 1                   NONE               NONE                0.0   \n",
       " 2                   NONE               NONE                0.0   \n",
       " 3                   NONE               NONE                0.0   \n",
       " 4                   NONE               NONE                0.0   \n",
       " ...                  ...                ...                ...   \n",
       " 62396               NONE               NONE                0.0   \n",
       " 62397               NONE               NONE                0.0   \n",
       " 62398               NONE               NONE                0.0   \n",
       " 62399               NONE               NONE                0.0   \n",
       " 62400               NONE               NONE                0.0   \n",
       " \n",
       "        MAX_HAIL_SIZE_CAT  MAX_HAIL_SIZE_NUM  \n",
       " 0                   NONE                0.0  \n",
       " 1                   NONE                0.0  \n",
       " 2                   NONE                0.0  \n",
       " 3                   NONE                0.0  \n",
       " 4                   NONE                0.0  \n",
       " ...                  ...                ...  \n",
       " 62396               NONE                0.0  \n",
       " 62397               NONE                0.0  \n",
       " 62398               NONE                0.0  \n",
       " 62399               NONE                0.0  \n",
       " 62400               NONE                0.0  \n",
       " \n",
       " [62401 rows x 28 columns],\n",
       " <xarray.Dataset>\n",
       " Dimensions:             (time: 16071, x: 93, y: 65)\n",
       " Coordinates:\n",
       "   * time                (time) object '197901010000' ... '202212310000'\n",
       "   * x                   (x) float64 0.0 1.0 2.0 3.0 4.0 ... 89.0 90.0 91.0 92.0\n",
       "   * y                   (y) float64 0.0 1.0 2.0 3.0 4.0 ... 61.0 62.0 63.0 64.0\n",
       " Data variables: (12/26)\n",
       "     lat                 (y, x) float64 ...\n",
       "     lon                 (y, x) float64 ...\n",
       "     p_perfect_wind      (time, y, x) float64 ...\n",
       "     p_perfect_sig_wind  (time, y, x) float64 ...\n",
       "     p_perfect_hail      (time, y, x) float64 ...\n",
       "     p_perfect_sig_hail  (time, y, x) float64 ...\n",
       "     ...                  ...\n",
       "     HAIL_REPORTS_NUM    (time) int32 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 0\n",
       "     MAX_TORNADO_RATING  (time) <U4 'NONE' 'NONE' 'NONE' ... '(E)F' '(E)F' 'NONE'\n",
       "     MAX_WIND_SPEED_CAT  (time) <U4 'NONE' 'seve' 'NONE' ... 'seve' 'sig_' 'NONE'\n",
       "     MAX_WIND_SPEED_NUM  (time) int32 0 50 0 0 0 0 0 0 0 ... 62 0 0 0 0 0 52 70 0\n",
       "     MAX_HAIL_SIZE_CAT   (time) <U4 'NONE' 'NONE' 'NONE' ... 'seve' 'NONE' 'NONE'\n",
       "     MAX_HAIL_SIZE_NUM   (time) int32 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 1 0 0\n",
       " Attributes:\n",
       "     title:         Practically Perfect Wind Hindcasts\n",
       "     grid:          80-km NCEP 211\n",
       "     sigma:         1.5\n",
       "     author:        Dr. Victor Gensini\n",
       "     author_email:  vgensini@niu.edu\n",
       "     citation:      https://doi.org/10.1175/BAMS-D-19-0321.1,\n",
       "         field_1         STATE         EVENT_TYPE CZ_TYPE   CZ_NAME  WFO  \\\n",
       " 0             0      OKLAHOMA            Tornado       C   WASHITA        \n",
       " 1             1         TEXAS            Tornado       C  COMANCHE        \n",
       " 2             2  PENNSYLVANIA            Tornado       C    LEHIGH        \n",
       " 3             3  PENNSYLVANIA            Tornado       C   DAUPHIN        \n",
       " 4             4  PENNSYLVANIA            Tornado       C  CRAWFORD        \n",
       " ...         ...           ...                ...     ...       ...  ...   \n",
       " 1054186   72123       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       " 1054187   72124       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       " 1054188   72125       GEORGIA  Thunderstorm Wind       C   DOUGLAS  FFC   \n",
       " 1054189   72132     MINNESOTA            Tornado       C    NORMAN  FGF   \n",
       " 1054190   72133      NEW YORK  Thunderstorm Wind       C    ORANGE  OKX   \n",
       " \n",
       "             BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME INJURIES_DIRECT  \\\n",
       " 0        28-APR-50 14:45:00         CST  28-APR-50 14:45:00               0   \n",
       " 1        29-APR-50 15:30:00         CST  29-APR-50 15:30:00               0   \n",
       " 2        05-JUL-50 18:00:00         CST  05-JUL-50 18:00:00               2   \n",
       " 3        05-JUL-50 18:30:00         CST  05-JUL-50 18:30:00               0   \n",
       " 4        24-JUL-50 14:40:00         CST  24-JUL-50 14:40:00               0   \n",
       " ...                     ...         ...                 ...             ...   \n",
       " 1054186  31-JUL-23 18:40:00       MST-7  31-JUL-23 18:40:00               0   \n",
       " 1054187  31-JUL-23 19:35:00       MST-7  31-JUL-23 19:35:00               0   \n",
       " 1054188  06-AUG-23 13:25:00       EST-5  06-AUG-23 13:27:00               0   \n",
       " 1054189  24-JUN-23 15:10:00       CST-6  24-JUN-23 15:14:00               0   \n",
       " 1054190  03-JUL-23 15:00:00       EST-5  03-JUL-23 15:00:00               0   \n",
       " \n",
       "          ... MAX_PPH_NUM NUM_REPORTS_NUM TOR_REPORTS_NUM WIND_REPORTS_NUM  \\\n",
       " 0        ...         0.0               7               7                0   \n",
       " 1        ...         0.0               2               2                0   \n",
       " 2        ...         0.0               2               2                0   \n",
       " 3        ...         0.0               2               2                0   \n",
       " 4        ...         0.0               1               1                0   \n",
       " ...      ...         ...             ...             ...              ...   \n",
       " 1054186  ...         0.0              84               1               55   \n",
       " 1054187  ...         0.0              84               1               55   \n",
       " 1054188  ...         0.0             289              12              248   \n",
       " 1054189  ...         0.0             134              13               55   \n",
       " 1054190  ...         0.0             286               2              234   \n",
       " \n",
       "         HAIL_REPORTS_NUM MAX_TORNADO_RATING MAX_WIND_SPEED_CAT  \\\n",
       " 0                      0              (E)F4               NONE   \n",
       " 1                      0              (E)F2               NONE   \n",
       " 2                      0              (E)F2               NONE   \n",
       " 3                      0              (E)F2               NONE   \n",
       " 4                      0              (E)F0               NONE   \n",
       " ...                  ...                ...                ...   \n",
       " 1054186               28                EFU         sig_severe   \n",
       " 1054187               28                EFU         sig_severe   \n",
       " 1054188               29                EFU         sig_severe   \n",
       " 1054189               66                EFU         sig_severe   \n",
       " 1054190               50              (E)F0         sig_severe   \n",
       " \n",
       "         MAX_WIND_SPEED_NUM MAX_HAIL_SIZE_CAT MAX_HAIL_SIZE_NUM  \n",
       " 0                      0.0              NONE              0.00  \n",
       " 1                      0.0              NONE              0.00  \n",
       " 2                      0.0              NONE              0.00  \n",
       " 3                      0.0              NONE              0.00  \n",
       " 4                      0.0              NONE              0.00  \n",
       " ...                    ...               ...               ...  \n",
       " 1054186               75.0            severe              1.50  \n",
       " 1054187               75.0            severe              1.50  \n",
       " 1054188               74.0        sig_severe              2.50  \n",
       " 1054189               74.0        sig_severe              3.00  \n",
       " 1054190               90.0        sig_severe              2.75  \n",
       " \n",
       " [1054191 rows x 53 columns])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, winds.index, winds.astype(float)['MAGNITUDE'].values, wind_thresholds, 'MAX_WIND_SPEED')\n",
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hails.index, hails.astype(float)['MAGNITUDE'].values, hail_thresholds, 'MAX_HAIL_SIZE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL BY ACCURACY OF FORECAST\n",
    "Brier, FSS, SAL, and/or wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONCE YOU ADD THE LABELS YOU WANT, SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\4289693162.py:6: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  new_outlooks.to_file(outlook_save_location + '/labelled_outlooks.shp')\n"
     ]
    }
   ],
   "source": [
    "# resave labelled data\n",
    "outlook_save_location = 'data/outlooks'\n",
    "pph_save_location = 'data/pph'\n",
    "report_save_location = 'data/storm_reports'\n",
    "\n",
    "new_outlooks.to_file(outlook_save_location + '/labelled_outlooks.shp')\n",
    "if labelled == True:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled_pph2.nc') \n",
    "else:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled_pph.nc') \n",
    "new_reports.to_csv(report_save_location + '/labelled_reports.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
