{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.metrics import brier_score_loss, mean_squared_error\n",
    "from scipy.ndimage import uniform_filter\n",
    "import geocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsets outlooks, pph, and report data in various ways (add label to each datapoint for each method of subsetting so can be pulled out of full dataset later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading outlooks 1\n",
      "reading outlooks 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miles\\OneDrive\\Documents\\UW\\Research\\utils_filter.py:39: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  outlooks = outlooks.append(gp.read_file(data_location + '/outlooks/' + mod_string + '_outlooks_2.shp'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pph\n",
      "reading storm reports\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data_location = 'data'\n",
    "labelled = False # if starting with already labelled data\n",
    "if labelled == True:\n",
    "    outlooks, pph, reports = read_datasets(data_location, 'labelled2023')\n",
    "else:\n",
    "    outlooks, pph, reports = read_datasets(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_conversions = {'PST': timedelta(hours=8),\n",
    "                  'MST': timedelta(hours=7),\n",
    "                  'CST': timedelta(hours=6),\n",
    "                  'CSt': timedelta(hours=6),\n",
    "                  'CSC': timedelta(hours=6),\n",
    "                  'SCT': timedelta(hours=6),\n",
    "                  'EST': timedelta(hours=5),\n",
    "                  'ESt': timedelta(hours=5),\n",
    "                  'PDT': timedelta(hours=7),\n",
    "                  'MDT': timedelta(hours=6),\n",
    "                  'CDT': timedelta(hours=5),\n",
    "                  'EDT': timedelta(hours=4),\n",
    "                  'HST': timedelta(hours=10),\n",
    "                  'SST': timedelta(hours=11),\n",
    "                  'GST': timedelta(hours=10),\n",
    "                  'AKS': timedelta(hours=9),\n",
    "                  'AST': timedelta(hours=4),\n",
    "                  'UNK': timedelta(hours=5),\n",
    "                  'GMT': timedelta(0)}\n",
    "\n",
    "def get_reports_date_strings(date_times, timezones):\n",
    "    # returns list of strings of date of given datetime and timezone (where day cutoffs are 12-12 UTC) formatted as 'YYYYMMDD0000'\n",
    "    for datetime, timezone, i in zip(date_times, timezones, range(len(timezones))):\n",
    "        #print(datetime + ' ' + timezone[:3])\n",
    "        datetime = parser.parse(datetime)\n",
    "        datetime = datetime + tz_conversions[timezone[:3]]\n",
    "        #print(datetime)\n",
    "        if (datetime.hour < 12):\n",
    "            datetime = datetime - timedelta(days = 1)\n",
    "        if datetime.year > 2049:\n",
    "            datetime = datetime - relativedelta(years = 100)\n",
    "        datetime = datetime.strftime(\"%Y%m%d\") + '0000'\n",
    "        if i == 0:\n",
    "            ret = [datetime]\n",
    "        else:\n",
    "            ret.append(datetime)\n",
    "    return ret\n",
    "\n",
    "def get_pph_date_strings(times):\n",
    "    # returns a list of strings of given dates formatted as 'YYYYMMDD0000'\n",
    "    for datetime, i in zip(times, range(len(times))):\n",
    "        string = datetime.dt.strftime(\"%Y%m%d\").values + '0000'\n",
    "        if i == 0:\n",
    "            ret = [string]\n",
    "        else:\n",
    "            ret.append(string)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dates to reports and pph in same format as in outlooks\n",
    "if labelled == False:\n",
    "    reports['DATE'] = get_reports_date_strings(reports['BEGIN_DATE_TIME'], reports['CZ_TIMEZONE']) \n",
    "    pph['time'] = get_pph_date_strings(pph.time) \n",
    "    # subset outlooks into only one day 1, two day 2, and one day 3 categorical outlooks \n",
    "    # day 3: cycle not -1. day 2: cycle not -1. Day 1: cycle 6. Category: categorical.\n",
    "    outlooks = outlooks[(((outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6)) | ((outlooks['DAY'] == 2) & (outlooks['CYCLE'] != -1)) | ((outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1)))\n",
    "            & (outlooks['CATEGORY'] == 'CATEGORICAL')]\n",
    "\n",
    "    # reset incicies\n",
    "    outlooks = outlooks.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outlooks_label(outlooks, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding a new column in outlooks\")\n",
    "    outlooks[label_name] = none_label\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label)\n",
    "        outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
    "    return outlooks\n",
    "\n",
    "def add_pph_label(pph, label_dates, labels, label_name, none_label):\n",
    "    # adds new variable with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding new variable in pph\")\n",
    "    if type(none_label) == str:\n",
    "        pph[label_name] = (('time'), np.full(len(pph['time']), none_label, dtype='<U16'))\n",
    "    else:\n",
    "        pph[label_name] = (('time'), np.full(len(pph['time']), none_label))\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        pph[label_name].loc[pph['time'].isin(dates)] = label  \n",
    "    return pph\n",
    "\n",
    "def add_reports_label(reports, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    reports[label_name] = none_label\n",
    "    print(\"adding a new column in reports\")\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "       #print(label)\n",
    "       reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
    "    return reports\n",
    "\n",
    "def add_labels(outlooks, pph, reports, label_dates, labels, label_name, none_label):\n",
    "    # adds labels, overwriting with later ones if a date has multiple labels\n",
    "    return(add_outlooks_label(outlooks, label_dates, labels, label_name, none_label), \n",
    "           add_pph_label(pph, label_dates, labels, label_name, none_label),\n",
    "           add_reports_label(reports, label_dates, labels, label_name, none_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, values, category_thresholds, name, add_cat = True, add_num = True, num_none_val = 0):\n",
    "    # from a list of dates and the corresponding values on those dates, adds numerical and categorical labels\n",
    "    # category thresholds: lower thresholds of each category for values (e.g. 60 for HIGH PPH)\n",
    "    num_dict = {\n",
    "        0: []\n",
    "    }\n",
    "    cat_dict  = dict((k, []) for k, v in category_thresholds.items())    \n",
    "        \n",
    "    for date, value in zip(dates, values):\n",
    "        # add to categorical dict\n",
    "        for cat in cat_dict:\n",
    "            if value >= category_thresholds[cat]:\n",
    "                cat_dict[cat].append(date)\n",
    "                break\n",
    "        \n",
    "        # add to numerical dict\n",
    "        if value in num_dict:\n",
    "            num_dict[value].append(date)\n",
    "        else:\n",
    "            num_dict[value] = [date]\n",
    "\n",
    "    if add_cat:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(cat_dict.values()), cat_dict.keys(), name + '_CAT', 'NONE')\n",
    "    if add_num:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(num_dict.values()), num_dict.keys(), name + '_NUM', num_none_val)\n",
    "\n",
    "    return(new_outlooks, new_pph, new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = outlooks\n",
    "new_pph = pph\n",
    "new_reports = reports.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALWAYS RUN THROUGH HERE. THEN TO ADD MORE LABELS, RUN JUST THE LABELLING YOU WISH TO BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    }
   ],
   "source": [
    "# add max threshold forecasted for valid day to each datapoint\n",
    "\n",
    "categories = ['TSTM', 'MRGL', 'SLGT', 'ENH', 'MDT', 'HIGH']\n",
    "category_dates = []\n",
    "for category in categories:\n",
    "    category_dates.append(identify_dates_above_threshold(new_outlooks, category))\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, category_dates, categories, 'MAX_CAT', 'NONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by ramp up/down amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put new_outlooks in correct order\n",
    "\n",
    "new_outlooks['DATE_ORDER'] = 0\n",
    "for index, row in new_outlooks.iterrows():\n",
    "    if row['DAY'] == 3:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '1'\n",
    "    elif row['DAY'] == 1:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '4'\n",
    "    elif row['CYCLE'] == 7:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '2'\n",
    "    else:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '3'\n",
    "new_outlooks = new_outlooks.sort_values('DATE_ORDER')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_3_cutoff(outlooks):\n",
    "    # returns list of dates in outlooks \n",
    "    return outlooks[outlooks['DAY'] == 3]['DATE'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ramp_lists(outlooks, category_dict):\n",
    "\n",
    "    first_day_3 = get_day_3_cutoff(outlooks)\n",
    "    first_day_2_1 = '199707100000'\n",
    "    first_day_2_2 = '199504040000'\n",
    "\n",
    "    ramp_ups = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: [],\n",
    "        6: []\n",
    "    }\n",
    "\n",
    "    ramp_downs = {\n",
    "        0: [],\n",
    "        -1: [],\n",
    "        -2: [],\n",
    "        -3: [],\n",
    "        -4: [],\n",
    "        -5: [],\n",
    "        -6: []\n",
    "    }\n",
    "\n",
    "    ramp_categories = {\n",
    "        'up': [],\n",
    "        'down': [],\n",
    "        'both': [],\n",
    "        'neither': []\n",
    "    }\n",
    "\n",
    "    old_date = '0'\n",
    "    old_do = '0'\n",
    "    first = True\n",
    "\n",
    "    for index, row in outlooks.iterrows(): #iterrating through each polygon in the outlook dataset\n",
    "        cat = category_dict[row['THRESHOLD']]\n",
    "        do = row['DATE_ORDER']\n",
    "        date = row['DATE']\n",
    "\n",
    "        if date != old_date: # New date, save ramp up and ramp down and save alongside old date, then reset ramps, max and min categories seen, and do threshold\n",
    "            \n",
    "            \n",
    "            if first == True:\n",
    "                first = False\n",
    "            else:\n",
    "\n",
    "                if max_cat_do - min_cat_date > ramp_up:\n",
    "                    ramp_up = max_cat_do - min_cat_date\n",
    "                if max_cat_do - max_cat_date < ramp_down:\n",
    "                    ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "                if max_cat_do > max_cat_date:\n",
    "                    max_cat_date = max_cat_do\n",
    "                if max_cat_do < min_cat_date:\n",
    "                    min_cat_date = max_cat_do\n",
    "\n",
    "                ramp_ups[ramp_up].append(old_date)\n",
    "                ramp_downs[ramp_down].append(old_date)\n",
    "                if ramp_up > 0 and ramp_down < 0:\n",
    "                    ramp_categories['both'].append(old_date)\n",
    "                elif ramp_up > 0:\n",
    "                    ramp_categories['up'].append(old_date)\n",
    "                elif ramp_down < 0:\n",
    "                    ramp_categories['down'].append(old_date)\n",
    "                else:\n",
    "                    ramp_categories['neither'].append(old_date)\n",
    "\n",
    "            old_date = date\n",
    "            old_do = do\n",
    "            \n",
    "            ramp_down = 0\n",
    "            ramp_up = 0\n",
    "            max_cat_date = -1\n",
    "            \n",
    "    \n",
    "            if date > first_day_3 and do[-1] == '1': # Since 2002\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_1 and (do[-1] == '2' or do[-1] == '1'): # Since 1997. Check for 1 is in case there is an earlier forecast issued.\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_2 and (do[-1] == '3' or do[-1] == '2' or do[-1] == '1'): # Since 1995\n",
    "                min_cat_date = 5 \n",
    "            elif date <= first_day_2_2:\n",
    "                min_cat_date = 5\n",
    "            else:\n",
    "                min_cat_date = -1 # The first outlook on this date was not the earliest forecast it could have been, so it ramped up from no forecast\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "\n",
    "        elif do != old_do: # new outlook, update min and max categories seen, ramp value\n",
    "            if max_cat_do - min_cat_date > ramp_up:\n",
    "                ramp_up = max_cat_do - min_cat_date\n",
    "            if max_cat_do - max_cat_date < ramp_down:\n",
    "                ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "            if max_cat_do > max_cat_date:\n",
    "                max_cat_date = max_cat_do\n",
    "            if max_cat_do < min_cat_date:\n",
    "                min_cat_date = max_cat_do\n",
    "\n",
    "            old_do = do\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "        else: # Just another threshold within the same polygon\n",
    "            if cat > max_cat_do:\n",
    "                max_cat_do = cat\n",
    "            \n",
    "        \n",
    "    # for last iteration\n",
    "    ramp_ups[ramp_up].append(old_date)\n",
    "    ramp_downs[ramp_down].append(old_date)\n",
    "    if ramp_up > 0 and ramp_down < 0:\n",
    "        ramp_categories['both'].append(old_date)\n",
    "    elif ramp_up > 0:\n",
    "        ramp_categories['up'].append(old_date)\n",
    "    elif ramp_down < 0:\n",
    "        ramp_categories['down'].append(old_date)\n",
    "    else:\n",
    "        ramp_categories['neither'].append(old_date)\n",
    "\n",
    "    return(ramp_ups, ramp_downs, ramp_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and add ramp category for each datapoint. Potentially add 2 binary ramp up and ramp down (4 options are [up, down, up and down, niether]). How many forecasts to consider for each day? The day 3, both day 2, and the first day 1 (so 4 forecasts ramp)\n",
    "# dictionary of category to number\n",
    "category_dict = {\n",
    "    None : -1,\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "(ramp_ups, ramp_downs, ramp_categories) = create_ramp_lists(new_outlooks, category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ramp up\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_ups.values()), ramp_ups.keys(), 'RAMP_UP', 0)\n",
    "\n",
    "# ramp down\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_downs.values()), ramp_downs.keys(), 'RAMP_DOWN', 0)\n",
    "\n",
    "# ramp categories\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_categories.values()), ramp_categories.keys(), 'RAMP_CAT', 'neither')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(pph):\n",
    "    dates = list(set(pph['time'].values))\n",
    "    season_dates = [[], [], [], []]\n",
    "    for date in dates:\n",
    "        month = int(date[4:6])\n",
    "        if month == 12 or month < 3:\n",
    "            season_dates[0].append(date)\n",
    "        elif month < 6:\n",
    "            season_dates[1].append(date)\n",
    "        elif month < 9:\n",
    "            season_dates[2].append(date)\n",
    "        else:\n",
    "            season_dates[3].append(date)\n",
    "    return season_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "# add column denoting season (4 met seasons as starting point)\n",
    "\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "season_dates = get_season_dates(new_pph)\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, season_dates, seasons, 'SEASON', 'NONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same regions as Anderson-Frey 2016\n",
    "# center is grid square with highest p_perfect prob of at least one hazard (assuming each hazard in independent)\n",
    "\n",
    "# practically perfect probability of at least one hazard occuring near datapoint # TODO: max instead of combined?\n",
    "\n",
    "p_perfect_max = np.maximum(new_pph['p_perfect_wind'].values, np.maximum(new_pph['p_perfect_hail'].values, new_pph['p_perfect_tor'].values))\n",
    "new_pph = new_pph.assign(p_perfect_max=(('time', 'y', 'x'), p_perfect_max))\n",
    "\n",
    "new_pph['p_perfect_total'] = (1 - (1-new_pph['p_perfect_wind']/100)*(1-new_pph['p_perfect_hail']/100)*(1-new_pph['p_perfect_tor']/100))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect regions in chunks (since doing it all at once can time out)\n",
    "regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(lat, lon, geolocator):\n",
    "    \n",
    "    location = geolocator.reverse(str(lat)+\",\"+str(lon))\n",
    "    if location == None:\n",
    "        return None\n",
    "    address = location.raw['address']\n",
    "    state = address.get('state', '')\n",
    "    print(state)\n",
    "    return state\n",
    "\n",
    "def get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator):\n",
    "    state = get_state(lat, lon, geolocator)\n",
    "    if state == 'Colorado' or state == 'New Mexico':\n",
    "        if lon < west_threshold_co_nm:\n",
    "            return('West')\n",
    "        else:\n",
    "            return('Great Plains')\n",
    "    for region in regions_dict:\n",
    "        if state in regions_dict[region]:\n",
    "            return region\n",
    "    # Cases where highest PPH is out of contiguous states, usually just outside bc nearest gridpoint is on other side of border\n",
    "    if lat > 38:\n",
    "        if lon > -80.5:\n",
    "            return('Northeast')\n",
    "        elif lon > -104:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    else:\n",
    "        if lon > -93.8:\n",
    "            return('South')\n",
    "        elif lon > -106.5:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    return('NONE')\n",
    "\n",
    "\n",
    "def create_regions(pph):\n",
    "    regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"severe_thunderstorm_miles_new1\")\n",
    "    west_threshold_co_nm = -105\n",
    "    regions_dict = { # list of states fully within each region (doesn't include AK, HI, CO, NM)\n",
    "        'West': ['Washington', 'Oregon', 'California', 'Idaho', 'Montana', 'Wyoming', 'Utah', 'Arizona'],\n",
    "        'Midwest': ['North Dakota', 'South Dakota', 'Minnesota', 'Iowa', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana', 'Ohio', 'Kentucky'],\n",
    "        'Great Plains': ['Nebraska', 'Kansas', 'Oklahoma', 'Texas', 'Missouri'],\n",
    "        'Northeast': ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Rhode Island', 'Connecticut', 'New York', 'Pennsylvania', 'New Jersey', 'Delaware', 'Maryland', 'District of Columbia', 'West Virginia'],\n",
    "        'South': ['Virginia', 'Arkansas', 'Louisiana', 'Tennessee', 'Mississippi', 'Alabama', 'Georgia', 'North Carolina', 'South Carolina', 'Florida']\n",
    "    }\n",
    "\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        if date_pph['p_perfect_totalsvr'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding regions for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_totalsvr'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            region = get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator)\n",
    "            regions[region].append(date)\n",
    "            \n",
    "    return(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding regions for 2021\n",
      "Louisiana\n",
      "Washington\n",
      "Oklahoma\n",
      "Kentucky\n",
      "Florida\n",
      "Kansas\n",
      "Florida\n",
      "Florida\n",
      "Alabama\n",
      "Mississippi\n",
      "Nevada\n",
      "Florida\n",
      "Florida\n",
      "Virginia\n",
      "Texas\n",
      "Tennessee\n",
      "Colorado\n",
      "Missouri\n",
      "Kansas\n",
      "Texas\n",
      "Texas\n",
      "Kansas\n",
      "Missouri\n",
      "Oklahoma\n",
      "Mississippi\n",
      "West Virginia\n",
      "Oregon\n",
      "Texas\n",
      "Iowa\n",
      "Texas\n",
      "Alabama\n",
      "New York\n",
      "Mississippi\n",
      "Delaware\n",
      "Mississippi\n",
      "Alabama\n",
      "Florida\n",
      "Kansas\n",
      "Minnesota\n",
      "Kansas\n",
      "Missouri\n",
      "Kentucky\n",
      "Mississippi\n",
      "Florida\n",
      "Florida\n",
      "Texas\n",
      "Texas\n",
      "North Carolina\n",
      "Texas\n",
      "Louisiana\n",
      "Florida\n",
      "Florida\n",
      "North Carolina\n",
      "New Jersey\n",
      "Oklahoma\n",
      "Alabama\n",
      "California\n",
      "Wisconsin\n",
      "Colorado\n",
      "Texas\n",
      "Illinois\n",
      "Maryland\n",
      "Wyoming\n",
      "Kansas\n",
      "Oklahoma\n",
      "Alabama\n",
      "Nebraska\n",
      "Tennessee\n",
      "Texas\n",
      "Kansas\n",
      "Mississippi\n",
      "Texas\n",
      "Texas\n",
      "Georgia\n",
      "Nebraska\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Utah\n",
      "South Dakota\n",
      "Nebraska\n",
      "New Mexico\n",
      "South Dakota\n",
      "Kansas\n",
      "Wisconsin\n",
      "New York\n",
      "Oklahoma\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Colorado\n",
      "Texas\n",
      "Delaware\n",
      "New Jersey\n",
      "North Dakota\n",
      "Oklahoma\n",
      "North Dakota\n",
      "South Dakota\n",
      "Delaware\n",
      "North Dakota\n",
      "Kansas\n",
      "Texas\n",
      "West Virginia\n",
      "Pennsylvania\n",
      "South Carolina\n",
      "Iowa\n",
      "Minnesota\n",
      "Indiana\n",
      "Added chunk 93 to regions\n",
      "Finding regions for 2021\n",
      "Colorado\n",
      "Missouri\n",
      "Pennsylvania\n",
      "Iowa\n",
      "Nebraska\n",
      "Kansas\n",
      "Kansas\n",
      "Texas\n",
      "Chihuahua\n",
      "Kentucky\n",
      "Vermont\n",
      "Vermont\n",
      "Maryland\n",
      "Pennsylvania\n",
      "Nebraska\n",
      "Texas\n",
      "South Dakota\n",
      "New York\n",
      "Pennsylvania\n",
      "South Dakota\n",
      "Nebraska\n",
      "Arizona\n",
      "Pennsylvania\n",
      "Pennsylvania\n",
      "New York\n",
      "Iowa\n",
      "Utah\n",
      "Pennsylvania\n",
      "Pennsylvania\n",
      "Kansas\n",
      "North Dakota\n",
      "New York\n",
      "Delaware\n",
      "North Dakota\n",
      "Minnesota\n",
      "Michigan\n",
      "South Dakota\n",
      "Wisconsin\n",
      "Pennsylvania\n",
      "Illinois\n",
      "Pennsylvania\n",
      "Alabama\n",
      "Tennessee\n",
      "North Carolina\n",
      "Florida\n",
      "West Virginia\n",
      "Nevada\n",
      "Idaho\n",
      "South Dakota\n",
      "Kansas\n",
      "Missouri\n",
      "North Dakota\n",
      "Wisconsin\n",
      "Michigan\n",
      "Pennsylvania\n",
      "Ohio\n",
      "North Carolina\n",
      "Kansas\n",
      "Georgia\n",
      "South Carolina\n",
      "Pennsylvania\n",
      "Colorado\n",
      "Kansas\n",
      "Utah\n",
      "Virginia\n",
      "South Dakota\n",
      "Michigan\n",
      "Alabama\n",
      "South Dakota\n",
      "Delaware\n",
      "Minnesota\n",
      "Michigan\n",
      "Virginia\n",
      "South Carolina\n",
      "Delaware\n",
      "Utah\n",
      "Kansas\n",
      "New Mexico\n",
      "Arizona\n",
      "Maine\n",
      "Illinois\n",
      "Vermont\n",
      "California\n",
      "Idaho\n",
      "Colorado\n",
      "Michigan\n",
      "Pennsylvania\n",
      "Michigan\n",
      "New Hampshire\n",
      "South Dakota\n",
      "Kansas\n",
      "Colorado\n",
      "South Dakota\n",
      "Wisconsin\n",
      "Arkansas\n",
      "Kentucky\n",
      "Delaware\n",
      "California\n",
      "Texas\n",
      "Virginia\n",
      "Kansas\n",
      "Texas\n",
      "New Mexico\n",
      "Illinois\n",
      "California\n",
      "Mississippi\n",
      "Tennessee\n",
      "Utah\n",
      "Ohio\n",
      "South Dakota\n",
      "Oklahoma\n",
      "Illinois\n",
      "Kansas\n",
      "North Dakota\n",
      "Oklahoma\n",
      "Ohio\n",
      "New York\n",
      "Ohio\n",
      "Texas\n",
      "Arkansas\n",
      "Missouri\n",
      "North Carolina\n",
      "Texas\n",
      "Texas\n",
      "Florida\n",
      "Texas\n",
      "Oklahoma\n",
      "Alabama\n",
      "New York\n",
      "\n",
      "Arkansas\n",
      "Added chunk 94 to regions\n",
      "Finding regions for 2021\n",
      "Kentucky\n",
      "Tennessee\n",
      "Florida\n",
      "Florida\n",
      "Louisiana\n",
      "Indiana\n",
      "Tennessee\n",
      "Arizona\n",
      "Iowa\n",
      "Oklahoma\n",
      "Louisiana\n",
      "Florida\n",
      "Florida\n",
      "Montana\n",
      "Georgia\n",
      "Georgia\n",
      "Kentucky\n",
      "Finding regions for 2022\n",
      "Kentucky\n",
      "Georgia\n",
      "North Carolina\n",
      "Texas\n",
      "Alabama\n",
      "Florida\n",
      "Florida\n",
      "Florida\n",
      "Texas\n",
      "Alabama\n",
      "Texas\n",
      "New Jersey\n",
      "New York\n",
      "Montana\n",
      "Texas\n",
      "Tennessee\n",
      "Kansas\n",
      "Illinois\n",
      "Arkansas\n",
      "Pennsylvania\n",
      "Alabama\n",
      "Florida\n",
      "Florida\n",
      "Florida\n",
      "Texas\n",
      "South Carolina\n",
      "Mississippi\n",
      "Alabama\n",
      "Pennsylvania\n",
      "Florida\n",
      "Texas\n",
      "Mississippi\n",
      "Ohio\n",
      "Illinois\n",
      "\n",
      "Texas\n",
      "Kansas\n",
      "Mississippi\n",
      "Pennsylvania\n",
      "Texas\n",
      "Florida\n",
      "Oklahoma\n",
      "Texas\n",
      "Georgia\n",
      "Georgia\n",
      "North Carolina\n",
      "Missouri\n",
      "Arkansas\n",
      "Iowa\n",
      "Mississippi\n",
      "New Jersey\n",
      "Arkansas\n",
      "Mississippi\n",
      "Mississippi\n",
      "Florida\n",
      "Texas\n",
      "Arkansas\n",
      "Missouri\n",
      "Kansas\n",
      "Iowa\n",
      "Michigan\n",
      "Ohio\n",
      "North Carolina\n",
      "Colorado\n",
      "Nebraska\n",
      "Kansas\n",
      "Illinois\n",
      "Texas\n",
      "Oklahoma\n",
      "Ohio\n",
      "Texas\n",
      "Texas\n",
      "North Carolina\n",
      "Nebraska\n",
      "South Dakota\n",
      "Minnesota\n",
      "Texas\n",
      "Minnesota\n",
      "Minnesota\n",
      "Texas\n",
      "Kansas\n",
      "Added chunk 95 to regions\n",
      "Finding regions for 2022\n",
      "Arkansas\n",
      "New York\n",
      "Kansas\n",
      "Kansas\n",
      "Kentucky\n",
      "Pennsylvania\n",
      "North Carolina\n",
      "Pennsylvania\n",
      "South Carolina\n",
      "Texas\n",
      "Mississippi\n",
      "North Carolina\n",
      "Maryland\n",
      "South Dakota\n",
      "South Dakota\n",
      "Minnesota\n",
      "Oklahoma\n",
      "Ohio\n",
      "Virginia\n",
      "South Carolina\n",
      "Kansas\n",
      "Nebraska\n",
      "Nebraska\n",
      "Kansas\n",
      "Ohio\n",
      "Texas\n",
      "Mississippi\n",
      "Kansas\n",
      "South Dakota\n",
      "Ohio\n",
      "Georgia\n",
      "Alabama\n",
      "North Carolina\n",
      "North Carolina\n",
      "Florida\n",
      "North Dakota\n",
      "Minnesota\n",
      "Kansas\n",
      "Pennsylvania\n",
      "Minnesota\n",
      "North Dakota\n",
      "Missouri\n",
      "Louisiana\n",
      "Minnesota\n",
      "Utah\n",
      "South Dakota\n",
      "Michigan\n",
      "Pennsylvania\n",
      "Virginia\n",
      "South Dakota\n",
      "Illinois\n",
      "South Dakota\n",
      "North Carolina\n",
      "South Carolina\n",
      "Montana\n",
      "North Dakota\n",
      "Minnesota\n",
      "Michigan\n",
      "New York\n",
      "Louisiana\n",
      "Texas\n",
      "Montana\n",
      "Virginia\n",
      "Missouri\n",
      "North Dakota\n",
      "South Carolina\n",
      "Kentucky\n",
      "New Hampshire\n",
      "Illinois\n",
      "Ohio\n",
      "Pennsylvania\n",
      "Maryland\n",
      "North Carolina\n",
      "Colorado\n",
      "Oklahoma\n",
      "Virginia\n",
      "Arizona\n",
      "Illinois\n",
      "Illinois\n",
      "Wisconsin\n",
      "Michigan\n",
      "Pennsylvania\n",
      "South Dakota\n",
      "Tennessee\n",
      "Massachusetts\n",
      "Missouri\n",
      "Tennessee\n",
      "Virginia\n",
      "Idaho\n",
      "Idaho\n",
      "Montana\n",
      "Nebraska\n",
      "North Carolina\n",
      "Mississippi\n",
      "Arizona\n",
      "Nebraska\n",
      "Iowa\n",
      "Ohio\n",
      "Arizona\n",
      "Virginia\n",
      "North Dakota\n",
      "California\n",
      "Montana\n",
      "New Hampshire\n",
      "Iowa\n",
      "Illinois\n",
      "Michigan\n",
      "New York\n",
      "Kansas\n",
      "Oklahoma\n",
      "Kansas\n",
      "Montana\n",
      "Texas\n",
      "Texas\n",
      "Nevada\n",
      "Montana\n",
      "Minnesota\n",
      "New Mexico\n",
      "Florida\n",
      "Arizona\n",
      "Virginia\n",
      "Vermont\n",
      "Texas\n",
      "Kansas\n",
      "Nebraska\n",
      "Iowa\n",
      "Illinois\n",
      "New York\n",
      "Wisconsin\n",
      "Idaho\n",
      "Massachusetts\n",
      "Arizona\n",
      "Arkansas\n",
      "Virginia\n",
      "Vermont\n",
      "Florida\n",
      "Utah\n",
      "Utah\n",
      "North Carolina\n",
      "Colorado\n",
      "Utah\n",
      "Arizona\n",
      "Arizona\n",
      "California\n",
      "Texas\n",
      "Nebraska\n",
      "Mississippi\n",
      "New York\n",
      "New Hampshire\n",
      "Arkansas\n",
      "New Mexico\n",
      "Virginia\n",
      "Washington\n",
      "South Dakota\n",
      "Texas\n",
      "Alabama\n",
      "Added chunk 96 to regions\n",
      "Finding regions for 2022\n",
      "Texas\n",
      "Alabama\n",
      "Florida\n",
      "Arkansas\n",
      "Illinois\n",
      "California\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "Louisiana\n",
      "Pennsylvania\n",
      "Mississippi\n",
      "Alabama\n",
      "Texas\n",
      "Oklahoma\n",
      "Texas\n",
      "Florida\n",
      "Florida\n",
      "North Carolina\n",
      "\n",
      "Texas\n",
      "Louisiana\n",
      "Finding regions for 2023\n",
      "Mississippi\n",
      "Louisiana\n",
      "Alabama\n",
      "Georgia\n",
      "Texas\n",
      "California\n",
      "California\n",
      "Kentucky\n",
      "Georgia\n",
      "California\n",
      "Iowa\n",
      "Louisiana\n",
      "Ohio\n",
      "Florida\n",
      "Georgia\n",
      "Louisiana\n",
      "Florida\n",
      "Texas\n",
      "Missouri\n",
      "Florida\n",
      "Texas\n",
      "Kansas\n",
      "Arkansas\n",
      "Tennessee\n",
      "Texas\n",
      "Pennsylvania\n",
      "Illinois\n",
      "California\n",
      "Oklahoma\n",
      "Indiana\n",
      "Texas\n",
      "Texas\n",
      "Tennessee\n",
      "California\n",
      "Texas\n",
      "Texas\n",
      "Florida\n",
      "Arkansas\n",
      "Georgia\n",
      "Florida\n",
      "Sonora\n",
      "Texas\n",
      "Alabama\n",
      "California\n",
      "Texas\n",
      "Tennessee\n",
      "Georgia\n",
      "Mississippi\n",
      "Georgia\n",
      "Mississippi\n",
      "Nebraska\n",
      "Illinois\n",
      "Pennsylvania\n",
      "Texas\n",
      "Georgia\n",
      "Iowa\n",
      "Kentucky\n",
      "Virginia\n",
      "\n",
      "Texas\n",
      "Added chunk 97 to regions\n",
      "Finding regions for 2023\n",
      "Oklahoma\n",
      "New Mexico\n",
      "Kansas\n",
      "Kansas\n",
      "Arkansas\n",
      "Florida\n",
      "Florida\n",
      "Nebraska\n",
      "Kansas\n",
      "Illinois\n",
      "North Carolina\n",
      "Pennsylvania\n",
      "Texas\n",
      "Florida\n",
      "Texas\n",
      "Texas\n",
      "Florida\n",
      "Texas\n",
      "Florida\n",
      "North Carolina\n",
      "Oregon\n",
      "Texas\n",
      "Texas\n",
      "Utah\n",
      "Nebraska\n",
      "Texas\n",
      "Iowa\n",
      "Missouri\n",
      "Kansas\n",
      "Colorado\n",
      "Mississippi\n",
      "Nebraska\n",
      "Illinois\n",
      "Illinois\n",
      "Tennessee\n",
      "Kentucky\n",
      "Florida\n",
      "Alabama\n",
      "Texas\n",
      "Nevada\n",
      "Idaho\n",
      "Texas\n",
      "Texas\n",
      "Texas\n",
      "New Mexico\n",
      "Colorado\n",
      "Wyoming\n",
      "Texas\n",
      "Nebraska\n",
      "Nebraska\n",
      "New Mexico\n",
      "Illinois\n",
      "Massachusetts\n",
      "Texas\n",
      "Tennessee\n",
      "Kansas\n",
      "South Carolina\n",
      "South Carolina\n",
      "Texas\n",
      "Oklahoma\n",
      "Texas\n",
      "Tennessee\n",
      "Texas\n",
      "Texas\n",
      "Georgia\n",
      "Oklahoma\n",
      "Alabama\n",
      "Arkansas\n",
      "Alabama\n",
      "North Carolina\n",
      "Wyoming\n",
      "Colorado\n",
      "New Mexico\n",
      "South Dakota\n",
      "Iowa\n",
      "Georgia\n",
      "North Carolina\n",
      "Kansas\n",
      "Colorado\n",
      "Illinois\n",
      "Kentucky\n",
      "Kentucky\n",
      "Kentucky\n",
      "Virginia\n",
      "Kansas\n",
      "New Mexico\n",
      "Colorado\n",
      "Texas\n",
      "Texas\n",
      "North Carolina\n",
      "Nebraska\n",
      "Nebraska\n",
      "Missouri\n",
      "New York\n",
      "Kansas\n",
      "New Mexico\n",
      "Louisiana\n",
      "Indiana\n",
      "Tennessee\n",
      "Minnesota\n",
      "Michigan\n",
      "Georgia\n",
      "Kansas\n",
      "North Carolina\n",
      "New York\n",
      "Connecticut\n",
      "Michigan\n",
      "Wisconsin\n",
      "West Virginia\n",
      "Tennessee\n",
      "Missouri\n",
      "Colorado\n",
      "North Dakota\n",
      "Missouri\n",
      "Alabama\n",
      "New York\n",
      "Kansas\n",
      "Alabama\n",
      "South Carolina\n",
      "Georgia\n",
      "Alabama\n",
      "South Dakota\n",
      "Minnesota\n",
      "Ohio\n",
      "Oklahoma\n",
      "Tennessee\n",
      "North Carolina\n",
      "Wisconsin\n",
      "Indiana\n",
      "Montana\n",
      "California\n",
      "Arizona\n",
      "Arizona\n",
      "Montana\n",
      "Ohio\n",
      "Ohio\n",
      "Kentucky\n",
      "North Carolina\n",
      "Louisiana\n",
      "Georgia\n",
      "Idaho\n",
      "Montana\n",
      "Arizona\n",
      "California\n",
      "Oregon\n",
      "Utah\n",
      "South Dakota\n",
      "Missouri\n",
      "Mississippi\n",
      "New York\n",
      "Massachusetts\n",
      "Pennsylvania\n",
      "Texas\n",
      "Texas\n",
      "North Carolina\n",
      "Massachusetts\n",
      "Texas\n",
      "Texas\n",
      "Kansas\n",
      "New Mexico\n",
      "New Mexico\n",
      "Oklahoma\n",
      "South Dakota\n",
      "Kansas\n",
      "Added chunk 98 to regions\n",
      "Finding regions for 2023\n",
      "Wyoming\n",
      "Missouri\n",
      "Texas\n",
      "Texas\n",
      "Missouri\n",
      "Kentucky\n",
      "Tennessee\n",
      "Minnesota\n",
      "Utah\n",
      "New Mexico\n",
      "Texas\n",
      "Nebraska\n",
      "Texas\n",
      "Texas\n",
      "Missouri\n",
      "Florida\n",
      "Kansas\n",
      "Iowa\n",
      "South Carolina\n",
      "Wisconsin\n",
      "Minnesota\n",
      "Oklahoma\n",
      "Texas\n",
      "Texas\n",
      "Oregon\n",
      "Michigan\n",
      "Florida\n",
      "Florida\n",
      "Arizona\n",
      "Arizona\n",
      "Mississippi\n",
      "Alabama\n",
      "Louisiana\n",
      "Florida\n",
      "New York\n",
      "Arkansas\n",
      "Mississippi\n",
      "Florida\n",
      "North Carolina\n",
      "California\n",
      "Arizona\n",
      "Texas\n",
      "Added chunk 99 to regions\n"
     ]
    }
   ],
   "source": [
    "chunks = 100\n",
    "time_array = new_pph['time']\n",
    "chunk_size = math.ceil(len(time_array)/chunks)\n",
    "time_arrays = [time_array[i:i + chunk_size] for i in range(0, len(time_array), chunk_size)]\n",
    "for i in range(93, chunks):\n",
    "    chunk_regions = create_regions(new_pph.sel(time = time_arrays[i]))\n",
    "    for region in regions:\n",
    "        regions[region] += chunk_regions[region]\n",
    "    print('Added chunk ' + str(i) + ' to regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(regions.values()), regions.keys(), 'REGION_M', 'NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label with lat and lon of max pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lat_lon(pph):\n",
    "    # lat and lon of max pph for each day, with days with zero pph given the mean lat and lon of all other days\n",
    "    lats = []\n",
    "    lons = []\n",
    "    dates = []\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        dates.append(date)\n",
    "        if date_pph['p_perfect_totalsvr'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding lat/lon for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_totalsvr'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lats.append(lat)\n",
    "            lons.append(lon)\n",
    "        else:\n",
    "            lats.append(np.nan)\n",
    "            lons.append(np.nan)\n",
    "        \n",
    "    meanlat = np.nanmean(lats)\n",
    "    meanlon = np.nanmean(lons)\n",
    "    lats = np.nan_to_num(lats, nan = meanlat)\n",
    "    lons = np.nan_to_num(lons, nan = meanlon)\n",
    "\n",
    "    return(lats, lons, dates, meanlat, meanlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding lat/lon for 1979\n",
      "Finding lat/lon for 1980\n",
      "Finding lat/lon for 1981\n",
      "Finding lat/lon for 1982\n",
      "Finding lat/lon for 1983\n",
      "Finding lat/lon for 1984\n",
      "Finding lat/lon for 1985\n",
      "Finding lat/lon for 1986\n",
      "Finding lat/lon for 1987\n",
      "Finding lat/lon for 1988\n",
      "Finding lat/lon for 1989\n",
      "Finding lat/lon for 1990\n",
      "Finding lat/lon for 1991\n",
      "Finding lat/lon for 1992\n",
      "Finding lat/lon for 1993\n",
      "Finding lat/lon for 1994\n",
      "Finding lat/lon for 1995\n",
      "Finding lat/lon for 1996\n",
      "Finding lat/lon for 1997\n",
      "Finding lat/lon for 1998\n",
      "Finding lat/lon for 1999\n",
      "Finding lat/lon for 2000\n",
      "Finding lat/lon for 2001\n",
      "Finding lat/lon for 2002\n",
      "Finding lat/lon for 2003\n",
      "Finding lat/lon for 2004\n",
      "Finding lat/lon for 2005\n",
      "Finding lat/lon for 2006\n",
      "Finding lat/lon for 2007\n",
      "Finding lat/lon for 2008\n",
      "Finding lat/lon for 2009\n",
      "Finding lat/lon for 2010\n",
      "Finding lat/lon for 2011\n",
      "Finding lat/lon for 2012\n",
      "Finding lat/lon for 2013\n",
      "Finding lat/lon for 2014\n",
      "Finding lat/lon for 2015\n",
      "Finding lat/lon for 2016\n",
      "Finding lat/lon for 2017\n",
      "Finding lat/lon for 2018\n",
      "Finding lat/lon for 2019\n",
      "Finding lat/lon for 2020\n",
      "Finding lat/lon for 2021\n",
      "Finding lat/lon for 2022\n",
      "Finding lat/lon for 2023\n"
     ]
    }
   ],
   "source": [
    "lats, lons, dates, meanlat, meanlon = create_lat_lon(new_pph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, lats, {}, 'LAT', add_cat = False, num_none_val = meanlat)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, lons, {}, 'LON', add_cat = False, num_none_val = meanlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by environmental data (to do later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by max total pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent = True\n",
    "# testing add numerical_categorical_columns\n",
    "dates = []\n",
    "pphs_d = []\n",
    "pphs_i = []\n",
    "pphs = []\n",
    "\n",
    "for date, date_pph in new_pph.groupby('time'):\n",
    "    dates.append(date)\n",
    "    pphs_d.append(float(date_pph['p_perfect_max'].max().values))\n",
    "    pphs_i.append(float(date_pph['p_perfect_total'].max().values))\n",
    "    pphs.append(float(date_pph['p_perfect_totalsvr'].max().values))\n",
    "\n",
    "pph_thresholds = { \n",
    "    'HIGH': 60,\n",
    "    'MDT': 45,\n",
    "    'ENH': 30,\n",
    "    'SLGT': 15,\n",
    "    'MRGL': 5,\n",
    "    'ZERO': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs_d, pph_thresholds, 'PPH_D', num_none_val = 0.0)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs_i, pph_thresholds, 'PPH_I', num_none_val = 0.0)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs, pph_thresholds, 'PPH', num_none_val = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of storm reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_severe = new_reports\n",
    "new_reports.loc[new_reports['MAGNITUDE'] == '', 'MAGNITUDE'] = 0\n",
    "reports_severe = reports_severe[(reports_severe['EVENT_TYPE'] == 'Tornado') | \n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind') & (reports_severe['MAGNITUDE'].astype(float) >= 50)) |\n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Hail') & (reports_severe['MAGNITUDE'].astype(float) >= 1))]\n",
    "full_dates = reports_severe['DATE']\n",
    "c = Counter(full_dates)\n",
    "\n",
    "num_reports_thresholds = {\n",
    "    '1000': 1000,\n",
    "    '500': 500,\n",
    "    '100': 100,\n",
    "    '50': 50,\n",
    "    '10': 10,\n",
    "    '1': 1,\n",
    "    '0': 0\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, c.keys(), c.values(), num_reports_thresholds, 'REPORT', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of each type of report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tornado_reports = new_reports[new_reports['EVENT_TYPE'] == 'Tornado']\n",
    "\n",
    "wind_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind']\n",
    "\n",
    "hail_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Hail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "tornado_dates = tornado_reports['DATE']\n",
    "tornado_c = Counter(tornado_dates)\n",
    "\n",
    "wind_dates = wind_reports['DATE']\n",
    "wind_c = Counter(wind_dates)\n",
    "\n",
    "hail_dates = hail_reports['DATE']\n",
    "hail_c = Counter(hail_dates)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, tornado_c.keys(), tornado_c.values(), num_reports_thresholds, 'TOR', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, wind_c.keys(), wind_c.values(), num_reports_thresholds, 'WIND', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hail_c.keys(), hail_c.values(), num_reports_thresholds, 'HAIL', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by size of largest report of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "# tor: Categorical. Decide if we want to combine f and ef max \n",
    "strongest_tornadoes = tornado_reports.groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "strongest_tornadoes_without_u = tornado_reports[tornado_reports['TOR_F_SCALE'] != 'EFU'].groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "\n",
    "tornado_strengths = {\n",
    "    'EFU': [],\n",
    "    '(E)F0': [],\n",
    "    '(E)F1': [],\n",
    "    '(E)F2': [],\n",
    "    '(E)F3': [],\n",
    "    '(E)F4': [],\n",
    "    '(E)F5': []\n",
    "}\n",
    "for date, s in zip(strongest_tornadoes.index, strongest_tornadoes):\n",
    "    if s == 'EFU':\n",
    "        if date in strongest_tornadoes_without_u:\n",
    "            tornado_strengths['(E)F' + strongest_tornadoes_without_u[date][-1]].append(date)\n",
    "        else:\n",
    "            tornado_strengths['EFU'].append(date)\n",
    "\n",
    "    elif s != '':\n",
    "        tornado_strengths['(E)F' + s[-1]].append(date)\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(tornado_strengths.values()), tornado_strengths.keys(), 'TOR_F', 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both wind and hail have a few weird outliers: Wind all pre-2000. Hail has 2 0.00 and 1 0.01 in 2005. Unmeasured wind reports are 50 (knots)\n",
    "winds = wind_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})\n",
    "hails = hail_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_thresholds = {\n",
    "    'sig_severe': 65,\n",
    "    'severe': 50,\n",
    "    'NONE': 0\n",
    "}\n",
    "\n",
    "hail_thresholds = {\n",
    "    'sig_severe': 2,\n",
    "    'severe': 1,\n",
    "    'NONE': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_11900\\3104462980.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(              ISSUE        EXPIRE       PRODISS TYPE  DAY THRESHOLD  \\\n",
       " 0      198701161200  198701171200  198701160615    C    1      TSTM   \n",
       " 1      198701271200  198701281200  198701270643    C    1      TSTM   \n",
       " 2      198701281200  198701291200  198701280631    C    1      TSTM   \n",
       " 3      198701291200  198701301200  198701290631    C    1      TSTM   \n",
       " 4      198701301200  198701311200  198701300636    C    1      TSTM   \n",
       " ...             ...           ...           ...  ...  ...       ...   \n",
       " 62394  202312301200  202312311200  202312280746    C    3      TSTM   \n",
       " 62397  202312301200  202312311200  202312290600    C    2      TSTM   \n",
       " 62399  202312301200  202312311200  202312291644    C    2      TSTM   \n",
       " 62398  202312311200  202401011200  202312290818    C    3      TSTM   \n",
       " 62400  202401021200  202401031200  202312310815    C    3      TSTM   \n",
       " \n",
       "           CATEGORY  CYCLE          DATE  \\\n",
       " 0      CATEGORICAL      6  198701160000   \n",
       " 1      CATEGORICAL      6  198701270000   \n",
       " 2      CATEGORICAL      6  198701280000   \n",
       " 3      CATEGORICAL      6  198701290000   \n",
       " 4      CATEGORICAL      6  198701300000   \n",
       " ...            ...    ...           ...   \n",
       " 62394  CATEGORICAL      8  202312300000   \n",
       " 62397  CATEGORICAL      7  202312300000   \n",
       " 62399  CATEGORICAL     17  202312300000   \n",
       " 62398  CATEGORICAL      8  202312310000   \n",
       " 62400  CATEGORICAL      8  202401020000   \n",
       " \n",
       "                                                 geometry  ... PPH_NUM  \\\n",
       " 0      POLYGON ((-81.10300 30.60200, -81.08700 30.521...  ...     0.0   \n",
       " 1      POLYGON ((-119.01700 33.49500, -119.54500 34.0...  ...     0.0   \n",
       " 2      MULTIPOLYGON (((-123.71244 38.67008, -123.7150...  ...     0.0   \n",
       " 3      MULTIPOLYGON (((-123.71500 38.67200, -123.7230...  ...     0.0   \n",
       " 4      POLYGON ((-103.95573 29.30252, -103.97500 29.3...  ...     0.0   \n",
       " ...                                                  ...  ...     ...   \n",
       " 62394  POLYGON ((-120.67500 33.78700, -121.27500 33.6...  ...     0.0   \n",
       " 62397  POLYGON ((-119.53500 33.98700, -120.06600 33.9...  ...     0.0   \n",
       " 62399  POLYGON ((-119.53500 33.98700, -120.06600 33.9...  ...     0.0   \n",
       " 62398  POLYGON ((-92.16400 29.18100, -92.62900 29.233...  ...     0.0   \n",
       " 62400  POLYGON ((-90.74500 28.71100, -91.08300 28.732...  ...     0.0   \n",
       " \n",
       "       REPORT_NUM  TOR_NUM  WIND_NUM HAIL_NUM TOR_F WINDSP_CAT  WINDSP_NUM  \\\n",
       " 0              0        0         0        0  NONE       NONE         0.0   \n",
       " 1              0        0         0        0  NONE       NONE         0.0   \n",
       " 2              0        0         0        0  NONE       NONE         0.0   \n",
       " 3              0        0         0        0  NONE       NONE         0.0   \n",
       " 4              0        0         0        0  NONE       NONE         0.0   \n",
       " ...          ...      ...       ...      ...   ...        ...         ...   \n",
       " 62394          0        0         0        0  NONE       NONE         0.0   \n",
       " 62397          0        0         0        0  NONE       NONE         0.0   \n",
       " 62399          0        0         0        0  NONE       NONE         0.0   \n",
       " 62398          0        0         0        0  NONE       NONE         0.0   \n",
       " 62400          0        0         0        0  NONE       NONE         0.0   \n",
       " \n",
       "        HAILSZ_CAT HAILSZ_NUM  \n",
       " 0            NONE        0.0  \n",
       " 1            NONE        0.0  \n",
       " 2            NONE        0.0  \n",
       " 3            NONE        0.0  \n",
       " 4            NONE        0.0  \n",
       " ...           ...        ...  \n",
       " 62394        NONE        0.0  \n",
       " 62397        NONE        0.0  \n",
       " 62399        NONE        0.0  \n",
       " 62398        NONE        0.0  \n",
       " 62400        NONE        0.0  \n",
       " \n",
       " [62401 rows x 34 columns],\n",
       " <xarray.Dataset>\n",
       " Dimensions:              (time: 16436, x: 93, y: 65)\n",
       " Coordinates:\n",
       "   * time                 (time) <U12 '197901010000' ... '202312310000'\n",
       "   * x                    (x) float64 0.0 1.0 2.0 3.0 4.0 ... 89.0 90.0 91.0 92.0\n",
       "   * y                    (y) float64 0.0 1.0 2.0 3.0 4.0 ... 61.0 62.0 63.0 64.0\n",
       " Data variables: (12/35)\n",
       "     lat                  (y, x) float64 ...\n",
       "     lon                  (y, x) float64 ...\n",
       "     p_perfect_wind       (time, y, x) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "     p_perfect_sig_wind   (time, y, x) float64 ...\n",
       "     p_perfect_hail       (time, y, x) float64 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "     p_perfect_sig_hail   (time, y, x) float64 ...\n",
       "     ...                   ...\n",
       "     HAIL_NUM             (time) int32 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0\n",
       "     TOR_F                (time) <U16 'NONE' 'NONE' 'NONE' ... 'NONE' 'NONE'\n",
       "     WINDSP_CAT           (time) <U16 'NONE' 'severe' 'NONE' ... 'NONE' 'NONE'\n",
       "     WINDSP_NUM           (time) float64 0.0 50.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
       "     HAILSZ_CAT           (time) <U16 'NONE' 'NONE' 'NONE' ... 'NONE' 'NONE'\n",
       "     HAILSZ_NUM           (time) float64 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0\n",
       " Attributes:\n",
       "     title:         Practically Perfect Wind Hindcasts\n",
       "     grid:          80-km NCEP 211\n",
       "     sigma:         1.5\n",
       "     author:        Dr. Victor Gensini\n",
       "     author_email:  vgensini@niu.edu\n",
       "     citation:      https://doi.org/10.1175/BAMS-D-19-0321.1,\n",
       "         field_1         STATE         EVENT_TYPE CZ_TYPE   CZ_NAME  WFO  \\\n",
       " 0             0      OKLAHOMA            Tornado       C   WASHITA        \n",
       " 1             1         TEXAS            Tornado       C  COMANCHE        \n",
       " 2             2  PENNSYLVANIA            Tornado       C    LEHIGH        \n",
       " 3             3  PENNSYLVANIA            Tornado       C   DAUPHIN        \n",
       " 4             4  PENNSYLVANIA            Tornado       C  CRAWFORD        \n",
       " ...         ...           ...                ...     ...       ...  ...   \n",
       " 1054186   72123       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       " 1054187   72124       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       " 1054188   72125       GEORGIA  Thunderstorm Wind       C   DOUGLAS  FFC   \n",
       " 1054189   72132     MINNESOTA            Tornado       C    NORMAN  FGF   \n",
       " 1054190   72133      NEW YORK  Thunderstorm Wind       C    ORANGE  OKX   \n",
       " \n",
       "             BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME INJURIES_DIRECT  \\\n",
       " 0        28-APR-50 14:45:00         CST  28-APR-50 14:45:00               0   \n",
       " 1        29-APR-50 15:30:00         CST  29-APR-50 15:30:00               0   \n",
       " 2        05-JUL-50 18:00:00         CST  05-JUL-50 18:00:00               2   \n",
       " 3        05-JUL-50 18:30:00         CST  05-JUL-50 18:30:00               0   \n",
       " 4        24-JUL-50 14:40:00         CST  24-JUL-50 14:40:00               0   \n",
       " ...                     ...         ...                 ...             ...   \n",
       " 1054186  31-JUL-23 18:40:00       MST-7  31-JUL-23 18:40:00               0   \n",
       " 1054187  31-JUL-23 19:35:00       MST-7  31-JUL-23 19:35:00               0   \n",
       " 1054188  06-AUG-23 13:25:00       EST-5  06-AUG-23 13:27:00               0   \n",
       " 1054189  24-JUN-23 15:10:00       CST-6  24-JUN-23 15:14:00               0   \n",
       " 1054190  03-JUL-23 15:00:00       EST-5  03-JUL-23 15:00:00               0   \n",
       " \n",
       "          ...    PPH_NUM REPORT_NUM TOR_NUM WIND_NUM HAIL_NUM  TOR_F  \\\n",
       " 0        ...   0.000000          7       7        0        0  (E)F4   \n",
       " 1        ...   0.000000          2       2        0        0  (E)F2   \n",
       " 2        ...   0.000000          2       2        0        0  (E)F2   \n",
       " 3        ...   0.000000          2       2        0        0  (E)F2   \n",
       " 4        ...   0.000000          1       1        0        0  (E)F0   \n",
       " ...      ...        ...        ...     ...      ...      ...    ...   \n",
       " 1054186  ...  26.974308         84       1       55       28    EFU   \n",
       " 1054187  ...  26.974308         84       1       55       28    EFU   \n",
       " 1054188  ...  72.438386        289      12      248       29  (E)F2   \n",
       " 1054189  ...  62.302221        134      13       55       66  (E)F2   \n",
       " 1054190  ...  65.413030        286       2      234       50  (E)F0   \n",
       " \n",
       "          WINDSP_CAT WINDSP_NUM  HAILSZ_CAT HAILSZ_NUM  \n",
       " 0              NONE        0.0        NONE       0.00  \n",
       " 1              NONE        0.0        NONE       0.00  \n",
       " 2              NONE        0.0        NONE       0.00  \n",
       " 3              NONE        0.0        NONE       0.00  \n",
       " 4              NONE        0.0        NONE       0.00  \n",
       " ...             ...        ...         ...        ...  \n",
       " 1054186  sig_severe       75.0      severe       1.50  \n",
       " 1054187  sig_severe       75.0      severe       1.50  \n",
       " 1054188  sig_severe       74.0  sig_severe       2.50  \n",
       " 1054189  sig_severe       74.0  sig_severe       3.00  \n",
       " 1054190  sig_severe       90.0  sig_severe       2.75  \n",
       " \n",
       " [1054191 rows x 59 columns])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, winds.index, winds.astype(float)['MAGNITUDE'].values, wind_thresholds, 'WINDSP', num_none_val = 0.0)\n",
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hails.index, hails.astype(float)['MAGNITUDE'].values, hail_thresholds, 'HAILSZ', num_none_val = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by accuracy of forecast\n",
    "Brier, FSS, SAL, and/or wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need gridded outlooks and reports\n",
    "grid_outlook_location = 'data/outlooks/grid_outlooks.nc'\n",
    "grid_report_location = 'data/storm_reports/grid_reports.nc'\n",
    "\n",
    "grid_outlooks = xr.open_dataset(grid_outlook_location)\n",
    "grid_reports = xr.open_dataset(grid_report_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(grid_outlooks, grid_reports, outlook_day_str = 'Day 1', report_type_str = 'Total Reports'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob'].data.flatten()\n",
    "        verification = gr.sel(time = date)['bool'].data.flatten()\n",
    "        scores.append(brier_score_loss(verification, outlooks))\n",
    "    return(scores)\n",
    "\n",
    "bs = brier_score(grid_outlooks, grid_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, bs, {}, 'BS', num_none_val = 0.0, add_cat = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhood probabilistic verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth (with variable neighborhood size) and then do brier score\n",
    "def neighborhood_verification(grid_outlooks, grid_reports, neighborhood_size, outlook_day_str = 'Day 1', report_type_str = 'Total Reports'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = uniform_filter(go.sel(time = date)['prob'], size=neighborhood_size, mode='constant')\n",
    "        verification = uniform_filter(gr.sel(time = date)['bool'].astype(float), size=neighborhood_size, mode='constant')\n",
    "        scores.append(np.mean((outlooks - verification) ** 2))\n",
    "    return scores\n",
    "npv = neighborhood_verification(grid_outlooks, grid_reports, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, npv, {}, 'NEIGH', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE between outlooks and PPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do rmse\n",
    "def rmse_verification(grid_outlooks, pph, outlook_day_str = 'Day 1', pph_hazard_str = 'max'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob']\n",
    "        p = pph.sel(time = date)['p_perfect_' + pph_hazard_str]/100\n",
    "        scores.append(mean_squared_error(p, outlooks, squared=False))\n",
    "    return(scores)\n",
    "        \n",
    "rmse = rmse_verification(grid_outlooks, pph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, rmse, {}, 'RMSE', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once grid outlooks updated, see with day 1\n",
    "# plt.scatter(bs, npv)\n",
    "#a = neighborhood_verification(grid_outlooks, grid_reports, 5, outlook_day_str='Day 3')\n",
    "#b = brier_score(grid_outlooks, grid_reports, outlook_day_str= 'Day 3')\n",
    "#c = rmse_verification(grid_outlooks, new_pph, outlook_day_str='Day 3')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(bs, npv, rmse, s = .1)\n",
    "ax.set_xlabel('Brier Score')\n",
    "ax.set_ylabel('Neighborhod Verification')\n",
    "ax.set_zlabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINGENCY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PC(a, b, c, d):\n",
    "    return np.divide(a + d, a + b + c + d)\n",
    "\n",
    "def POD(a, b, c, d):\n",
    "    r = np.divide(a, a + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FOM(a, b, c, d):\n",
    "    r = np.divide(c, a + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FAR(a, b, c, d):\n",
    "    r = np.divide(b, a + b)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FOH(a, b, c, d):\n",
    "    r = np.divide(a, a + b)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def POFD(a, b, c, d):\n",
    "    return np.divide(b, b + d)\n",
    "\n",
    "def PCR(a, b, c, d): \n",
    "    return np.divide(d, b + d)\n",
    "\n",
    "def DFR(a, b, c, d):\n",
    "    return np.divide(c, c + d)\n",
    "\n",
    "def FOCN(a, b, c, d):\n",
    "    return np.divide(d, c + d)\n",
    "\n",
    "def CSI(a, b, c, d):\n",
    "    r = np.divide(a, a + b + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def Bias(a, b, c, d):\n",
    "    r = np.divide(a + b, a + c)\n",
    "    r[np.isnan(r)] = 1\n",
    "    return r\n",
    "\n",
    "def ETS(a, b, c, d):\n",
    "    bias = np.divide(Bias(a, b, c, d), a + b + c + d)\n",
    "    return np.divide(a - bias, a + b + c - bias)\n",
    "\n",
    "def TSS(a, b, c, d):\n",
    "    r = np.divide((np.multiply(a, d) - np.multiply(b, c)), np.multiply(a + c, b + d))\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contingency\n",
    "contingency = xr.load_dataset('data/contingency/contingency.nc')\n",
    "\n",
    "hazard_types= ['Wind', 'Hail', 'Tornado', 'All Hazard']\n",
    "\n",
    "hazard_string_dict = {\n",
    "    'Wind': '_W',\n",
    "    'Hail': '_H',\n",
    "    'Tornado': '_T',\n",
    "    'All Hazard': ''\n",
    "}\n",
    "\n",
    "# loop through functions (without repeating inverses)/hazard type, add numerical/categorical columns\n",
    "for fn in [PC, POD, FAR, POFD, DFR, CSI, Bias, ETS, TSS]:\n",
    "    for hazard in hazard_types:\n",
    "        scores = fn(contingency.sel(hazard = hazard)['a'], contingency.sel(hazard = hazard)['b'], contingency.sel(hazard = hazard)['c'], contingency.sel(hazard = hazard)['d']).values\n",
    "        new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, contingency['time'].data, scores, {}, fn.__name__ + hazard_string_dict[hazard], add_cat = False, num_none_val = 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POD and FAR--DON'T RUN (accounted for in contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1', report_type_str = 'Total Reports', squared = False):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "\n",
    "    pods = []\n",
    "    farts = []\n",
    "    hrs = []\n",
    "    fars = []\n",
    "\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob'].data.flatten()\n",
    "        verification = gr.sel(time = date)['bool'].data.flatten()\n",
    "\n",
    "        pod = np.mean(outlooks[verification]) \n",
    "        if np.isnan(pod):\n",
    "            pods.append(0.0)\n",
    "        else:\n",
    "            if squared:\n",
    "                pods.append(pod**2)\n",
    "            else:\n",
    "                pods.append(pod)\n",
    "\n",
    "        fart = np.mean(outlooks[~verification])\n",
    "        if np.isnan(fart):\n",
    "            farts.append(0.0)\n",
    "        else:\n",
    "            if squared:\n",
    "                farts.append(fart**2)\n",
    "            else:\n",
    "                farts.append(fart)\n",
    "        \n",
    "        hr = np.sum(outlooks[verification]) / np.sum(outlooks)\n",
    "        if np.isnan(hr):\n",
    "            hrs.append(0.0)\n",
    "        else:\n",
    "            hrs.append(hr)\n",
    "\n",
    "        far = np.sum(outlooks[~verification]) / np.sum(outlooks)\n",
    "        if np.isnan(far):\n",
    "            fars.append(0.0)\n",
    "        else:\n",
    "            fars.append(far)\n",
    "\n",
    "    return(pods, farts, hrs, fars)\n",
    "\n",
    "pod, fart, hr, far = pod_farate_hr_faratio(grid_outlooks, grid_reports)\n",
    "hail_pod, hail_fart, hail_hr, hail_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Hail', report_type_str = 'Hail')\n",
    "wind_pod, wind_fart, wind_hr, wind_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Wind', report_type_str = 'Wind')\n",
    "tornado_pod, tornado_fart, tornado_hr, tornado_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Tornado', report_type_str = 'Tornado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, pod, {}, 'POD', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, fart, {}, 'FART', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hr, {}, 'HR', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, far, {}, 'FAR', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_pod, {}, 'POD_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_fart, {}, 'FART_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_hr, {}, 'HR_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_far, {}, 'FAR_H', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_pod, {}, 'POD_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_fart, {}, 'FART_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_hr, {}, 'HR_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_far, {}, 'FAR_W', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_pod, {}, 'POD_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_fart, {}, 'FART_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_hr, {}, 'HR_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_far, {}, 'FAR_T', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_dataset = xr.load_dataset('data/displacement/displacements_new.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_times = displacement_dataset['time'].data\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['e_shift'].data, {}, 'E_SH', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['n_shift'].data, {}, 'N_SH', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['total_div'].data, {}, 'DIV', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['e_shift'].data, {}, 'E_SH_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['n_shift'].data, {}, 'N_SH_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['total_div'].data, {}, 'DIV_W', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['e_shift'].data, {}, 'E_SH_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['n_shift'].data, {}, 'N_SH_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['total_div'].data, {}, 'DIV_H', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['e_shift'].data, {}, 'E_SH_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['n_shift'].data, {}, 'N_SH_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['total_div'].data, {}, 'DIV_T', num_none_val = 0.0, add_cat = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# resave labelled data\n",
    "outlook_save_location = 'data/outlooks'\n",
    "pph_save_location = 'data/pph'\n",
    "report_save_location = 'data/storm_reports'\n",
    "\n",
    "new_outlooks.to_file(outlook_save_location + '/labelled2023_outlooks2.shp')\n",
    "if labelled == True:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled_pph2.nc') \n",
    "else:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled2023_pph2.nc') \n",
    "new_reports.to_csv(report_save_location + '/labelled2023_reports2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING COLUMNS (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['FART_NUM', 'HR_NUM', 'FART_H_NUM', 'HR_H_NUM', 'FART_W_NUM', 'HR_W_NUM', 'FART_T_NUM', 'HR_T_NUM']\n",
    "new_outlooks = new_outlooks.drop(columns = columns_to_remove)\n",
    "new_reports = new_reports.drop(columns = columns_to_remove)\n",
    "new_pph = new_pph.drop_vars(columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabelling existing columns (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengthen strings in region, severity \n",
    "labels1 = {\n",
    "    'West': [],\n",
    "    'Midwest': [],\n",
    "    'Great Plains': [],\n",
    "    'Northeast': [],\n",
    "    'South': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, region in zip(new_pph['time'].values, new_pph['REGION'].values):\n",
    "    if region == 'Northeas':\n",
    "        labels1['Northeast'].append(date)\n",
    "    elif region == 'Great Pl':\n",
    "        labels1['Great Plains'].append(date)\n",
    "    else:\n",
    "        labels1[region].append(date)\n",
    "\n",
    "\n",
    "labels2 = {\n",
    "    'sig_severe': [],\n",
    "    'severe': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, cat in zip(new_pph['time'].values, new_pph['WINDSP_CAT'].values):\n",
    "    if cat == 'sig_seve':\n",
    "        labels2['sig_severe'].append(date)\n",
    "    else:\n",
    "        labels2[cat].append(date)\n",
    "\n",
    "\n",
    "labels3 = {\n",
    "    'sig_severe': [],\n",
    "    'severe': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, cat in zip(new_pph['time'].values, new_pph['HAILSZ_CAT'].values):\n",
    "    if cat == 'sig_seve':\n",
    "        labels3['sig_severe'].append(date)\n",
    "    else:\n",
    "        labels3[cat].append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels1.values(), labels1.keys(), 'REGION', 'NONE')\n",
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels2.values(), labels2.keys(), 'WINDSP_CAT', 'NONE')\n",
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels3.values(), labels3.keys(), 'HAILSZ_CAT', 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = new_outlooks.rename(columns = {'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "new_reports = new_reports.rename(columns = {'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "new_pph = new_pph.rename({'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
