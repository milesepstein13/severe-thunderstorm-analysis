{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import math\n",
    "from sklearn.metrics import brier_score_loss, mean_squared_error\n",
    "from scipy.ndimage import uniform_filter\n",
    "import geocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsets outlooks, pph, and report data in various ways (add label to each datapoint for each method of subsetting so can be pulled out of full dataset later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data_location = 'data'\n",
    "labelled = False # if starting with already labelled data\n",
    "if labelled == True:\n",
    "    outlooks, pph, reports = read_datasets(data_location, 'labelled')\n",
    "else:\n",
    "    outlooks, pph, reports = read_datasets(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_conversions = {'PST': timedelta(hours=8),\n",
    "                  'MST': timedelta(hours=7),\n",
    "                  'CST': timedelta(hours=6),\n",
    "                  'CSt': timedelta(hours=6),\n",
    "                  'CSC': timedelta(hours=6),\n",
    "                  'SCT': timedelta(hours=6),\n",
    "                  'EST': timedelta(hours=5),\n",
    "                  'ESt': timedelta(hours=5),\n",
    "                  'PDT': timedelta(hours=7),\n",
    "                  'MDT': timedelta(hours=6),\n",
    "                  'CDT': timedelta(hours=5),\n",
    "                  'EDT': timedelta(hours=4),\n",
    "                  'HST': timedelta(hours=10),\n",
    "                  'SST': timedelta(hours=11),\n",
    "                  'GST': timedelta(hours=10),\n",
    "                  'AKS': timedelta(hours=9),\n",
    "                  'AST': timedelta(hours=4),\n",
    "                  'UNK': timedelta(hours=5),\n",
    "                  'GMT': timedelta(0)}\n",
    "\n",
    "def get_reports_date_strings(date_times, timezones):\n",
    "    # returns list of strings of date of given datetime and timezone (where day cutoffs are 12-12 UTC) formatted as 'YYYYMMDD0000'\n",
    "    for datetime, timezone, i in zip(date_times, timezones, range(len(timezones))):\n",
    "        #print(datetime + ' ' + timezone[:3])\n",
    "        datetime = parser.parse(datetime)\n",
    "        datetime = datetime + tz_conversions[timezone[:3]]\n",
    "        #print(datetime)\n",
    "        if (datetime.hour < 12):\n",
    "            datetime = datetime - timedelta(days = 1)\n",
    "        if datetime.year > 2049:\n",
    "            datetime = datetime - relativedelta(years = 100)\n",
    "        datetime = datetime.strftime(\"%Y%m%d\") + '0000'\n",
    "        if i == 0:\n",
    "            ret = [datetime]\n",
    "        else:\n",
    "            ret.append(datetime)\n",
    "    return ret\n",
    "\n",
    "def get_pph_date_strings(times):\n",
    "    # returns a list of strings of given dates formatted as 'YYYYMMDD0000'\n",
    "    for datetime, i in zip(times, range(len(times))):\n",
    "        string = datetime.dt.strftime(\"%Y%m%d\").values + '0000'\n",
    "        if i == 0:\n",
    "            ret = [string]\n",
    "        else:\n",
    "            ret.append(string)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dates to reports and pph in same format as in outlooks\n",
    "if labelled == False:\n",
    "    reports['DATE'] = get_reports_date_strings(reports['BEGIN_DATE_TIME'], reports['CZ_TIMEZONE']) \n",
    "    pph['time'] = get_pph_date_strings(pph.time) \n",
    "    # subset outlooks into only one day 1, two day 2, and one day 3 categorical outlooks \n",
    "    # day 3: cycle not -1. day 2: cycle not -1. Day 1: cycle 6. Category: categorical.\n",
    "    outlooks = outlooks[(((outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6)) | ((outlooks['DAY'] == 2) & (outlooks['CYCLE'] != -1)) | ((outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1)))\n",
    "            & (outlooks['CATEGORY'] == 'CATEGORICAL')]\n",
    "\n",
    "    # reset incicies\n",
    "    outlooks = outlooks.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outlooks_label(outlooks, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding a new column in outlooks\")\n",
    "    outlooks[label_name] = none_label\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label)\n",
    "        outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
    "    return outlooks\n",
    "\n",
    "def add_pph_label(pph, label_dates, labels, label_name, none_label):\n",
    "    # adds new variable with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding new variable in pph\")\n",
    "    if type(none_label) == str:\n",
    "        pph[label_name] = (('time'), np.full(len(pph['time']), none_label, dtype='<U16'))\n",
    "    else:\n",
    "        pph[label_name] = (('time'), np.full(len(pph['time']), none_label))\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        pph[label_name].loc[pph['time'].isin(dates)] = label  \n",
    "    return pph\n",
    "\n",
    "def add_reports_label(reports, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    reports[label_name] = none_label\n",
    "    print(\"adding a new column in reports\")\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "       #print(label)\n",
    "       reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
    "    return reports\n",
    "\n",
    "def add_labels(outlooks, pph, reports, label_dates, labels, label_name, none_label):\n",
    "    # adds labels, overwriting with later ones if a date has multiple labels\n",
    "    return(add_outlooks_label(outlooks, label_dates, labels, label_name, none_label), \n",
    "           add_pph_label(pph, label_dates, labels, label_name, none_label),\n",
    "           add_reports_label(reports, label_dates, labels, label_name, none_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, values, category_thresholds, name, add_cat = True, add_num = True, num_none_val = 0):\n",
    "    # from a list of dates and the corresponding values on those dates, adds numerical and categorical labels\n",
    "    # category thresholds: lower thresholds of each category for values (e.g. 60 for HIGH PPH)\n",
    "    num_dict = {\n",
    "        0: []\n",
    "    }\n",
    "    cat_dict  = dict((k, []) for k, v in category_thresholds.items())    \n",
    "        \n",
    "    for date, value in zip(dates, values):\n",
    "        # add to categorical dict\n",
    "        for cat in cat_dict:\n",
    "            if value >= category_thresholds[cat]:\n",
    "                cat_dict[cat].append(date)\n",
    "                break\n",
    "        \n",
    "        # add to numerical dict\n",
    "        if value in num_dict:\n",
    "            num_dict[value].append(date)\n",
    "        else:\n",
    "            num_dict[value] = [date]\n",
    "\n",
    "    if add_cat:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(cat_dict.values()), cat_dict.keys(), name + '_CAT', 'NONE')\n",
    "    if add_num:\n",
    "        (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(num_dict.values()), num_dict.keys(), name + '_NUM', num_none_val)\n",
    "\n",
    "    return(new_outlooks, new_pph, new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = outlooks\n",
    "new_pph = pph\n",
    "new_reports = reports.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALWAYS RUN THROUGH HERE. THEN TO ADD MORE LABELS, RUN JUST THE LABELLING YOU WISH TO BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max threshold forecasted for valid day to each datapoint\n",
    "\n",
    "categories = ['TSTM', 'MRGL', 'SLGT', 'ENH', 'MDT', 'HIGH']\n",
    "category_dates = []\n",
    "for category in categories:\n",
    "    category_dates.append(identify_dates_above_threshold(new_outlooks, category))\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, category_dates, categories, 'MAX_CAT', 'NONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by ramp up/down amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put new_outlooks in correct order\n",
    "\n",
    "new_outlooks['DATE_ORDER'] = 0\n",
    "for index, row in new_outlooks.iterrows():\n",
    "    if row['DAY'] == 3:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '1'\n",
    "    elif row['DAY'] == 1:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '4'\n",
    "    elif row['CYCLE'] == 7:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '2'\n",
    "    else:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '3'\n",
    "new_outlooks = new_outlooks.sort_values('DATE_ORDER')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_3_cutoff(outlooks):\n",
    "    # returns list of dates in outlooks \n",
    "    return outlooks[outlooks['DAY'] == 3]['DATE'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ramp_lists(outlooks, category_dict):\n",
    "\n",
    "    first_day_3 = get_day_3_cutoff(outlooks)\n",
    "    first_day_2_1 = '199707100000'\n",
    "    first_day_2_2 = '199504040000'\n",
    "\n",
    "    ramp_ups = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: [],\n",
    "        6: []\n",
    "    }\n",
    "\n",
    "    ramp_downs = {\n",
    "        0: [],\n",
    "        -1: [],\n",
    "        -2: [],\n",
    "        -3: [],\n",
    "        -4: [],\n",
    "        -5: [],\n",
    "        -6: []\n",
    "    }\n",
    "\n",
    "    ramp_categories = {\n",
    "        'up': [],\n",
    "        'down': [],\n",
    "        'both': [],\n",
    "        'neither': []\n",
    "    }\n",
    "\n",
    "    old_date = '0'\n",
    "    old_do = '0'\n",
    "    first = True\n",
    "\n",
    "    for index, row in outlooks.iterrows(): #iterrating through each polygon in the outlook dataset\n",
    "        cat = category_dict[row['THRESHOLD']]\n",
    "        do = row['DATE_ORDER']\n",
    "        date = row['DATE']\n",
    "\n",
    "        if date != old_date: # New date, save ramp up and ramp down and save alongside old date, then reset ramps, max and min categories seen, and do threshold\n",
    "            \n",
    "            \n",
    "            if first == True:\n",
    "                first = False\n",
    "            else:\n",
    "\n",
    "                if max_cat_do - min_cat_date > ramp_up:\n",
    "                    ramp_up = max_cat_do - min_cat_date\n",
    "                if max_cat_do - max_cat_date < ramp_down:\n",
    "                    ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "                if max_cat_do > max_cat_date:\n",
    "                    max_cat_date = max_cat_do\n",
    "                if max_cat_do < min_cat_date:\n",
    "                    min_cat_date = max_cat_do\n",
    "\n",
    "                ramp_ups[ramp_up].append(old_date)\n",
    "                ramp_downs[ramp_down].append(old_date)\n",
    "                if ramp_up > 0 and ramp_down < 0:\n",
    "                    ramp_categories['both'].append(old_date)\n",
    "                elif ramp_up > 0:\n",
    "                    ramp_categories['up'].append(old_date)\n",
    "                elif ramp_down < 0:\n",
    "                    ramp_categories['down'].append(old_date)\n",
    "                else:\n",
    "                    ramp_categories['neither'].append(old_date)\n",
    "\n",
    "            old_date = date\n",
    "            old_do = do\n",
    "            \n",
    "            ramp_down = 0\n",
    "            ramp_up = 0\n",
    "            max_cat_date = -1\n",
    "            \n",
    "    \n",
    "            if date > first_day_3 and do[-1] == '1': # Since 2002\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_1 and (do[-1] == '2' or do[-1] == '1'): # Since 1997. Check for 1 is in case there is an earlier forecast issued.\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_2 and (do[-1] == '3' or do[-1] == '2' or do[-1] == '1'): # Since 1995\n",
    "                min_cat_date = 5 \n",
    "            elif date <= first_day_2_2:\n",
    "                min_cat_date = 5\n",
    "            else:\n",
    "                min_cat_date = -1 # The first outlook on this date was not the earliest forecast it could have been, so it ramped up from no forecast\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "\n",
    "        elif do != old_do: # new outlook, update min and max categories seen, ramp value\n",
    "            if max_cat_do - min_cat_date > ramp_up:\n",
    "                ramp_up = max_cat_do - min_cat_date\n",
    "            if max_cat_do - max_cat_date < ramp_down:\n",
    "                ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "            if max_cat_do > max_cat_date:\n",
    "                max_cat_date = max_cat_do\n",
    "            if max_cat_do < min_cat_date:\n",
    "                min_cat_date = max_cat_do\n",
    "\n",
    "            old_do = do\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "        else: # Just another threshold within the same polygon\n",
    "            if cat > max_cat_do:\n",
    "                max_cat_do = cat\n",
    "            \n",
    "        \n",
    "    # for last iteration\n",
    "    ramp_ups[ramp_up].append(old_date)\n",
    "    ramp_downs[ramp_down].append(old_date)\n",
    "    if ramp_up > 0 and ramp_down < 0:\n",
    "        ramp_categories['both'].append(old_date)\n",
    "    elif ramp_up > 0:\n",
    "        ramp_categories['up'].append(old_date)\n",
    "    elif ramp_down < 0:\n",
    "        ramp_categories['down'].append(old_date)\n",
    "    else:\n",
    "        ramp_categories['neither'].append(old_date)\n",
    "\n",
    "    return(ramp_ups, ramp_downs, ramp_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and add ramp category for each datapoint. Potentially add 2 binary ramp up and ramp down (4 options are [up, down, up and down, niether]). How many forecasts to consider for each day? The day 3, both day 2, and the first day 1 (so 4 forecasts ramp)\n",
    "# dictionary of category to number\n",
    "category_dict = {\n",
    "    None : -1,\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "(ramp_ups, ramp_downs, ramp_categories) = create_ramp_lists(new_outlooks, category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ramp up\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_ups.values()), ramp_ups.keys(), 'RAMP_UP', 0)\n",
    "\n",
    "# ramp down\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_downs.values()), ramp_downs.keys(), 'RAMP_DOWN', 0)\n",
    "\n",
    "# ramp categories\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_categories.values()), ramp_categories.keys(), 'RAMP_CAT', 'neither')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(pph):\n",
    "    dates = list(set(pph['time'].values))\n",
    "    season_dates = [[], [], [], []]\n",
    "    for date in dates:\n",
    "        month = int(date[4:6])\n",
    "        if month == 12 or month < 3:\n",
    "            season_dates[0].append(date)\n",
    "        elif month < 6:\n",
    "            season_dates[1].append(date)\n",
    "        elif month < 9:\n",
    "            season_dates[2].append(date)\n",
    "        else:\n",
    "            season_dates[3].append(date)\n",
    "    return season_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column denoting season (4 met seasons as starting point)\n",
    "\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "season_dates = get_season_dates(new_pph)\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, season_dates, seasons, 'SEASON', 'NONE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same regions as Anderson-Frey 2016\n",
    "# center is grid square with highest p_perfect prob of at least one hazard (assuming each hazard in independent)\n",
    "\n",
    "# practically perfect probability of at least one hazard occuring near datapoint # TODO: max instead of combined?\n",
    "\n",
    "p_perfect_max = np.maximum(new_pph['p_perfect_wind'].values, np.maximum(new_pph['p_perfect_hail'].values, new_pph['p_perfect_tor'].values))\n",
    "new_pph = new_pph.assign(p_perfect_max=(('time', 'y', 'x'), p_perfect_max))\n",
    "\n",
    "new_pph['p_perfect_total'] = (1 - (1-new_pph['p_perfect_wind']/100)*(1-new_pph['p_perfect_hail']/100)*(1-new_pph['p_perfect_tor']/100))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect regions in chunks (since doing it all at once can time out)\n",
    "regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(lat, lon, geolocator):\n",
    "    \n",
    "    location = geolocator.reverse(str(lat)+\",\"+str(lon))\n",
    "    if location == None:\n",
    "        return None\n",
    "    address = location.raw['address']\n",
    "    state = address.get('state', '')\n",
    "    print(state)\n",
    "    return state\n",
    "\n",
    "def get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator):\n",
    "    state = get_state(lat, lon, geolocator)\n",
    "    if state == 'Colorado' or state == 'New Mexico':\n",
    "        if lon < west_threshold_co_nm:\n",
    "            return('West')\n",
    "        else:\n",
    "            return('Great Plains')\n",
    "    for region in regions_dict:\n",
    "        if state in regions_dict[region]:\n",
    "            return region\n",
    "    # Cases where highest PPH is out of contiguous states, usually just outside bc nearest gridpoint is on other side of border\n",
    "    if lat > 38:\n",
    "        if lon > -80.5:\n",
    "            return('Northeast')\n",
    "        elif lon > -104:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    else:\n",
    "        if lon > -93.8:\n",
    "            return('South')\n",
    "        elif lon > -106.5:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    return('NONE')\n",
    "\n",
    "\n",
    "def create_regions(pph):\n",
    "    regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"severe_thunderstorm_miles_new1\")\n",
    "    west_threshold_co_nm = -105\n",
    "    regions_dict = { # list of states fully within each region (doesn't include AK, HI, CO, NM)\n",
    "        'West': ['Washington', 'Oregon', 'California', 'Idaho', 'Montana', 'Wyoming', 'Utah', 'Arizona'],\n",
    "        'Midwest': ['North Dakota', 'South Dakota', 'Minnesota', 'Iowa', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana', 'Ohio', 'Kentucky'],\n",
    "        'Great Plains': ['Nebraska', 'Kansas', 'Oklahoma', 'Texas', 'Missouri'],\n",
    "        'Northeast': ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Rhode Island', 'Connecticut', 'New York', 'Pennsylvania', 'New Jersey', 'Delaware', 'Maryland', 'District of Columbia', 'West Virginia'],\n",
    "        'South': ['Virginia', 'Arkansas', 'Louisiana', 'Tennessee', 'Mississippi', 'Alabama', 'Georgia', 'North Carolina', 'South Carolina', 'Florida']\n",
    "    }\n",
    "\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        if date_pph['p_perfect_totalsvr'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding regions for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_totalsvr'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            region = get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator)\n",
    "            regions[region].append(date)\n",
    "            \n",
    "    return(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 100\n",
    "time_array = new_pph['time']\n",
    "chunk_size = math.ceil(len(time_array)/chunks)\n",
    "time_arrays = [time_array[i:i + chunk_size] for i in range(0, len(time_array), chunk_size)]\n",
    "for i in range(93, chunks):\n",
    "    chunk_regions = create_regions(new_pph.sel(time = time_arrays[i]))\n",
    "    for region in regions:\n",
    "        regions[region] += chunk_regions[region]\n",
    "    print('Added chunk ' + str(i) + ' to regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(regions.values()), regions.keys(), 'REGION_M', 'NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label with lat and lon of max pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lat_lon(pph):\n",
    "    # lat and lon of max pph for each day, with days with zero pph given the mean lat and lon of all other days\n",
    "    lats = []\n",
    "    lons = []\n",
    "    dates = []\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        dates.append(date)\n",
    "        if date_pph['p_perfect_totalsvr'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding lat/lon for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_totalsvr'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lats.append(lat)\n",
    "            lons.append(lon)\n",
    "        else:\n",
    "            lats.append(np.nan)\n",
    "            lons.append(np.nan)\n",
    "        \n",
    "    meanlat = np.nanmean(lats)\n",
    "    meanlon = np.nanmean(lons)\n",
    "    lats = np.nan_to_num(lats, nan = meanlat)\n",
    "    lons = np.nan_to_num(lons, nan = meanlon)\n",
    "\n",
    "    return(lats, lons, dates, meanlat, meanlon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lats, lons, dates, meanlat, meanlon = create_lat_lon(new_pph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, lats, {}, 'LAT', add_cat = False, num_none_val = meanlat)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, lons, {}, 'LON', add_cat = False, num_none_val = meanlon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by environmental data (to do later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by max total pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent = True\n",
    "# testing add numerical_categorical_columns\n",
    "dates = []\n",
    "pphs_d = []\n",
    "pphs_i = []\n",
    "pphs = []\n",
    "\n",
    "for date, date_pph in new_pph.groupby('time'):\n",
    "    dates.append(date)\n",
    "    pphs_d.append(float(date_pph['p_perfect_max'].max().values))\n",
    "    pphs_i.append(float(date_pph['p_perfect_total'].max().values))\n",
    "    pphs.append(float(date_pph['p_perfect_totalsvr'].max().values))\n",
    "\n",
    "pph_thresholds = { \n",
    "    'HIGH': 60,\n",
    "    'MDT': 45,\n",
    "    'ENH': 30,\n",
    "    'SLGT': 15,\n",
    "    'MRGL': 5,\n",
    "    'ZERO': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs_d, pph_thresholds, 'PPH_D', num_none_val = 0.0)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs_i, pph_thresholds, 'PPH_I', num_none_val = 0.0)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs, pph_thresholds, 'PPH', num_none_val = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of storm reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_severe = new_reports\n",
    "new_reports.loc[new_reports['MAGNITUDE'] == '', 'MAGNITUDE'] = 0\n",
    "reports_severe = reports_severe[(reports_severe['EVENT_TYPE'] == 'Tornado') | \n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind') & (reports_severe['MAGNITUDE'].astype(float) >= 50)) |\n",
    "                             ((reports_severe['EVENT_TYPE'] == 'Hail') & (reports_severe['MAGNITUDE'].astype(float) >= 1))]\n",
    "full_dates = reports_severe['DATE']\n",
    "c = Counter(full_dates)\n",
    "\n",
    "num_reports_thresholds = {\n",
    "    '1000': 1000,\n",
    "    '500': 500,\n",
    "    '100': 100,\n",
    "    '50': 50,\n",
    "    '10': 10,\n",
    "    '1': 1,\n",
    "    '0': 0\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, c.keys(), c.values(), num_reports_thresholds, 'REPORT', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of each type of report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tornado_reports = new_reports[new_reports['EVENT_TYPE'] == 'Tornado']\n",
    "\n",
    "wind_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Thunderstorm Wind']\n",
    "\n",
    "hail_reports = reports_severe[reports_severe['EVENT_TYPE'] == 'Hail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tornado_dates = tornado_reports['DATE']\n",
    "tornado_c = Counter(tornado_dates)\n",
    "\n",
    "wind_dates = wind_reports['DATE']\n",
    "wind_c = Counter(wind_dates)\n",
    "\n",
    "hail_dates = hail_reports['DATE']\n",
    "hail_c = Counter(hail_dates)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, tornado_c.keys(), tornado_c.values(), num_reports_thresholds, 'TOR', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, wind_c.keys(), wind_c.values(), num_reports_thresholds, 'WIND', add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hail_c.keys(), hail_c.values(), num_reports_thresholds, 'HAIL', add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by size of largest report of each type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tor: Categorical. Decide if we want to combine f and ef max \n",
    "strongest_tornadoes = tornado_reports.groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "strongest_tornadoes_without_u = tornado_reports[tornado_reports['TOR_F_SCALE'] != 'EFU'].groupby('DATE').agg({'TOR_F_SCALE': 'max'})['TOR_F_SCALE']\n",
    "\n",
    "tornado_strengths = {\n",
    "    'EFU': [],\n",
    "    '(E)F0': [],\n",
    "    '(E)F1': [],\n",
    "    '(E)F2': [],\n",
    "    '(E)F3': [],\n",
    "    '(E)F4': [],\n",
    "    '(E)F5': []\n",
    "}\n",
    "for date, s in zip(strongest_tornadoes.index, strongest_tornadoes):\n",
    "    if s == 'EFU':\n",
    "        if date in strongest_tornadoes_without_u:\n",
    "            tornado_strengths['(E)F' + strongest_tornadoes_without_u[date][-1]].append(date)\n",
    "        else:\n",
    "            tornado_strengths['EFU'].append(date)\n",
    "\n",
    "    elif s != '':\n",
    "        tornado_strengths['(E)F' + s[-1]].append(date)\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(tornado_strengths.values()), tornado_strengths.keys(), 'TOR_F', 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both wind and hail have a few weird outliers: Wind all pre-2000. Hail has 2 0.00 and 1 0.01 in 2005. Unmeasured wind reports are 50 (knots)\n",
    "winds = wind_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})\n",
    "hails = hail_reports.groupby('DATE').agg({'MAGNITUDE': 'max'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_thresholds = {\n",
    "    'sig_severe': 65,\n",
    "    'severe': 50,\n",
    "    'NONE': 0\n",
    "}\n",
    "\n",
    "hail_thresholds = {\n",
    "    'sig_severe': 2,\n",
    "    'severe': 1,\n",
    "    'NONE': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, winds.index, winds.astype(float)['MAGNITUDE'].values, wind_thresholds, 'WINDSP', num_none_val = 0.0)\n",
    "add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, hails.index, hails.astype(float)['MAGNITUDE'].values, hail_thresholds, 'HAILSZ', num_none_val = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by accuracy of forecast\n",
    "Brier, FSS, SAL, and/or wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need gridded outlooks and reports\n",
    "grid_outlook_location = 'data/outlooks/grid_outlooks.nc'\n",
    "grid_report_location = 'data/storm_reports/grid_reports.nc'\n",
    "\n",
    "grid_outlooks = xr.open_dataset(grid_outlook_location)\n",
    "grid_reports = xr.open_dataset(grid_report_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(grid_outlooks, grid_reports, outlook_day_str = 'Day 1', report_type_str = 'Total Reports'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob'].data.flatten()\n",
    "        verification = gr.sel(time = date)['bool'].data.flatten()\n",
    "        scores.append(brier_score_loss(verification, outlooks))\n",
    "    return(scores)\n",
    "\n",
    "bs = brier_score(grid_outlooks, grid_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, bs, {}, 'BS', num_none_val = 0.0, add_cat = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhood probabilistic verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth (with variable neighborhood size) and then do brier score\n",
    "def neighborhood_verification(grid_outlooks, grid_reports, neighborhood_size, outlook_day_str = 'Day 1', report_type_str = 'Total Reports'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = uniform_filter(go.sel(time = date)['prob'], size=neighborhood_size, mode='constant')\n",
    "        verification = uniform_filter(gr.sel(time = date)['bool'].astype(float), size=neighborhood_size, mode='constant')\n",
    "        scores.append(np.mean((outlooks - verification) ** 2))\n",
    "    return scores\n",
    "npv = neighborhood_verification(grid_outlooks, grid_reports, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, npv, {}, 'NEIGH', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE between outlooks and PPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do rmse\n",
    "def rmse_verification(grid_outlooks, pph, outlook_day_str = 'Day 1', pph_hazard_str = 'max'):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    scores = []\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob']\n",
    "        p = pph.sel(time = date)['p_perfect_' + pph_hazard_str]/100\n",
    "        scores.append(mean_squared_error(p, outlooks, squared=False))\n",
    "    return(scores)\n",
    "        \n",
    "rmse = rmse_verification(grid_outlooks, pph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, rmse, {}, 'RMSE', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once grid outlooks updated, see with day 1\n",
    "# plt.scatter(bs, npv)\n",
    "#a = neighborhood_verification(grid_outlooks, grid_reports, 5, outlook_day_str='Day 3')\n",
    "#b = brier_score(grid_outlooks, grid_reports, outlook_day_str= 'Day 3')\n",
    "#c = rmse_verification(grid_outlooks, new_pph, outlook_day_str='Day 3')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(bs, npv, rmse, s = .1)\n",
    "ax.set_xlabel('Brier Score')\n",
    "ax.set_ylabel('Neighborhod Verification')\n",
    "ax.set_zlabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINGENCY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PC(a, b, c, d):\n",
    "    return np.divide(a + d, a + b + c + d)\n",
    "\n",
    "def POD(a, b, c, d):\n",
    "    r = np.divide(a, a + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FOM(a, b, c, d):\n",
    "    r = np.divide(c, a + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FAR(a, b, c, d):\n",
    "    r = np.divide(b, a + b)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def FOH(a, b, c, d):\n",
    "    r = np.divide(a, a + b)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def POFD(a, b, c, d):\n",
    "    return np.divide(b, b + d)\n",
    "\n",
    "def PCR(a, b, c, d): \n",
    "    return np.divide(d, b + d)\n",
    "\n",
    "def DFR(a, b, c, d):\n",
    "    return np.divide(c, c + d)\n",
    "\n",
    "def FOCN(a, b, c, d):\n",
    "    return np.divide(d, c + d)\n",
    "\n",
    "def CSI(a, b, c, d):\n",
    "    r = np.divide(a, a + b + c)\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n",
    "def Bias(a, b, c, d):\n",
    "    r = np.divide(a + b, a + c)\n",
    "    r[np.isnan(r)] = 1\n",
    "    return r\n",
    "\n",
    "def ETS(a, b, c, d):\n",
    "    bias = np.divide(Bias(a, b, c, d), a + b + c + d)\n",
    "    return np.divide(a - bias, a + b + c - bias)\n",
    "\n",
    "def TSS(a, b, c, d):\n",
    "    r = np.divide((np.multiply(a, d) - np.multiply(b, c)), np.multiply(a + c, b + d))\n",
    "    r[np.isnan(r)] = 0\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import contingency\n",
    "contingency = xr.load_dataset('data/contingency/contingencyfinal.nc')\n",
    "\n",
    "hazard_types= ['Wind', 'Hail', 'Tornado', 'All Hazard']\n",
    "\n",
    "hazard_string_dict = {\n",
    "    'Wind': '_W',\n",
    "    'Hail': '_H',\n",
    "    'Tornado': '_T',\n",
    "    'All Hazard': ''\n",
    "}\n",
    "\n",
    "# loop through functions (without repeating inverses)/hazard type, add numerical/categorical columns\n",
    "for fn in [PC, POD, FAR, POFD, DFR, CSI, Bias, ETS, TSS]:\n",
    "    for hazard in hazard_types:\n",
    "        scores = fn(contingency.sel(hazard = hazard)['a'], contingency.sel(hazard = hazard)['b'], contingency.sel(hazard = hazard)['c'], contingency.sel(hazard = hazard)['d']).values\n",
    "        new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, contingency['time'].data, scores, {}, fn.__name__ + hazard_string_dict[hazard], add_cat = False, num_none_val = 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POD and FAR--DON'T RUN (accounted for in contingency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1', report_type_str = 'Total Reports', squared = False):\n",
    "    go = grid_outlooks.sel(outlook = outlook_day_str)\n",
    "    gr = grid_reports.sel(hazard = report_type_str)\n",
    "\n",
    "    pods = []\n",
    "    farts = []\n",
    "    hrs = []\n",
    "    fars = []\n",
    "\n",
    "    for date in grid_outlooks['time']:\n",
    "        outlooks = go.sel(time = date)['prob'].data.flatten()\n",
    "        verification = gr.sel(time = date)['bool'].data.flatten()\n",
    "\n",
    "        pod = np.mean(outlooks[verification]) \n",
    "        if np.isnan(pod):\n",
    "            pods.append(0.0)\n",
    "        else:\n",
    "            if squared:\n",
    "                pods.append(pod**2)\n",
    "            else:\n",
    "                pods.append(pod)\n",
    "\n",
    "        fart = np.mean(outlooks[~verification])\n",
    "        if np.isnan(fart):\n",
    "            farts.append(0.0)\n",
    "        else:\n",
    "            if squared:\n",
    "                farts.append(fart**2)\n",
    "            else:\n",
    "                farts.append(fart)\n",
    "        \n",
    "        hr = np.sum(outlooks[verification]) / np.sum(outlooks)\n",
    "        if np.isnan(hr):\n",
    "            hrs.append(0.0)\n",
    "        else:\n",
    "            hrs.append(hr)\n",
    "\n",
    "        far = np.sum(outlooks[~verification]) / np.sum(outlooks)\n",
    "        if np.isnan(far):\n",
    "            fars.append(0.0)\n",
    "        else:\n",
    "            fars.append(far)\n",
    "\n",
    "    return(pods, farts, hrs, fars)\n",
    "\n",
    "pod, fart, hr, far = pod_farate_hr_faratio(grid_outlooks, grid_reports)\n",
    "hail_pod, hail_fart, hail_hr, hail_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Hail', report_type_str = 'Hail')\n",
    "wind_pod, wind_fart, wind_hr, wind_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Wind', report_type_str = 'Wind')\n",
    "tornado_pod, tornado_fart, tornado_hr, tornado_far = pod_farate_hr_faratio(grid_outlooks, grid_reports, outlook_day_str = 'Day 1 Tornado', report_type_str = 'Tornado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, pod, {}, 'POD', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, fart, {}, 'FART', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hr, {}, 'HR', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, far, {}, 'FAR', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_pod, {}, 'POD_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_fart, {}, 'FART_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_hr, {}, 'HR_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, hail_far, {}, 'FAR_H', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_pod, {}, 'POD_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_fart, {}, 'FART_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_hr, {}, 'HR_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, wind_far, {}, 'FAR_W', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_pod, {}, 'POD_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_fart, {}, 'FART_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_hr, {}, 'HR_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, grid_outlooks['time'].data, tornado_far, {}, 'FAR_T', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_dataset = xr.load_dataset('data/displacement/displacements.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_times = displacement_dataset['time'].data\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['e_shift'].data, {}, 'E_SH', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['n_shift'].data, {}, 'N_SH', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'All Hazard')['total_div'].data, {}, 'DIV', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['e_shift'].data, {}, 'E_SH_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['n_shift'].data, {}, 'N_SH_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Wind')['total_div'].data, {}, 'DIV_W', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['e_shift'].data, {}, 'E_SH_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['n_shift'].data, {}, 'N_SH_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Hail')['total_div'].data, {}, 'DIV_H', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['e_shift'].data, {}, 'E_SH_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['n_shift'].data, {}, 'N_SH_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacement_dataset.sel(hazard = 'Tornado')['total_div'].data, {}, 'DIV_T', num_none_val = 0.0, add_cat = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacements_recentered = xr.load_dataset('~/recentered_data/displacements_recentered.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacement_times = displacements_recentered['time'].data\n",
    "\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'All Hazard')['e_shift_e'].data, {}, 'EE_S', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'All Hazard')['e_shift_w'].data, {}, 'EW_S', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "ew_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Wind')['e_shift_e'].data, {}, 'EE_S_W', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Wind')['e_shift_w'].data, {}, 'EW_S_W', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "ew_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Hail')['e_shift_e'].data, {}, 'EE_S_H', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Hail')['e_shift_w'].data, {}, 'EW_S_H', num_none_val = 0.0, add_cat = False)\n",
    "\n",
    "ew_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Tornado')['e_shift_e'].data, {}, 'EE_S_T', num_none_val = 0.0, add_cat = False)\n",
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, displacement_times, displacements_recentered.sel(hazard = 'Tornado')['e_shift_w'].data, {}, 'EW_S_T', num_none_val = 0.0, add_cat = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave labelled data\n",
    "outlook_save_location = 'data/outlooks'\n",
    "pph_save_location = 'data/pph'\n",
    "report_save_location = 'data/storm_reports'\n",
    "\n",
    "new_outlooks.to_file(outlook_save_location + '/labelled_outlooks.shp')\n",
    "if labelled == True:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled_pph2.nc') \n",
    "else:\n",
    "    new_pph.to_netcdf(pph_save_location + '/labelled_pph.nc') \n",
    "new_reports.to_csv(report_save_location + '/labelled_reports.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMOVING COLUMNS (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['FART_NUM', 'HR_NUM', 'FART_H_NUM', 'HR_H_NUM', 'FART_W_NUM', 'HR_W_NUM', 'FART_T_NUM', 'HR_T_NUM']\n",
    "new_outlooks = new_outlooks.drop(columns = columns_to_remove)\n",
    "new_reports = new_reports.drop(columns = columns_to_remove)\n",
    "new_pph = new_pph.drop_vars(columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabelling existing columns (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengthen strings in region, severity \n",
    "labels1 = {\n",
    "    'West': [],\n",
    "    'Midwest': [],\n",
    "    'Great Plains': [],\n",
    "    'Northeast': [],\n",
    "    'South': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, region in zip(new_pph['time'].values, new_pph['REGION'].values):\n",
    "    if region == 'Northeas':\n",
    "        labels1['Northeast'].append(date)\n",
    "    elif region == 'Great Pl':\n",
    "        labels1['Great Plains'].append(date)\n",
    "    else:\n",
    "        labels1[region].append(date)\n",
    "\n",
    "\n",
    "labels2 = {\n",
    "    'sig_severe': [],\n",
    "    'severe': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, cat in zip(new_pph['time'].values, new_pph['WINDSP_CAT'].values):\n",
    "    if cat == 'sig_seve':\n",
    "        labels2['sig_severe'].append(date)\n",
    "    else:\n",
    "        labels2[cat].append(date)\n",
    "\n",
    "\n",
    "labels3 = {\n",
    "    'sig_severe': [],\n",
    "    'severe': [],\n",
    "    'NONE': []\n",
    "}\n",
    "for date, cat in zip(new_pph['time'].values, new_pph['HAILSZ_CAT'].values):\n",
    "    if cat == 'sig_seve':\n",
    "        labels3['sig_severe'].append(date)\n",
    "    else:\n",
    "        labels3[cat].append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels1.values(), labels1.keys(), 'REGION', 'NONE')\n",
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels2.values(), labels2.keys(), 'WINDSP_CAT', 'NONE')\n",
    "new_outlooks, new_pph, new_reports = add_labels(new_outlooks, new_pph, new_reports, labels3.values(), labels3.keys(), 'HAILSZ_CAT', 'NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = new_outlooks.rename(columns = {'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "new_reports = new_reports.rename(columns = {'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "new_pph = new_pph.rename({'NEIGH_VER_NUM': 'NEIGH_NUM', 'RMSE_VER_NUM': 'RMSE_NUM', 'BS_DAY_1_NUM': 'BS_NUM' })\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
