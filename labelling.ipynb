{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_filter import *\n",
    "from utils_datetime import *\n",
    "from utils_geography import *\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsets outlooks, pph, and report data in various ways (add label to each datapoint for each method of subsetting so can be pulled out of full dataset later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading outlooks\n",
      "reading pph\n",
      "reading storm reports\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "data_location = 'data'\n",
    "labelled = True # if starting with already labelled data\n",
    "outlooks, pph, reports = read_datasets(data_location, 'labelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_conversions = {'PST': timedelta(hours=8),\n",
    "                  'MST': timedelta(hours=7),\n",
    "                  'CST': timedelta(hours=6),\n",
    "                  'CSt': timedelta(hours=6),\n",
    "                  'CSC': timedelta(hours=6),\n",
    "                  'SCT': timedelta(hours=6),\n",
    "                  'EST': timedelta(hours=5),\n",
    "                  'ESt': timedelta(hours=5),\n",
    "                  'PDT': timedelta(hours=7),\n",
    "                  'MDT': timedelta(hours=6),\n",
    "                  'CDT': timedelta(hours=5),\n",
    "                  'EDT': timedelta(hours=4),\n",
    "                  'HST': timedelta(hours=10),\n",
    "                  'SST': timedelta(hours=11),\n",
    "                  'GST': timedelta(hours=10),\n",
    "                  'AKS': timedelta(hours=9),\n",
    "                  'AST': timedelta(hours=4),\n",
    "                  'UNK': timedelta(hours=5),\n",
    "                  'GMT': timedelta(0)}\n",
    "\n",
    "def get_reports_date_strings(date_times, timezones):\n",
    "    # returns list of strings of date of given datetime and timezone (where day cutoffs are 12-12 UTC) formatted as 'YYYYMMDD0000'\n",
    "    for datetime, timezone, i in zip(date_times, timezones, range(len(timezones))):\n",
    "        #print(datetime + ' ' + timezone[:3])\n",
    "        datetime = parser.parse(datetime)\n",
    "        datetime = datetime + tz_conversions[timezone[:3]]\n",
    "        #print(datetime)\n",
    "        if (datetime.hour < 12):\n",
    "            datetime = datetime - timedelta(days = 1)\n",
    "        if datetime.year > 2049:\n",
    "            datetime = datetime - relativedelta(years = 100)\n",
    "        datetime = datetime.strftime(\"%Y%m%d\") + '0000'\n",
    "        if i == 0:\n",
    "            ret = [datetime]\n",
    "        else:\n",
    "            ret.append(datetime)\n",
    "    return ret\n",
    "\n",
    "def get_pph_date_strings(times):\n",
    "    # returns a list of strings of given dates formatted as 'YYYYMMDD0000'\n",
    "    for datetime, i in zip(times, range(len(times))):\n",
    "        string = datetime.dt.strftime(\"%Y%m%d\").values + '0000'\n",
    "        if i == 0:\n",
    "            ret = [string]\n",
    "        else:\n",
    "            ret.append(string)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dates to reports and pph in same format as in outlooks\n",
    "if labelled == False:\n",
    "    reports['DATE'] = get_reports_date_strings(reports['BEGIN_DATE_TIME'], reports['CZ_TIMEZONE']) \n",
    "    pph['time'] = get_pph_date_strings(pph.time) \n",
    "    # subset outlooks into only one day 1, two day 2, and one day 3 categorical outlooks \n",
    "    # day 3: cycle not -1. day 2: cycle not -1. Day 1: cycle 6. Category: categorical.\n",
    "    outlooks = outlooks[(((outlooks['DAY'] == 1) & (outlooks['CYCLE'] == 6)) | ((outlooks['DAY'] == 2) & (outlooks['CYCLE'] != -1)) | ((outlooks['DAY'] == 3) & (outlooks['CYCLE'] != -1)))\n",
    "            & (outlooks['CATEGORY'] == 'CATEGORICAL')]\n",
    "\n",
    "    # reset incicies\n",
    "    outlooks = outlooks.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outlooks_label(outlooks, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding a new column in outlooks\")\n",
    "    outlooks[label_name] = none_label\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label)\n",
    "        outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n",
    "    return outlooks\n",
    "\n",
    "def add_pph_label(pph, label_dates, labels, label_name, none_label):\n",
    "    # adds new variable with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    print(\"adding new variable in pph\")\n",
    "    pph[label_name] = (('time'), np.full(len(pph['time']), none_label))\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "        #print(label) # TODO: may only be adding first 4 letters of label\n",
    "        pph[label_name].loc[pph['time'].isin(dates)] = label  \n",
    "    return pph\n",
    "\n",
    "def add_reports_label(reports, label_dates, labels, label_name, none_label):\n",
    "    # adds new column with values from labels on the corresponding list of dates in label_dates (DONE)\n",
    "    reports[label_name] = none_label\n",
    "    print(\"adding a new column in reports\")\n",
    "    for label, dates in zip(labels, label_dates):\n",
    "       #print(label)\n",
    "       reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n",
    "    return reports\n",
    "\n",
    "def add_labels(outlooks, pph, reports, label_dates, labels, label_name, none_label):\n",
    "    # adds labels, overwriting with later ones if a date has multiple labels\n",
    "    return(add_outlooks_label(outlooks, label_dates, labels, label_name, none_label), \n",
    "           add_pph_label(pph, label_dates, labels, label_name, none_label),\n",
    "           add_reports_label(reports, label_dates, labels, label_name, none_label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, values, category_thresholds, name):\n",
    "    # from a list of dates and the corresponding values on those dates, adds numerical and categorical labels\n",
    "    # category thresholds: lower thresholds of each category for values (e.g. 60 for HIGH PPH)\n",
    "    num_dict = {\n",
    "        0: []\n",
    "    }\n",
    "    cat_dict  = dict((k, []) for k, v in category_thresholds.items())    \n",
    "        \n",
    "    for date, value in zip(dates, values):\n",
    "        # add to categorical dict\n",
    "        for cat in cat_dict:\n",
    "            if value >= category_thresholds[cat]:\n",
    "                # ... copy from non-generalized PPH code below\n",
    "                cat_dict[cat].append(date)\n",
    "                break\n",
    "        \n",
    "        # add to numerical dict\n",
    "        if value in num_dict:\n",
    "            num_dict[value].append(date)\n",
    "        else:\n",
    "            num_dict[value] = [date]\n",
    "\n",
    "    (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(cat_dict.values()), cat_dict.keys(), name + '_CAT', 'NONE')\n",
    "    (new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(num_dict.values()), num_dict.keys(), name + '_NUM', 0)\n",
    "\n",
    "    return(new_outlooks, new_pph, new_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_outlooks = outlooks\n",
    "new_pph = pph\n",
    "new_reports = reports.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALWAYS RUN THROUGH HERE. THEN TO ADD MORE LABELS, RUN JUST THE LABELLING YOU WISH TO BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add max threshold forecasted for valid day to each datapoint\n",
    "\n",
    "categories = ['TSTM', 'MRGL', 'SLGT', 'ENH', 'MDT', 'HIGH']\n",
    "category_dates = []\n",
    "for category in categories:\n",
    "    category_dates.append(identify_dates_above_threshold(new_outlooks, category))\n",
    "\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, category_dates, categories, 'MAX_CAT', 'NONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by ramp up/down amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put new_outlooks in correct order\n",
    "\n",
    "new_outlooks['DATE_ORDER'] = 0\n",
    "for index, row in new_outlooks.iterrows():\n",
    "    if row['DAY'] == 3:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '1'\n",
    "    elif row['DAY'] == 1:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '4'\n",
    "    elif row['CYCLE'] == 7:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '2'\n",
    "    else:\n",
    "        new_outlooks.at[index, 'DATE_ORDER'] = row['DATE'] + '3'\n",
    "new_outlooks = new_outlooks.sort_values('DATE_ORDER')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_3_cutoff(outlooks):\n",
    "    # returns list of dates in outlooks \n",
    "    return outlooks[outlooks['DAY'] == 3]['DATE'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ramp_lists(outlooks, category_dict):\n",
    "\n",
    "    first_day_3 = get_day_3_cutoff(outlooks)\n",
    "    first_day_2_1 = '199707100000'\n",
    "    first_day_2_2 = '199504040000'\n",
    "\n",
    "    ramp_ups = {\n",
    "        0: [],\n",
    "        1: [],\n",
    "        2: [],\n",
    "        3: [],\n",
    "        4: [],\n",
    "        5: [],\n",
    "        6: []\n",
    "    }\n",
    "\n",
    "    ramp_downs = {\n",
    "        0: [],\n",
    "        -1: [],\n",
    "        -2: [],\n",
    "        -3: [],\n",
    "        -4: [],\n",
    "        -5: [],\n",
    "        -6: []\n",
    "    }\n",
    "\n",
    "    ramp_categories = {\n",
    "        'up': [],\n",
    "        'down': [],\n",
    "        'both': [],\n",
    "        'neither': []\n",
    "    }\n",
    "\n",
    "    old_date = '0'\n",
    "    old_do = '0'\n",
    "    first = True\n",
    "\n",
    "    for index, row in outlooks.iterrows(): #iterrating through each polygon in the outlook dataset\n",
    "        cat = category_dict[row['THRESHOLD']]\n",
    "        do = row['DATE_ORDER']\n",
    "        date = row['DATE']\n",
    "\n",
    "        if date != old_date: # New date, save ramp up and ramp down and save alongside old date, then reset ramps, max and min categories seen, and do threshold\n",
    "            \n",
    "            \n",
    "            if first == True:\n",
    "                first = False\n",
    "            else:\n",
    "\n",
    "                if max_cat_do - min_cat_date > ramp_up:\n",
    "                    ramp_up = max_cat_do - min_cat_date\n",
    "                if max_cat_do - max_cat_date < ramp_down:\n",
    "                    ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "                if max_cat_do > max_cat_date:\n",
    "                    max_cat_date = max_cat_do\n",
    "                if max_cat_do < min_cat_date:\n",
    "                    min_cat_date = max_cat_do\n",
    "\n",
    "                ramp_ups[ramp_up].append(old_date)\n",
    "                ramp_downs[ramp_down].append(old_date)\n",
    "                if ramp_up > 0 and ramp_down < 0:\n",
    "                    ramp_categories['both'].append(old_date)\n",
    "                elif ramp_up > 0:\n",
    "                    ramp_categories['up'].append(old_date)\n",
    "                elif ramp_down < 0:\n",
    "                    ramp_categories['down'].append(old_date)\n",
    "                else:\n",
    "                    ramp_categories['neither'].append(old_date)\n",
    "\n",
    "            old_date = date\n",
    "            old_do = do\n",
    "            \n",
    "            ramp_down = 0\n",
    "            ramp_up = 0\n",
    "            max_cat_date = -1\n",
    "            \n",
    "    \n",
    "            if date > first_day_3 and do[-1] == '1': # Since 2002\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_1 and (do[-1] == '2' or do[-1] == '1'): # Since 1997. Check for 1 is in case there is an earlier forecast issued.\n",
    "                min_cat_date = 5 \n",
    "            elif date > first_day_2_2 and (do[-1] == '3' or do[-1] == '2' or do[-1] == '1'): # Since 1995\n",
    "                min_cat_date = 5 \n",
    "            elif date <= first_day_2_2:\n",
    "                min_cat_date = 5\n",
    "            else:\n",
    "                min_cat_date = -1 # The first outlook on this date was not the earliest forecast it could have been, so it ramped up from no forecast\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "\n",
    "        elif do != old_do: # new outlook, update min and max categories seen, ramp value\n",
    "            if max_cat_do - min_cat_date > ramp_up:\n",
    "                ramp_up = max_cat_do - min_cat_date\n",
    "            if max_cat_do - max_cat_date < ramp_down:\n",
    "                ramp_down = max_cat_do - max_cat_date\n",
    "\n",
    "            if max_cat_do > max_cat_date:\n",
    "                max_cat_date = max_cat_do\n",
    "            if max_cat_do < min_cat_date:\n",
    "                min_cat_date = max_cat_do\n",
    "\n",
    "            old_do = do\n",
    "\n",
    "            max_cat_do = cat\n",
    "\n",
    "        else: # Just another threshold within the same polygon\n",
    "            if cat > max_cat_do:\n",
    "                max_cat_do = cat\n",
    "            \n",
    "        \n",
    "    # for last iteration\n",
    "    ramp_ups[ramp_up].append(old_date)\n",
    "    ramp_downs[ramp_down].append(old_date)\n",
    "    if ramp_up > 0 and ramp_down < 0:\n",
    "        ramp_categories['both'].append(old_date)\n",
    "    elif ramp_up > 0:\n",
    "        ramp_categories['up'].append(old_date)\n",
    "    elif ramp_down < 0:\n",
    "        ramp_categories['down'].append(old_date)\n",
    "    else:\n",
    "        ramp_categories['neither'].append(old_date)\n",
    "\n",
    "    return(ramp_ups, ramp_downs, ramp_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and add ramp category for each datapoint. Potentially add 2 binary ramp up and ramp down (4 options are [up, down, up and down, niether]). How many forecasts to consider for each day? The day 3, both day 2, and the first day 1 (so 4 forecasts ramp)\n",
    "# dictionary of category to number\n",
    "category_dict = {\n",
    "    None : -1,\n",
    "    'NONE' : -1,\n",
    "    'TSTM': 0,\n",
    "    'MRGL': 1,\n",
    "    'SLGT': 2,\n",
    "    'ENH': 3,\n",
    "    'MDT': 4,\n",
    "    'HIGH': 5\n",
    "}\n",
    "\n",
    "(ramp_ups, ramp_downs, ramp_categories) = create_ramp_lists(new_outlooks, category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ramp up\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_ups.values()), ramp_ups.keys(), 'RAMP_UP', 0)\n",
    "\n",
    "# ramp down\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_downs.values()), ramp_downs.keys(), 'RAMP_DOWN', 0)\n",
    "\n",
    "# ramp categories\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(ramp_categories.values()), ramp_categories.keys(), 'RAMP_CATEGORIES', 'neither')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by accurate/inaccurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider forecast issue time for day 1--subset reports (and revise pph?)--look at other studies to see how they handled multiple day 1 outlooks (see the one in slack). Starting point should be use first forecasts, later ones would be a different analysis for another time. Decide if we should denote these somehow.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_dates(pph):\n",
    "    dates = list(set(pph['time'].values))\n",
    "    season_dates = [[], [], [], []]\n",
    "    for date in dates:\n",
    "        month = int(date[4:6])\n",
    "        if month == 12 or month < 3:\n",
    "            season_dates[0].append(date)\n",
    "        elif month < 6:\n",
    "            season_dates[1].append(date)\n",
    "        elif month < 9:\n",
    "            season_dates[2].append(date)\n",
    "        else:\n",
    "            season_dates[3].append(date)\n",
    "    return season_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column denoting season (4 met seasons as starting point)\n",
    "\n",
    "seasons = ['Winter', 'Spring', 'Summer', 'Fall']\n",
    "season_dates = get_season_dates(new_pph)\n",
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, season_dates, seasons, 'SEASON', 'xxxxxx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide how to do... lower priotity, could do by state of center of highest category (or center of PPH?) Could start with super basic version\n",
    "# same regions as Anderson-Frey 2016\n",
    "# center is grid square with highest p_perfect prob of at least one hazard (assuming each hazard in independent)\n",
    "\n",
    "# practically perfect probability of at least one hazard occuring near datapoint\n",
    "new_pph['p_perfect_total'] = (1 - (1-new_pph['p_perfect_wind']/100)*(1-new_pph['p_perfect_hail']/100)*(1-new_pph['p_perfect_tor']/100))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect regions in chunks (since doing it all at once can time out)\n",
    "regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(lat, lon, geolocator):\n",
    "    location = geolocator.reverse(str(lat)+\",\"+str(lon))\n",
    "    if location == None:\n",
    "        return None\n",
    "    address = location.raw['address']\n",
    "    state = address.get('state', '')\n",
    "    return state\n",
    "\n",
    "def get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator):\n",
    "    state = get_state(lat, lon, geolocator)\n",
    "    if state == 'Colorado' or state == 'New Mexico':\n",
    "        if lon < west_threshold_co_nm:\n",
    "            return('West')\n",
    "        else:\n",
    "            return('Great Plains')\n",
    "    for region in regions_dict:\n",
    "        if state in regions_dict[region]:\n",
    "            return region\n",
    "    # Cases where highest PPH is out of contiguous states, usually just outside bc nearest gridpoint is on other side of border\n",
    "    if lat > 38:\n",
    "        if lon > -80.5:\n",
    "            return('Northeast')\n",
    "        elif lon > -104:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    else:\n",
    "        if lon > -93.8:\n",
    "            return('South')\n",
    "        elif lon > -106.5:\n",
    "            return('Great Plains')\n",
    "        else:\n",
    "            return('West')\n",
    "    return('NONE')\n",
    "\n",
    "\n",
    "def create_regions(pph):\n",
    "    regions = {\n",
    "        'West': [],\n",
    "        'Midwest': [],\n",
    "        'Great Plains': [],\n",
    "        'Northeast': [],\n",
    "        'South': [],\n",
    "        'NONE': []\n",
    "    }\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"severe_thunderstorm_miles\")\n",
    "    west_threshold_co_nm = -105\n",
    "    regions_dict = { # list of states fully within each region (doesn't include AK, HI, CO, NM)\n",
    "        'West': ['Washington', 'Oregon', 'California', 'Idaho', 'Montana', 'Wyoming', 'Utah', 'Arizona'],\n",
    "        'Midwest': ['North Dakota', 'South Dakota', 'Minnesota', 'Iowa', 'Wisconsin', 'Illinois', 'Michigan', 'Indiana', 'Ohio', 'Kentucky'],\n",
    "        'Great Plains': ['Nebraska', 'Kansas', 'Oklahoma', 'Texas', 'Missouri'],\n",
    "        'Northeast': ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Rhode Island', 'Connecticut', 'New York', 'Pennsylvania', 'New Jersey', 'Delaware', 'Maryland', 'District of Columbia', 'West Virginia'],\n",
    "        'South': ['Virginia', 'Arkansas', 'Louisiana', 'Tennessee', 'Mississippi', 'Alabama', 'Georgia', 'North Carolina', 'South Carolina', 'Florida']\n",
    "    }\n",
    "\n",
    "    old_year = ''\n",
    "    for date, date_pph in pph.groupby('time'):\n",
    "        if date_pph['p_perfect_total'].max() > 0:\n",
    "            year = date[0:4]\n",
    "            if year != old_year:\n",
    "                print(\"Finding regions for \" + year)\n",
    "                old_year = year\n",
    "            max_coords = date_pph['p_perfect_total'].argmax(dim = ['x', 'y'])\n",
    "            max_x_coord = max_coords['x'].values\n",
    "            max_y_coord = max_coords['y'].values\n",
    "            lat = date_pph['lat'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            lon = date_pph['lon'].loc[dict(x = max_x_coord, y = max_y_coord)].values\n",
    "            region = get_region(lat, lon, west_threshold_co_nm, regions_dict, geolocator)\n",
    "            regions[region].append(date)\n",
    "            \n",
    "    return(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = 10\n",
    "time_array = new_pph['time']\n",
    "chunk_size = math.ceil(len(time_array)/chunks)\n",
    "time_arrays = [time_array[i:i + chunk_size] for i in range(0, len(time_array), chunk_size)]\n",
    "for i in range(2, 3):\n",
    "    chunk_regions = create_regions(new_pph.sel(time = time_arrays[i]))\n",
    "    for region in regions:\n",
    "        regions[region] += chunk_regions[region]\n",
    "    print('Added chunk ' + str(i) + ' to regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(new_outlooks, new_pph, new_reports) = add_labels(new_outlooks, new_pph, new_reports, list(regions.values()), regions.keys(), 'REGION', 'NONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset by environmental data (to do later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by max total pph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing add numerical_categorical_columns\n",
    "dates = []\n",
    "pphs = []\n",
    "for date, date_pph in pph.groupby('time'):\n",
    "    dates.append(date)\n",
    "    pphs.append(float(date_pph['p_perfect_total'].max().values))\n",
    "\n",
    "pph_thresholds = { \n",
    "    'HIGH': 60,\n",
    "    'MDT': 45,\n",
    "    'ENH': 30,\n",
    "    'SLGT': 15,\n",
    "    'MRGL': 5,\n",
    "    'ZERO': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n",
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, dates, pphs, pph_thresholds, 'MAX_PPH')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of storm reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of storm reports \n",
    "# should be pretty trivial. Look up function\n",
    "full_dates = reports['DATE']\n",
    "c = Counter(full_dates)\n",
    "\n",
    "num_reports_thresholds = {\n",
    "    '1000': 1000,\n",
    "    '500': 500,\n",
    "    '100': 100,\n",
    "    '50': 50,\n",
    "    '10': 10,\n",
    "    '1': 1,\n",
    "    '0': 0\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n",
      "adding new variable in pph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding a new column in outlooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  outlooks[label_name].loc[outlooks['DATE'].isin(dates)] = label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding new variable in pph\n",
      "adding a new column in reports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miles\\AppData\\Local\\Temp\\ipykernel_19636\\3803889815.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reports[label_name].loc[reports['DATE'].isin(dates)] = label #\n"
     ]
    }
   ],
   "source": [
    "new_outlooks, new_pph, new_reports = add_numerical_categorical_columns(new_outlooks, new_pph, new_reports, c.keys(), c.values(), num_reports_thresholds, 'NUM_REPORTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by number of each type of report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_1</th>\n",
       "      <th>STATE</th>\n",
       "      <th>EVENT_TYPE</th>\n",
       "      <th>CZ_TYPE</th>\n",
       "      <th>CZ_NAME</th>\n",
       "      <th>WFO</th>\n",
       "      <th>BEGIN_DATE_TIME</th>\n",
       "      <th>CZ_TIMEZONE</th>\n",
       "      <th>END_DATE_TIME</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>...</th>\n",
       "      <th>MAX_CAT</th>\n",
       "      <th>RAMP_UP</th>\n",
       "      <th>RAMP_DOWN</th>\n",
       "      <th>RAMP_CATEGORIES</th>\n",
       "      <th>SEASON</th>\n",
       "      <th>REGION</th>\n",
       "      <th>MAX_PPH_CAT</th>\n",
       "      <th>MAX_PPH_NUM</th>\n",
       "      <th>NUM_REPORTS_CAT</th>\n",
       "      <th>NUM_REPORTS_NUM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>WASHITA</td>\n",
       "      <td></td>\n",
       "      <td>28-APR-50 14:45:00</td>\n",
       "      <td>CST</td>\n",
       "      <td>28-APR-50 14:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>COMANCHE</td>\n",
       "      <td></td>\n",
       "      <td>29-APR-50 15:30:00</td>\n",
       "      <td>CST</td>\n",
       "      <td>29-APR-50 15:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>LEHIGH</td>\n",
       "      <td></td>\n",
       "      <td>05-JUL-50 18:00:00</td>\n",
       "      <td>CST</td>\n",
       "      <td>05-JUL-50 18:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>DAUPHIN</td>\n",
       "      <td></td>\n",
       "      <td>05-JUL-50 18:30:00</td>\n",
       "      <td>CST</td>\n",
       "      <td>05-JUL-50 18:30:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>CRAWFORD</td>\n",
       "      <td></td>\n",
       "      <td>24-JUL-50 14:40:00</td>\n",
       "      <td>CST</td>\n",
       "      <td>24-JUL-50 14:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054186</th>\n",
       "      <td>72123</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>PIMA</td>\n",
       "      <td>TWC</td>\n",
       "      <td>31-JUL-23 18:40:00</td>\n",
       "      <td>MST-7</td>\n",
       "      <td>31-JUL-23 18:40:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>MRGL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054187</th>\n",
       "      <td>72124</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>PIMA</td>\n",
       "      <td>TWC</td>\n",
       "      <td>31-JUL-23 19:35:00</td>\n",
       "      <td>MST-7</td>\n",
       "      <td>31-JUL-23 19:35:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>MRGL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054188</th>\n",
       "      <td>72125</td>\n",
       "      <td>GEORGIA</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>DOUGLAS</td>\n",
       "      <td>FFC</td>\n",
       "      <td>06-AUG-23 13:25:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>06-AUG-23 13:27:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>SLGT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054189</th>\n",
       "      <td>72132</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>Tornado</td>\n",
       "      <td>C</td>\n",
       "      <td>NORMAN</td>\n",
       "      <td>FGF</td>\n",
       "      <td>24-JUN-23 15:10:00</td>\n",
       "      <td>CST-6</td>\n",
       "      <td>24-JUN-23 15:14:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>SLGT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054190</th>\n",
       "      <td>72133</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>Thunderstorm Wind</td>\n",
       "      <td>C</td>\n",
       "      <td>ORANGE</td>\n",
       "      <td>OKX</td>\n",
       "      <td>03-JUL-23 15:00:00</td>\n",
       "      <td>EST-5</td>\n",
       "      <td>03-JUL-23 15:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>SLGT</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>up</td>\n",
       "      <td>xxxxxx</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NONE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1054191 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        field_1         STATE         EVENT_TYPE CZ_TYPE   CZ_NAME  WFO  \\\n",
       "0             0      OKLAHOMA            Tornado       C   WASHITA        \n",
       "1             1         TEXAS            Tornado       C  COMANCHE        \n",
       "2             2  PENNSYLVANIA            Tornado       C    LEHIGH        \n",
       "3             3  PENNSYLVANIA            Tornado       C   DAUPHIN        \n",
       "4             4  PENNSYLVANIA            Tornado       C  CRAWFORD        \n",
       "...         ...           ...                ...     ...       ...  ...   \n",
       "1054186   72123       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       "1054187   72124       ARIZONA  Thunderstorm Wind       C      PIMA  TWC   \n",
       "1054188   72125       GEORGIA  Thunderstorm Wind       C   DOUGLAS  FFC   \n",
       "1054189   72132     MINNESOTA            Tornado       C    NORMAN  FGF   \n",
       "1054190   72133      NEW YORK  Thunderstorm Wind       C    ORANGE  OKX   \n",
       "\n",
       "            BEGIN_DATE_TIME CZ_TIMEZONE       END_DATE_TIME INJURIES_DIRECT  \\\n",
       "0        28-APR-50 14:45:00         CST  28-APR-50 14:45:00               0   \n",
       "1        29-APR-50 15:30:00         CST  29-APR-50 15:30:00               0   \n",
       "2        05-JUL-50 18:00:00         CST  05-JUL-50 18:00:00               2   \n",
       "3        05-JUL-50 18:30:00         CST  05-JUL-50 18:30:00               0   \n",
       "4        24-JUL-50 14:40:00         CST  24-JUL-50 14:40:00               0   \n",
       "...                     ...         ...                 ...             ...   \n",
       "1054186  31-JUL-23 18:40:00       MST-7  31-JUL-23 18:40:00               0   \n",
       "1054187  31-JUL-23 19:35:00       MST-7  31-JUL-23 19:35:00               0   \n",
       "1054188  06-AUG-23 13:25:00       EST-5  06-AUG-23 13:27:00               0   \n",
       "1054189  24-JUN-23 15:10:00       CST-6  24-JUN-23 15:14:00               0   \n",
       "1054190  03-JUL-23 15:00:00       EST-5  03-JUL-23 15:00:00               0   \n",
       "\n",
       "         ... MAX_CAT RAMP_UP RAMP_DOWN RAMP_CATEGORIES  SEASON REGION  \\\n",
       "0        ...    NONE       0         0         neither  xxxxxx   NONE   \n",
       "1        ...    NONE       0         0         neither  xxxxxx   NONE   \n",
       "2        ...    NONE       0         0         neither  xxxxxx   NONE   \n",
       "3        ...    NONE       0         0         neither  xxxxxx   NONE   \n",
       "4        ...    NONE       0         0         neither  xxxxxx   NONE   \n",
       "...      ...     ...     ...       ...             ...     ...    ...   \n",
       "1054186  ...    MRGL       0         0         neither  xxxxxx   NONE   \n",
       "1054187  ...    MRGL       0         0         neither  xxxxxx   NONE   \n",
       "1054188  ...    SLGT       0         0         neither  xxxxxx   NONE   \n",
       "1054189  ...    SLGT       0         0         neither  xxxxxx   NONE   \n",
       "1054190  ...    SLGT       1         0              up  xxxxxx   NONE   \n",
       "\n",
       "        MAX_PPH_CAT MAX_PPH_NUM NUM_REPORTS_CAT NUM_REPORTS_NUM  \n",
       "0              NONE         0.0               1               7  \n",
       "1              NONE         0.0               1               2  \n",
       "2              NONE         0.0               1               2  \n",
       "3              NONE         0.0               1               2  \n",
       "4              NONE         0.0               1               1  \n",
       "...             ...         ...             ...             ...  \n",
       "1054186        NONE         0.0             100             116  \n",
       "1054187        NONE         0.0             100             116  \n",
       "1054188        NONE         0.0             100             331  \n",
       "1054189        NONE         0.0             100             157  \n",
       "1054190        NONE         0.0             100             332  \n",
       "\n",
       "[1054191 rows x 46 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label by size of largest report of each type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LABEL BY ACCURACY OF FORECAST\n",
    "Brier, FSS, SAL, and/or wavelet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONCE YOU ADD THE LABELS YOU WANT, SAVE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave labelled data\n",
    "outlook_save_location = 'data/outlooks'\n",
    "pph_save_location = 'data/pph'\n",
    "report_save_location = 'data/storm_reports'\n",
    "\n",
    "new_outlooks.to_file(outlook_save_location + '/labelled_outlooks.shp')\n",
    "new_pph.to_netcdf(pph_save_location + '/labelled_pph2.nc')\n",
    "new_reports.to_csv(report_save_location + '/labelled_reports.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
